# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/base.ipynb.

# %% auto 0
__all__ = ['DiffPASSMixin', 'scalar_or_1d_tensor', 'EnsembleMixin']

# %% ../nbs/base.ipynb 2
# Stdlib imports
from collections.abc import Iterable, Sequence
from typing import Optional, Union, Any

# NumPy
# import numpy as np

# PyTorch
import torch

# PLOTTING
# from matplotlib import colormaps as cm
# import matplotlib.pyplot as plt
# from matplotlib.colors import CenteredNorm

# %% ../nbs/base.ipynb 3
class DiffPASSMixin:
    group_sizes: Iterable[int]

    @staticmethod
    def reduce_num_tokens(x: torch.Tensor) -> torch.Tensor:
        """Reduce the number of tokens in a one-hot encoded tensor."""
        used_tokens = x.clone()
        for _ in range(x.ndim - 1):
            used_tokens = used_tokens.any(-2)

        return x[..., used_tokens]

    def validate_inputs(
        self, x: torch.Tensor, y: torch.Tensor, check_same_alphabet_size: bool = False
    ) -> None:
        """Validate input tensors representing aligned objects."""
        size_x, length_x, alphabet_size_x = x.shape
        size_y, length_y, alphabet_size_y = y.shape
        if size_x != size_x:
            raise ValueError(f"Size mismatch between x ({size_x}) and y ({size_y}).")
        if check_same_alphabet_size and (alphabet_size_x != alphabet_size_y):
            raise ValueError("Inputs must have the same alphabet size.")

        # Validate size attribute
        total_size = sum(self.group_sizes)
        if size_x != total_size:
            raise ValueError(
                f"Inputs have size {total_size} but `group_sizes` implies a total "
                f"size of {total_size}."
            )

# %% ../nbs/base.ipynb 4
def scalar_or_1d_tensor(
    *, param: Any, param_name: str, dtype: torch.dtype = torch.float32
) -> torch.Tensor:
    if not isinstance(param, (float, torch.Tensor)):
        raise TypeError(f"`{param_name}` must be a float or a torch.Tensor.")
    if isinstance(param, float):
        param = torch.tensor(param, dtype=dtype)
    elif param.ndim > 1:
        raise ValueError(
            f"`{param_name}` must be a scalar or a tensor of dimension <= 1."
        )

    return param


class EnsembleMixin:
    def _validate_ensemble_param(
        self,
        *,
        param: Union[float, torch.Tensor],
        param_name: str,
        ensemble_shape: Sequence[int],
        dim_in_ensemble: Optional[int] = None,
        n_dims_per_instance: Optional[int] = None,
    ) -> torch.Tensor:
        param = scalar_or_1d_tensor(param=param, param_name=param_name)

        param = self._reshape_ensemble_param(
            param=param,
            ensemble_shape=ensemble_shape,
            dim_in_ensemble=dim_in_ensemble,
            n_dims_per_instance=n_dims_per_instance,
            param_name=param_name,
        )

        return param

    @staticmethod
    def _reshape_ensemble_param(
        *,
        param: torch.Tensor,
        ensemble_shape: Sequence[int],
        dim_in_ensemble: Optional[int],
        n_dims_per_instance: int,
        param_name: str,
    ) -> torch.Tensor:
        n_ensemble_dims = len(ensemble_shape)
        if param.ndim == 1:
            if dim_in_ensemble is None:
                raise ValueError(
                    f"`dim_in_ensemble` cannot be None if {param_name} is 1D."
                )
            param = param.to(torch.float32)
            # If param is not a scalar, broadcast it along the `ensemble_dim`-th ensemble dimension
            if dim_in_ensemble >= n_ensemble_dims or dim_in_ensemble < -n_ensemble_dims:
                raise ValueError(
                    f"Ensemble dimension for {param_name} must be an available index "
                    f"in `ensemble_shape`."
                )
            elif len(param) != ensemble_shape[dim_in_ensemble]:
                raise ValueError(
                    f"Parameter `{param_name}` must have the same length as "
                    f"``ensemble_shape[dim_in_ensemble]`` = "
                    f"{ensemble_shape[dim_in_ensemble]}."
                )
            new_shape = (
                (1,) * dim_in_ensemble
                + param.shape
                + (1,) * (n_ensemble_dims - dim_in_ensemble - 1)
                + (1,) * n_dims_per_instance
            )
            param = param.view(*new_shape)

        return param
