# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/model.ipynb.

# %% auto 0
__all__ = ['GeneralizedPermutation', 'EnsembleMatrixApply', 'TwoBodyEntropyLoss', 'MILoss', 'HammingSimilarities',
           'Blosum62Similarities', 'ReciprocalBestHits', 'InterGroupLoss', 'IntraGroupLoss']

# %% ../nbs/model.ipynb 3
# Stdlib imports
from collections.abc import Iterable, Sequence
from typing import Optional, Union, Iterator, Literal
from copy import deepcopy
from warnings import warn

# NumPy
import numpy as np

# PyTorch
import torch
from torch.nn import Module, ParameterList, Parameter, CosineSimilarity
from torch.nn.functional import softmax

# DiffPASS imports
from .base import EnsembleMixin
from .gumbel_sinkhorn_ops import gumbel_sinkhorn, gumbel_matching
from diffpass.entropy_ops import (
    smooth_mean_one_body_entropy,
    smooth_mean_two_body_entropy,
)
from .constants import get_blosum62_data
from diffpass.sequence_similarity_ops import (
    smooth_hamming_similarities_dot,
    smooth_hamming_similarities_cdist,
    smooth_substitution_matrix_similarities,
    soft_reciprocal_best_hits,
    hard_reciprocal_best_hits,
)

# %% ../nbs/model.ipynb 4
def _consecutive_slices_from_sizes(group_sizes: Optional[Sequence[int]]) -> list[slice]:
    if group_sizes is None:
        return [slice(None)]
    cumsum = np.cumsum(group_sizes).tolist()

    return [slice(start, end) for start, end in zip([0] + cumsum, cumsum)]

# %% ../nbs/model.ipynb 6
class GeneralizedPermutation(Module, EnsembleMixin):
    """Generalized permutation layer implementing both soft and hard permutations."""

    def __init__(
        self,
        *,
        group_sizes: Iterable[int],
        ensemble_shape: Optional[Iterable[int]] = None,
        fixed_matchings: Optional[Sequence[Sequence[Sequence[int]]]] = None,
        tau: Union[float, torch.Tensor] = 1.0,
        tau_dim_in_ensemble: Optional[int] = None,
        n_iter: int = 10,
        noise: bool = False,
        noise_factor: float = 1.0,
        noise_std: bool = False,
        mode: Literal["soft", "hard"] = "soft",
    ) -> None:
        super().__init__()
        self.group_sizes = tuple(s for s in group_sizes)
        self.ensemble_shape = torch.Size(
            ensemble_shape if ensemble_shape is not None else []
        )
        self._validate_fixed_matchings(fixed_matchings)
        self.fixed_matchings = fixed_matchings
        self.n_iter = n_iter
        self.noise = noise
        self.noise_factor = noise_factor
        self.noise_std = noise_std

        # By default, initialize all parametrization matrices to zero
        # If `ensemble_shape` is not None, use the first len(ensemble_shape) dimensions
        # as "batch" dimensions.
        self.nonfixed_group_sizes_ = (
            tuple(
                s - num_efm
                for s, num_efm in zip(
                    self.group_sizes, self._effective_number_fixed_matchings
                )
            )
            if self.fixed_matchings
            else self.group_sizes
        )
        self.log_alphas = ParameterList(
            [
                Parameter(torch.zeros(*self.ensemble_shape, s, s, dtype=torch.float32))
                for s in self.nonfixed_group_sizes_
            ]
        )

        tau = self._validate_ensemble_param(
            param=tau,
            dim_in_ensemble=tau_dim_in_ensemble,
            param_name="tau",
            ensemble_shape=self.ensemble_shape,
            n_dims_per_instance=2,
        )
        self.register_buffer("tau", tau)
        self.tau_dim_in_ensemble = tau_dim_in_ensemble
        self.mode = mode

    def _validate_fixed_matchings(
        self, fixed_matchings: Optional[Sequence[Sequence[Sequence[int]]]] = None
    ) -> None:
        if fixed_matchings:
            if len(fixed_matchings) != len(self.group_sizes):
                raise ValueError(
                    "If `fixed_matchings` is provided, it must have the same length as "
                    "`group_sizes`."
                )
            for s, fm in zip(self.group_sizes, fixed_matchings):
                if not fm:
                    continue
                if any([len(p) != 2 for p in fm]):
                    raise ValueError(
                        "All fixed matchings must be pairs of indices (i, j)."
                    )
                if any(min(i, j) < 0 or max(i, j) >= s for i, j in fm):
                    raise ValueError(
                        "All fixed matchings must be within the range of the corresponding "
                        "group size."
                    )
            self._effective_number_fixed_matchings = []
            self._fixed_matchings_zip = []
            for idx, (s, fm) in enumerate(zip(self.group_sizes, fixed_matchings)):
                _fm = [] if fm is None else fm
                num_fm = len(_fm)
                is_not_fully_fixed = s - num_fm > 1
                num_efm = s - (s - num_fm) * is_not_fully_fixed
                self._effective_number_fixed_matchings.append(num_efm)
                if not is_not_fully_fixed:
                    mask = torch.zeros(*self.ensemble_shape, s, s, dtype=torch.bool)
                else:
                    mask = torch.ones(*self.ensemble_shape, s, s, dtype=torch.bool)
                    for i, j in _fm:
                        mask[..., j, :] = False
                        mask[..., :, i] = False
                self.register_buffer(f"_not_fixed_masks_{idx}", mask)
                self._fixed_matchings_zip.append(tuple(zip(*_fm)) if _fm else ((), ()))

    @property
    def _not_fixed_masks(self) -> list[torch.Tensor]:
        return [
            getattr(self, f"_not_fixed_masks_{idx}")
            for idx in range(len(self.group_sizes))
        ]

    @property
    def mode(self) -> str:
        return self._mode

    @mode.setter
    def mode(self, value) -> None:
        value = value.lower()
        if value not in ["soft", "hard"]:
            raise ValueError("mode must be either 'soft' or 'hard'.")
        self._mode = value.lower()
        _mats_fn_no_fixed = getattr(self, f"_{self._mode}_mats")
        self._mats_fn = (
            _mats_fn_no_fixed
            if not self.fixed_matchings
            else self._impl_fixed_matchings(_mats_fn_no_fixed)
        )

    def soft_(self) -> None:
        self.mode = "soft"

    def hard_(self) -> None:
        self.mode = "hard"

    def _impl_fixed_matchings(self, func: callable) -> callable:
        """Include fixed matchings in the Gumbel-Sinkhorn or Gumbel-matching operators."""

        def wrapper(gen: Iterator[torch.Tensor]) -> Iterator[torch.Tensor]:
            for s, mat, (row_group, col_group), mask in zip(
                self.group_sizes, gen, self._fixed_matchings_zip, self._not_fixed_masks
            ):
                mat_all = torch.zeros(
                    *self.ensemble_shape,
                    s,
                    s,
                    dtype=mat.dtype,
                    layout=mat.layout,
                    device=mat.device,
                )
                # mat_all[j, i] = 1 means that row i becomes row j under a permutation,
                # using our conventions
                mat_all[..., col_group, row_group] = 1
                mat_all.masked_scatter_(mask.to(torch.bool), mat)
                yield mat_all

        return lambda: wrapper(func())

    def _soft_mats(self) -> Iterator[torch.Tensor]:
        """Evaluate the Gumbel-Sinkhorn operator on the current `log_alpha` parameters."""
        return (
            gumbel_sinkhorn(
                log_alpha,
                tau=self.tau,
                n_iter=self.n_iter,
                noise=self.noise,
                noise_factor=self.noise_factor,
                noise_std=self.noise_std,
            )
            for log_alpha in self.log_alphas
        )

    def _hard_mats(self) -> Iterator[torch.Tensor]:
        """Evaluate the Gumbel-matching operator on the current `log_alpha` parameters."""
        return (
            gumbel_matching(
                log_alpha,
                noise=self.noise,
                noise_factor=self.noise_factor,
                noise_std=self.noise_std,
                unbias_lsa=True,
            )
            for log_alpha in self.log_alphas
        )

    def forward(self) -> list[torch.Tensor]:
        """Compute the soft/hard permutations according to ``self._mats_fn.``"""
        mats = self._mats_fn()

        return list(mats)


class EnsembleMatrixApply(Module):
    """Apply batches of matrices to chunks of a tensor of shape (n_samples, length, alphabet_size)
    and collate the results."""

    def __init__(self, group_sizes: Iterable[int]) -> None:
        super().__init__()
        self.group_sizes = tuple(s for s in group_sizes)
        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)

    def forward(self, x: torch.Tensor, *, mats: Sequence[torch.Tensor]) -> torch.Tensor:
        ensemble_shape = mats[0].shape[:-2]
        out = torch.full_like(x, torch.nan).repeat(*ensemble_shape, 1, 1, 1)
        for mats_this_group, sl in zip(mats, self._group_slices):
            out[..., sl, :, :].copy_(
                torch.tensordot(mats_this_group, x[sl, :, :], dims=1)
            )

        return out

# %% ../nbs/model.ipynb 10
class TwoBodyEntropyLoss(Module):
    def __init__(self):
        super().__init__()

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        return smooth_mean_two_body_entropy(x, y)


class MILoss(Module):
    def __init__(self):
        super().__init__()

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        return smooth_mean_two_body_entropy(x, y) - smooth_mean_one_body_entropy(x)

# %% ../nbs/model.ipynb 13
class HammingSimilarities(Module):
    def __init__(
        self,
        *,
        group_sizes: Optional[Iterable[int]] = None,
        use_dot: bool = True,
        p: Optional[float] = None,
    ) -> None:
        super().__init__()
        self.group_sizes = (
            tuple(s for s in group_sizes) if group_sizes is not None else None
        )
        self.use_dot = use_dot
        self.p = p

        if self.use_dot:
            if self.p is not None:
                warn("Since a `p` was provided, `use_dot` will be ignored.")
            self._similarities_fn = smooth_hamming_similarities_dot
            self._similarities_fn_kwargs = {}
        else:
            if self.p is None:
                raise ValueError("If `use_dot` is False, `p` must be provided.")
            self._similarities_fn = smooth_hamming_similarities_cdist
            self._similarities_fn_kwargs = {"p": p}

        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        size = x.shape[:-3] + (x.shape[-3],) * 2
        out = torch.full(
            size, torch.nan, dtype=x.dtype, layout=x.layout, device=x.device
        )
        for sl in self._group_slices:
            out[..., sl, sl].copy_(
                self._similarities_fn(x[..., sl, :, :], **self._similarities_fn_kwargs)
            )

        return out


class Blosum62Similarities(Module):
    def __init__(
        self,
        *,
        group_sizes: Optional[Iterable[int]] = None,
        use_scoredist: bool = False,
        aa_to_int: Optional[dict[str, int]] = None,
        gaps_as_stars: bool = True,
    ) -> None:
        super().__init__()
        self.group_sizes = (
            tuple(s for s in group_sizes) if group_sizes is not None else None
        )
        self.use_scoredist = use_scoredist
        self.aa_to_int = aa_to_int
        self.gaps_as_stars = gaps_as_stars

        blosum62_data = get_blosum62_data(
            aa_to_int=self.aa_to_int, gaps_as_stars=self.gaps_as_stars
        )
        self.register_buffer("subs_mat", blosum62_data.mat)
        self.expected_value = blosum62_data.expected_value

        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        size = x.shape[:-3] + (x.shape[-3],) * 2
        out = torch.full(
            size, torch.nan, dtype=x.dtype, layout=x.layout, device=x.device
        )
        for sl in self._group_slices:
            out[..., sl, sl].copy_(
                smooth_substitution_matrix_similarities(
                    x[..., sl, :, :],
                    subs_mat=self.subs_mat,
                    expected_value=self.expected_value,
                    use_scoredist=self.use_scoredist,
                )
            )

        return out

# %% ../nbs/model.ipynb 16
class ReciprocalBestHits(Module, EnsembleMixin):
    def __init__(
        self,
        *,
        group_sizes: Iterable[int],
        ensemble_shape: Optional[Iterable[int]] = None,
        tau: Union[float, torch.Tensor] = 0.1,
        tau_dim_in_ensemble: Optional[int] = None,
        mode: Literal["soft", "hard"] = "soft",
    ) -> None:
        super().__init__()
        self.group_sizes = tuple(s for s in group_sizes)
        self.ensemble_shape = torch.Size(
            ensemble_shape if ensemble_shape is not None else []
        )

        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)

        tau = self._validate_ensemble_param(
            param=tau,
            param_name="tau",
            dim_in_ensemble=tau_dim_in_ensemble,
            ensemble_shape=self.ensemble_shape,
            n_dims_per_instance=2,
        )
        self.register_buffer("tau", tau)
        self.tau_dim_in_ensemble = tau_dim_in_ensemble

        self.mode = mode

    @property
    def mode(self) -> str:
        return self._mode

    @mode.setter
    def mode(self, value) -> None:
        value = value.lower()
        if value not in ["soft", "hard"]:
            raise ValueError("`mode` must be either 'soft' or 'hard'.")
        self._mode = value.lower()
        self._rbh_fn = getattr(self, f"_{self._mode}_rbh_fn")

    def soft_(self) -> None:
        self.mode = "soft"

    def hard_(self) -> None:
        self.mode = "hard"

    def _soft_rbh_fn(self, similarities: torch.Tensor) -> torch.Tensor:
        """Compute soft reciprocal best hits."""
        return soft_reciprocal_best_hits(
            similarities,
            group_slices=self._group_slices,
            tau=self.tau,
        )

    def _hard_rbh_fn(self, similarities: torch.Tensor) -> torch.Tensor:
        """Compute hard reciprocal best hits."""
        return hard_reciprocal_best_hits(
            similarities,
            group_slices=self._group_slices,
        )

    def prepare_fixed(self, similarities: torch.Tensor) -> torch.Tensor:
        """Repeat a similarity matrix along the tau ensemble dimension and unsqueeze."""
        assert similarities.ndim == 2
        if self.mode == "soft" and self.tau.ndim:
            if self.tau_dim_in_ensemble is None:
                raise ValueError(
                    "If using soft reciprocal best hits and a 1D `tau`, "
                    "`tau_dim_in_ensemble` must be provided."
                )
            n_ensemble_dims = len(self.ensemble_shape)
            # FIXME (self.tau.shape[self.tau_dim_in_ensemble],) is a hack, it's there
            #  because we lost the original 1D tau
            new_shape = (
                (1,) * self.tau_dim_in_ensemble
                + (self.tau.shape[self.tau_dim_in_ensemble],)
                + (1,) * (n_ensemble_dims - self.tau_dim_in_ensemble - 1)
                + (1,) * 2
            )
            similarities = similarities.repeat(*new_shape)

        return similarities

    def forward(self, similarities: torch.Tensor) -> torch.Tensor:
        # Input validation
        if self.mode == "soft" and self.tau.ndim:
            if similarities.ndim != len(self.ensemble_shape) + 2:
                raise ValueError(
                    f"If using soft reciprocal best hits and a 1D `tau`, the input must have the "
                    f"same number of dimensions as the ensemble shape plus 2 = "
                    f"{len(self.ensemble_shape) + 2}, not {similarities.ndim}."
                )
        else:
            if similarities.ndim != 2:
                raise ValueError(
                    f"If using hard reciprocal best hits or a 0D `tau`, "
                    f"the input must have 2 dimensions, not {similarities.ndim}."
                )

        return self._rbh_fn(similarities)

# %% ../nbs/model.ipynb 17
class InterGroupLoss(Module, EnsembleMixin):
    def __init__(
        self,
        *,
        group_sizes: Iterable[int],
        score_fn: Union[callable, None] = None,
    ) -> None:
        super().__init__()
        self.group_sizes = tuple(s for s in group_sizes)
        self.score_fn = CosineSimilarity(dim=-1) if score_fn is None else score_fn

        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)
        diag_blocks_mask = torch.block_diag(
            *[torch.ones((s, s), dtype=torch.bool) for s in self.group_sizes]
        )
        self._upper_no_diag_blocks_mask = torch.triu(~diag_blocks_mask)

    def forward(
        self, similarities_x: torch.Tensor, similarities_y: torch.Tensor
    ) -> torch.Tensor:
        # Input validation
        assert similarities_x.ndim >= 2 and similarities_y.ndim >= 2

        scores = self.score_fn(
            similarities_x[..., self._upper_no_diag_blocks_mask],
            similarities_y[..., self._upper_no_diag_blocks_mask],
        )
        loss = -scores

        return loss


class IntraGroupLoss(Module):
    def __init__(
        self,
        *,
        group_sizes: Optional[Iterable[int]] = None,
        score_fn: Union[callable, None] = None,
    ) -> None:
        super().__init__()
        self.group_sizes = (
            tuple(s for s in group_sizes) if group_sizes is not None else None
        )
        self.score_fn = CosineSimilarity(dim=-1) if score_fn is None else score_fn

        if self.group_sizes is not None:
            diag_blocks_mask = torch.block_diag(
                *[torch.ones((s, s), dtype=torch.bool) for s in self.group_sizes]
            )
            # Exclude main diagonal
            self._upper_diag_blocks_mask = torch.triu(diag_blocks_mask, diagonal=1)
        else:
            self._upper_diag_blocks_mask = None

    def forward(
        self, similarities_x: torch.Tensor, similarities_y: torch.Tensor
    ) -> torch.Tensor:
        assert similarities_x.ndim >= 2 and similarities_y.ndim >= 2

        if self._upper_diag_blocks_mask is None:
            mask = torch.triu(
                torch.ones(
                    similarities_x.shape[-2:],
                    dtype=torch.bool,
                    layout=similarities_x.layout,
                    device=similarities_x.device,
                ),
                diagonal=1,
            )
        else:
            mask = self._upper_diag_blocks_mask

        scores = self.score_fn(
            similarities_x[..., mask],
            similarities_y[..., mask],
        )
        loss = -scores

        return loss
