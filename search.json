[
  {
    "objectID": "entropy_ops.html",
    "href": "entropy_ops.html",
    "title": "entropy_ops",
    "section": "",
    "text": "source\n\nsmooth_mean_one_body_entropy\n\n smooth_mean_one_body_entropy (x:torch.Tensor)\n\nSmooth extension of the plug-in estimator of the onr-body Shannon entropy. x must have shape (…, N, L, R). The result has shape (…,).\n\nsource\n\n\npointwise_shannon\n\n pointwise_shannon (ps, eps=1e-20)\n\n\nsource\n\n\nsmooth_mean_two_body_entropy\n\n smooth_mean_two_body_entropy (x:torch.Tensor, y:torch.Tensor)\n\nSmooth extension of the plug-in estimator of the two-body Shannon entropy. x must have shape (…, N, L, R), and y must have shape (N, L, R). The result has shape (…,).",
    "crumbs": [
      "entropy_ops"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "DiffPaSS – Differentiable Pairing using Soft Scores",
    "section": "Overview",
    "text": "Overview\nDiffPaSS is a family of high-performance and scalable PyTorch modules for finding optimal one-to-one pairings between two collections of biological sequences, and for performing general graph alignment.\n\nPairing multiple-sequence alignments (MSAs)\nA typical example of the problem DiffPaSS is designed to solve is the following: given two multiple sequence alignments (MSAs) A and B, containing interacting biological sequences, find the optimal one-to-one pairing between the sequences in A and B.\n\n\n\nPairing problem for two multiple sequence alignments, where pairings are restricted to be within the same species\n\n\nTo find an optimal pairing, we can maximize the average mutual information between columns of the two paired MSAs (InformationPairing), or we can maximize the similarity between distance-based (MirrortreePairing) or orthology (BestHitsPairing) networks constructed from the two MSAs.\n\n\nGraph alignment and pairing unaligned sequence collections\nDiffPaSS can be used for general graph alignment problems (GraphAlignment), where the goal is to find the one-to-one pairing between the nodes of two weighted graphs that maximizes the similarity between the two graphs. The user can specify the (dis-)similarity measure to be optimized, as an arbitrary differentiable function of the adjacency matrices of the two graphs.\nUsing this capability, DiffPaSS can be used for finding the optimal one-to-one pairing between two unaligned collections of sequences, if weighted graphs are built in advance from the two collections (for example, using the pairwise Levenshtein distance). This is useful when alignments are not available or reliable.\n\n\nCan I pair two collections with a different number of sequences?\nDiffPaSS optimizes and returns permutation matrices. Hence, its inputs are required to have the same number of sequences. However, DiffPaSS can be used to pair two collections (e.g. MSAs) containing a different number of sequences, by padding the smaller collection with dummy sequences. For multiple sequence alignments, a simple choice is to add dummy sequences consisting entirely of gap symbols. For general graphs, dummy nodes, connected to the other nodes with arbitrary edge weights, can be added to the smaller graph.\n\n\nHow DiffPaSS works: soft scores, differentiable optimization, bootstrap\nCheck our paper for details of the DiffPaSS and DiffPaSS-IPA algorithms. Briefly, the main ingredients are as follows:\n\nUsing “soft” scores that differentiably extend information-theoretic scores between two paired multiple sequence alignments (MSAs), or scores based on sequence similarity or graph similarity measures.\nThe (truncated) Sinkhorn operator for smoothly parametrizing “soft permutations”, and the matching operator for parametrizing real permutations [Mena et al, 2018].\nA novel and efficient bootstrap technique, motivated by mathematical results and heuristic insights into this smooth optimization process. See the animation below for an illustration.\nA notion of “robust pairs” that can be used to identify pairs that are consistently found throughout a DiffPaSS bootstrap. These pairs can be used as ground truths in another DiffPaSS run, giving rise to the DiffPaSS-Iterative Pairing Algorithm (DiffPaSS-IPA).\n\n\n\n\n\nThe DiffPaSS bootstrap technique and robust pairs",
    "crumbs": [
      "DiffPaSS – Differentiable Pairing using Soft Scores"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "DiffPaSS – Differentiable Pairing using Soft Scores",
    "section": "Installation",
    "text": "Installation\n\nFrom PyPI\nDiffPaSS requires Python 3.7 or later. It is available on PyPI and can be installed with pip:\npython -m pip install diffpass\n\n\nFrom source\nClone this repository on your local machine by running\ngit clone git@github.com:Bitbol-Lab/DiffPaSS.git\nand move inside the root folder. We recommend creating and activating a dedicated conda or virtualenv Python virtual environment. Then, make an editable install of the package:\npython -m pip install -e .",
    "crumbs": [
      "DiffPaSS – Differentiable Pairing using Soft Scores"
    ]
  },
  {
    "objectID": "index.html#quickstart",
    "href": "index.html#quickstart",
    "title": "DiffPaSS – Differentiable Pairing using Soft Scores",
    "section": "Quickstart",
    "text": "Quickstart\n\nInput data preprocessing (MSA pairing)\nFirst, parse your multiple sequence alignments (MSAs) in FASTA format into a list of tuples (header, sequence) using read_msa.\nfrom diffpass.msa_parsing import read_msa\n\n# Parse the MSAs into lists of tuples (header, sequence)\nmsa_A = read_msa(\"path/to/msa_A.fasta\")\nmsa_B = read_msa(\"path/to/msa_B.fasta\")\nWe assume that the MSAs contain species information in the headers, which will be used to restrict the pairings to be within the same species (more generally, “groups”). We need a simple function to extract the species information from the headers. For instance, if the headers are in the format &gt;sequence_id|species_name|..., we can use:\ndef species_name_func(header):\n    return header.split(\"|\")[1]\nThis function will be used to group the sequences by species:\nfrom diffpass.data_utils import create_groupwise_seq_records\n\nmsa_A_by_sp = create_groupwise_seq_records(msa_A, species_name_func)\nmsa_B_by_sp = create_groupwise_seq_records(msa_B, species_name_func)\nIf one of the MSAs contains sequences from species not present in the other MSA, we can remove these species from both MSAs:\nfrom diffpass.data_utils import remove_groups_not_in_both\n\nmsa_A_by_sp, msa_B_by_sp = remove_groups_not_in_both(\n    msa_A_by_sp, msa_B_by_sp\n)\nIf there are species with different numbers of sequences in the two MSAs, we can add dummy sequences to the smaller species to make the number of sequences equal. For example, we can add dummy sequences consisting entirely of gap symbols:\nfrom diffpass.data_utils import pad_msas_with_dummy_sequences\n\nmsa_A_by_sp_pad, msa_B_by_sp_pad = pad_msas_with_dummy_sequences(\n    msa_A_by_sp, msa_B_by_sp\n)\n\nspecies = list(msa_A_by_sp_pad.keys())\nspecies_sizes = list(map(len, msa_A_by_sp_pad.values()))\nNext, one-hot encode the MSAs using the one_hot_encode_msa function.\nfrom diffpass.data_utils import one_hot_encode_msa\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Unpack the padded MSAs into a list of records\nmsa_A_for_pairing = [\n    rec for recs_this_sp in msa_A_by_sp_pad.values() for rec in recs_this_sp\n]\nmsa_B_for_pairing = [\n    rec for recs_this_sp in msa_B_by_sp_pad.values() for rec in recs_this_sp\n]\n\n# One-hot encode the MSAs and load them to a device\nmsa_A_oh = one_hot_encode_msa(msa_A_for_pairing, device=device)\nmsa_B_oh = one_hot_encode_msa(msa_B_for_pairing, device=device)\n\n\nPairing optimization\nFinally, we can instantiate a class from diffpass.train to find an optimal pairing between x and y. Here, x and y are MSAs, so we can look for a pairing that optimizes the mutual information between x and y. For this, we use InformationPairing and the DiffPaSS bootstrapped optimization algorithm. See the tutorials below for other examples, including for graph alignment when x and y are weighted adjacency matrices.\nfrom diffpass.train import InformationPairing\n\ninformation_pairing = InformationPairing(group_sizes=species_sizes).to(device)\nbootstrap_results = information_pairing.fit_bootstrap(x, y)\nThe results are stored in a DiffPaSSResults container. The lists of (hard) losses and permutations found during the optimization can be accessed as attributes of the container:\nprint(f\"Final hard loss: {bootstrap_results.hard_losses[-1].item()}\")\nprint(f\"Final hard permutations (one permutation per species): {bootstrap_results.hard_perms[-1][-1].item()}\")\nFor more details and examples, including the DiffPaSS-IPA variant, see the tutorials.",
    "crumbs": [
      "DiffPaSS – Differentiable Pairing using Soft Scores"
    ]
  },
  {
    "objectID": "index.html#tutorials",
    "href": "index.html#tutorials",
    "title": "DiffPaSS – Differentiable Pairing using Soft Scores",
    "section": "Tutorials",
    "text": "Tutorials\n\nmutual_information_msa_pairing.ipynb: paired MSA optimization using mutual information in the case of well-known prokaryotic datasets, for which ground truth pairings are given by genome proximity.\ngraph_alignment.ipynb: general graph alignment using diffpass.train.GraphAlignment, with an example of aligning two weighted adjacency matrices.",
    "crumbs": [
      "DiffPaSS – Differentiable Pairing using Soft Scores"
    ]
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "DiffPaSS – Differentiable Pairing using Soft Scores",
    "section": "Documentation",
    "text": "Documentation\nThe full documentation is available at https://Bitbol-Lab.github.io/DiffPaSS/.",
    "crumbs": [
      "DiffPaSS – Differentiable Pairing using Soft Scores"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "DiffPaSS – Differentiable Pairing using Soft Scores",
    "section": "Citation",
    "text": "Citation\nTo cite this work, please refer to the following publication:\n@inproceedings{\n  lupo2024diffpass,\n  title={{DiffPaSS} -- {D}ifferentiable and scalable pairing of biological sequences using soft scores},\n  author={Umberto Lupo and Damiano Sgarbossa and Martina Milighetti and Anne-Florence Bitbol},\n  booktitle={ICLR 2024 Workshop on Generative and Experimental Perspectives for Biomolecular Design},\n  year={2024},\n  url={https://openreview.net/forum?id=n5hO5seROB}\n}",
    "crumbs": [
      "DiffPaSS – Differentiable Pairing using Soft Scores"
    ]
  },
  {
    "objectID": "index.html#nbdev",
    "href": "index.html#nbdev",
    "title": "DiffPaSS – Differentiable Pairing using Soft Scores",
    "section": "nbdev",
    "text": "nbdev\nProject developed using nbdev.",
    "crumbs": [
      "DiffPaSS – Differentiable Pairing using Soft Scores"
    ]
  },
  {
    "objectID": "gumbel_sinkhorn_ops.html",
    "href": "gumbel_sinkhorn_ops.html",
    "title": "gumbel_sinkhorn_ops",
    "section": "",
    "text": "Throughout, a permutation \\(0 \\to p_0, 1 \\to p_1, ..., n-1 \\to p_{n-1}\\) is encoded by the permutation matrix \\(P = (p_{ij})_{i,j=0}^{n-1}\\) with \\(p_{ij} = 1\\) if and only if \\(j = p_i\\), and \\(0\\) otherwise.\nIn NumPy/PyTorch, P[arange(n), p] is identically equal to 1, and we can obtain p from P by p = P.argmax(-1).\n\nsource\n\ngumbel_noise_like\n\n gumbel_noise_like (log_alpha:torch.Tensor, noise_factor:float=1.0,\n                    noise_std:bool=False)\n\nGenerate rescaled Gumbel noise with the same shape as log_alpha. The noise is rescaled by noise_factor or, if noise_std is True, by noise_factor times the standard deviation of log_alpha.\n\nsource\n\n\nunbias_by_randperms\n\n unbias_by_randperms (func:&lt;built-infunctioncallable&gt;)\n\nDecorator to unbias func with two random permutations.\n\nsource\n\n\nrandperm_mat_like\n\n randperm_mat_like (log_alpha:torch.Tensor)\n\nGenerate a random permutation matrix with the same shape as log_alpha[-2:]. Assume log_alpha is of shape (batch_size, n, n).*\n\nsource\n\n\nlog_sinkhorn_norm\n\n log_sinkhorn_norm (log_alpha:torch.Tensor, n_iter:int=20)\n\nIterative Sinkhorn normalization in log space, for numerical stability.\n\nsource\n\n\nsinkhorn_norm\n\n sinkhorn_norm (alpha:torch.Tensor, n_iter:int=20)\n\nIterative Sinkhorn normalization of non-negative matrices.\n\nsource\n\n\ngumbel_sinkhorn\n\n gumbel_sinkhorn (log_alpha:torch.Tensor,\n                  tau:Union[float,torch.Tensor]=1.0, n_iter:int=10,\n                  noise:bool=False, noise_factor:float=1.0,\n                  noise_std:bool=False)\n\nGumbel-Sinkhorn operator with a temperature parameter tau. Given arbitrary square matrices, outputs bistochastic matrices that are close to permutation matrices when tau is small.\n\nsource\n\n\nmatching\n\n matching (log_alpha:torch.Tensor)\n\n\nsource\n\n\nnp_matching\n\n np_matching (cost:numpy.ndarray)\n\nFind an assignment matrix with maximum cost, using the Hungarian algorithm. Return the matrix in dense format.\n\nsource\n\n\ngumbel_matching\n\n gumbel_matching (log_alpha:torch.Tensor, noise:bool=False,\n                  noise_factor:float=1.0, noise_std:bool=False,\n                  unbias_lsa:bool=False)\n\nGumbel-matching operator, i.e. the solution of the linear assignment problem with optional Gumbel noise.\n\nsource\n\n\ninverse_permutation\n\n inverse_permutation (x:torch.Tensor, mats:torch.Tensor)\n\nWhen mats contains permutation matrices, exchange the rows of x using the inverse(s) of the permutation(s) encoded in mats.",
    "crumbs": [
      "gumbel_sinkhorn_ops"
    ]
  },
  {
    "objectID": "msa_parsing.html",
    "href": "msa_parsing.html",
    "title": "msa_parsing",
    "section": "",
    "text": "source\n\nremove_insertions\n\n remove_insertions (sequence:str)\n\nRemoves any insertions into the sequence. Needed to load aligned sequences in an MSA.\n\nsource\n\n\nread_sequence\n\n read_sequence (filename:str)\n\nReads the first (reference) sequences from a fasta or MSA file.\n\nsource\n\n\nread_msa\n\n read_msa (filename:str, nseq:int)\n\nReads the first nseq sequences from an MSA file, automatically removes insertions.",
    "crumbs": [
      "msa_parsing"
    ]
  },
  {
    "objectID": "constants.html",
    "href": "constants.html",
    "title": "constants",
    "section": "",
    "text": "source\n\nget_blosum62_data\n\n get_blosum62_data (aa_to_int:Optional[dict[str,int]]=None,\n                    gaps_as_stars:bool=False)\n\n\nsource\n\n\nTokenizedSubstitutionMatrix\n\n TokenizedSubstitutionMatrix (name:str, mat:torch.Tensor,\n                              expected_value:float)\n\n\nsource\n\n\nSubstitutionMatrix\n\n SubstitutionMatrix (name:str, mat:pandas.core.frame.DataFrame,\n                     expected_value:float)",
    "crumbs": [
      "constants"
    ]
  },
  {
    "objectID": "tutorials/mutual_information_msa_pairing.html",
    "href": "tutorials/mutual_information_msa_pairing.html",
    "title": "Pairing two protein MSAs by maximising mutual information",
    "section": "",
    "text": "# Stdlib imports\nfrom typing import Optional\n\n# NumPy\nimport numpy as np\n\n# PyTorch\nimport torch\n\n# Plotting\nfrom matplotlib import pyplot as plt\n\n# Set the number of threads for PyTorch\ntorch.set_num_threads(8)\n\n# Device\nDEVICE = torch.device(\n    f\"cuda{(':' + input('Enter the CUDA device number:')) if torch.cuda.device_count() &gt; 1 else ''}\"\n    if torch.cuda.is_available() else \"cpu\"\n)\n# DEVICE = torch.device(\"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\n# Set the seeds for NumPy and PyTorch\nNUMPY_SEED = 42\nnp.random.seed(NUMPY_SEED)\n\nTORCH_SEED = 42\ntorch.manual_seed(TORCH_SEED);\n\nUsing device: cuda",
    "crumbs": [
      "tutorials",
      "Pairing two protein MSAs by maximising mutual information"
    ]
  },
  {
    "objectID": "tutorials/mutual_information_msa_pairing.html#load-the-interacting-msas",
    "href": "tutorials/mutual_information_msa_pairing.html#load-the-interacting-msas",
    "title": "Pairing two protein MSAs by maximising mutual information",
    "section": "1. Load the interacting MSAs",
    "text": "1. Load the interacting MSAs\nWe provide two example interacting sequence datasets in FASTA format: HK-RR and MALG-MALK. The HK-RR dataset contains sequences of histidine kinases (HK) and response regulators (RR) from prokaryotic two-component systems. The MALG-MALK dataset contains sequences of the MalG and MalK subunits of the maltose transporter from prokaryotic ABC transporters.\nFor these datasets, the ground truth pairings are known (using genome proximity). We have ordered the two MSAs in each interacting systems so that the \\(i\\)-th sequence in the first MSA interacts with the \\(i\\)-th sequence in the second MSA. Therefore, the ground truth permutation is the identity permutation in this case.\nWe will need to parse the FASTA files and extract species names to group sequences coming from the same species. species_name_func extracts species names from the FASTA headers.\n\n# DiffPaSS parsing and preprocessing utilities\nfrom diffpass.msa_parsing import read_msa\nfrom diffpass.data_utils import create_groupwise_seq_records, one_hot_encode_msa, compute_num_correct_pairings\n\n\n# Load prokaryotic datasets\n\n# HK-RR datasets\nmsa_data = [\n    read_msa(\"../../data/HK-RR/HK_in_Concat_nnn.fasta\", -1),\n    read_msa(\"../../data/HK-RR/RR_in_Concat_nnn.fasta\", -1)\n]\nspecies_name_func = lambda header: header.split(\"|\")[1]\n\n## MALG-MALK datasets\n# msa_data = [\n#     read_msa(\"../../data/MALG-MALK/MALG_cov75_hmmsearch_extr5000_withLast_b.fasta\", -1),\n#     read_msa(\"../../data/MALG-MALK/MALK_cov75_hmmsearch_extr5000_withLast_b.fasta\", -1)\n# ]\n# species_name_func = lambda header: header.split(\"_\")[-1]",
    "crumbs": [
      "tutorials",
      "Pairing two protein MSAs by maximising mutual information"
    ]
  },
  {
    "objectID": "tutorials/mutual_information_msa_pairing.html#create-two-pairable-multiple-sequence-alignments-msas-and-tokenize-them",
    "href": "tutorials/mutual_information_msa_pairing.html#create-two-pairable-multiple-sequence-alignments-msas-and-tokenize-them",
    "title": "Pairing two protein MSAs by maximising mutual information",
    "section": "2. Create two pairable multiple sequence alignments (MSAs) and tokenize them",
    "text": "2. Create two pairable multiple sequence alignments (MSAs) and tokenize them\nWe pick all sequences from 50 species picked uniformly at random from the datasets. This yields two pairable MSAs, each with 581 sequences. Species with only one sequence are removed.\n\n# Organize the MSAs by species (\"groupwise\")\nmsa_data_species_by_species = [\n    create_groupwise_seq_records(msa, species_name_func, remove_groups_with_one_seq=True) \n    for msa in msa_data\n]\nall_species = list(msa_data_species_by_species[0])\nassert all_species == list(msa_data_species_by_species[1])\n\n\n# Sample a few species to work with, and filter the MSAs to only include these species\nn_species_to_sample = 50\nspecies = np.random.choice(all_species, n_species_to_sample, replace=False)\nmsa_data_species_by_species = [\n    {sp: msa_species_by_species[sp] for sp in species}\n    for msa_species_by_species in msa_data_species_by_species\n]\n\nspecies_sizes = [len(records) for records in msa_data_species_by_species[0].values()]\nprint(f\"Species sizes: {species_sizes}\")\n\nn_seqs = sum(species_sizes)\nprint(f\"Number of pairable sequences in this selection: {n_seqs}\")\n\nSpecies sizes: [3, 21, 7, 20, 21, 14, 25, 2, 7, 15, 6, 6, 6, 17, 6, 22, 10, 13, 4, 15, 14, 8, 15, 14, 11, 19, 14, 2, 21, 19, 30, 2, 27, 7, 9, 22, 2, 6, 14, 17, 3, 6, 2, 9, 13, 19, 3, 9, 2, 2]\nNumber of pairable sequences in this selection: 581\n\n\n\n# Bring data back into the original form (list of records)\nmsa_data = [\n    [record for records_this_species in msa_species_by_species.values() for record in records_this_species]\n    for msa_species_by_species in msa_data_species_by_species\n]\n\nx = one_hot_encode_msa(msa_data[0], device=DEVICE)\ny = one_hot_encode_msa(msa_data[1], device=DEVICE)",
    "crumbs": [
      "tutorials",
      "Pairing two protein MSAs by maximising mutual information"
    ]
  },
  {
    "objectID": "tutorials/mutual_information_msa_pairing.html#optimize-pairings-by-maximising-mutual-information-between-chains-informationalignment",
    "href": "tutorials/mutual_information_msa_pairing.html#optimize-pairings-by-maximising-mutual-information-between-chains-informationalignment",
    "title": "Pairing two protein MSAs by maximising mutual information",
    "section": "3. Optimize pairings by maximising mutual information between chains: InformationAlignment",
    "text": "3. Optimize pairings by maximising mutual information between chains: InformationAlignment\n\nfrom diffpass.train import InformationPairing\n\n\n# Optimization parameters for DiffPaSS bootstrap\nbootstrap_cfg = {\n    \"n_start\": 1,\n    \"n_end\": None,\n    \"step_size\": 1,  # Increase to speed up if needed\n    \"show_pbar\": True,\n    \"single_fit_cfg\": None  # Default\n}\n\n\ninformation_pairing = InformationPairing(group_sizes=species_sizes).to(DEVICE)\n\nbootstrap_results = information_pairing.fit_bootstrap(x, y, **bootstrap_cfg)\n\n 99%|██████████████████████████████████▌| 572/579 [00:34&lt;00:00, 16.75it/s]",
    "crumbs": [
      "tutorials",
      "Pairing two protein MSAs by maximising mutual information"
    ]
  },
  {
    "objectID": "tutorials/mutual_information_msa_pairing.html#plot-the-results-hard-losses-and-fraction-of-correct-pairings",
    "href": "tutorials/mutual_information_msa_pairing.html#plot-the-results-hard-losses-and-fraction-of-correct-pairings",
    "title": "Pairing two protein MSAs by maximising mutual information",
    "section": "4. Plot the results: hard losses and fraction of correct pairings",
    "text": "4. Plot the results: hard losses and fraction of correct pairings\n\n# Ground truth hard loss\ntarget_hard_loss = information_pairing.compute_losses_identity_perm(x, y)[\"hard\"]\n\n\ndef plot_hard_losses_and_frac_correct(\n    results,\n    target_hard_loss: Optional[float] = None\n):\n    hard_losses = [\n        min(hard_losses_this_step)\n        for hard_losses_this_step in results.hard_losses\n    ]\n    frac_correct = [\n        compute_num_correct_pairings(\n            hard_perms_this_step[np.argmin(hard_losses_this_step)],\n            compare_to_identity_permutation=True\n        ) / n_seqs\n        for hard_perms_this_step, hard_losses_this_step in zip(results.hard_perms, results.hard_losses)\n    ]\n\n    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n\n    axs[0].plot(hard_losses, label=\"DiffPaSS hard permutation\")\n    axs[0].axhline(target_hard_loss, color=\"red\", label=f\"Ground truth loss (identity) = {target_hard_loss:.4f}\")\n    axs[0].set_ylabel(\"Hard loss\")\n    axs[0].set_xlabel(\"Bootstrap iteration\")\n    axs[0].set_title(f\"Minimum loss during optimization: {np.min(hard_losses):.4f}\")\n    axs[0].legend()\n\n    axs[1].plot(frac_correct)\n    axs[1].set_ylabel(\"Fraction of correct pairings\")\n    axs[1].set_xlabel(\"Bootstrap iteration\")\n    axs[1].set_title(f\"Percentage of correct pairings at loss minimum: {frac_correct[np.argmin(hard_losses)] * 100:.1f}\")\n\n    plt.show()\n\n\nplot_hard_losses_and_frac_correct(bootstrap_results, target_hard_loss=target_hard_loss)",
    "crumbs": [
      "tutorials",
      "Pairing two protein MSAs by maximising mutual information"
    ]
  },
  {
    "objectID": "tutorials/mutual_information_msa_pairing.html#using-robustly-predicted-pairs-to-bootstrap-again-iterative-pairing-algorithm-ipa",
    "href": "tutorials/mutual_information_msa_pairing.html#using-robustly-predicted-pairs-to-bootstrap-again-iterative-pairing-algorithm-ipa",
    "title": "Pairing two protein MSAs by maximising mutual information",
    "section": "4. Using robustly predicted pairs to bootstrap again! Iterative Pairing Algorithm (IPA)",
    "text": "4. Using robustly predicted pairs to bootstrap again! Iterative Pairing Algorithm (IPA)\n\nfrom diffpass.ipa_utils import get_robust_pairs\n\nrobust_pairs = get_robust_pairs(\n    bootstrap_results,\n    cutoff=1.  # Decrease to consider more pairs as robust\n)\n\n\ndef print_robust_pairs_stats(robust_pairs):\n    n_robust_pairs = sum(len(robust_pairs_this_species) for robust_pairs_this_species in robust_pairs)\n    frac_pairs_robust = n_robust_pairs / n_seqs\n    print(f\"Percentage of all predicted pairs that are robust: {frac_pairs_robust * 100:.1f}%\", flush=True)\n    \n    frac_robust_pairs_correct = sum(\n        [pair[0] == pair[1] for robust_pairs_this_species in robust_pairs for pair in robust_pairs_this_species]\n    ) / n_robust_pairs\n    print(f\"Percentage of robust pairs that are correct pairs: {frac_robust_pairs_correct * 100:.1f}%\", flush=True)\n\n\nprint_robust_pairs_stats(robust_pairs)\n\nPercentage of all predicted pairs that are robust: 9.6%\nPercentage of robust pairs that are correct pairs: 100.0%\n\n\nEvery robust pair is correct! We can exploit this to start another DiffPaSS bootstrap with these pairs as fixed pairings. And then repeat the process to obtain more robust pairs, and so on. Let’s run this process 4 times, so that we will have run 5 DiffPaSS optimizations in total.\n\nn_ipa_runs = 4\n\nfor ipa_idx in range(2, n_ipa_runs + 2):\n    print(f\"DiffPaSS-IPA: run {ipa_idx}\")\n    fixed_pairings = robust_pairs\n    information_pairing = InformationPairing(group_sizes=species_sizes, fixed_pairings=fixed_pairings).to(DEVICE)\n\n    bootstrap_results = information_pairing.fit_bootstrap(x, y, **bootstrap_cfg)\n    plot_hard_losses_and_frac_correct(bootstrap_results, target_hard_loss=target_hard_loss)\n\n    robust_pairs = get_robust_pairs(bootstrap_results, cutoff=1.)\n    print_robust_pairs_stats(robust_pairs)\n    print()\n\nDiffPaSS-IPA: run 2\nPercentage of all predicted pairs that are robust: 33.0%\nPercentage of robust pairs that are correct pairs: 100.0%\n\nDiffPaSS-IPA: run 3\nPercentage of all predicted pairs that are robust: 52.7%\nPercentage of robust pairs that are correct pairs: 99.3%\n\nDiffPaSS-IPA: run 4\nPercentage of all predicted pairs that are robust: 60.1%\nPercentage of robust pairs that are correct pairs: 98.0%\n\nDiffPaSS-IPA: run 5\nPercentage of all predicted pairs that are robust: 65.2%\nPercentage of robust pairs that are correct pairs: 94.2%\n\n\n\n 98%|██████████████████████████████████▍| 515/523 [00:31&lt;00:00, 16.19it/s]\n 97%|██████████████████████████████████ | 377/387 [00:22&lt;00:00, 16.51it/s]\n 99%|██████████████████████████████████▌| 270/273 [00:15&lt;00:00, 17.80it/s]\n 98%|██████████████████████████████████▏| 225/230 [00:12&lt;00:00, 18.24it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBy running DiffPaSS-IPA a total of 5 times, we have obtained a large set of robust pairs (65.2% of all possible pairs!), 94.2% of which are correct.\nWe have also decreased the hard losses and simultaneously increased the overall percentage of correct pairs, from 68.0% at the first IPA step to 74.5% at the last.\nThe lowest hard losses obtained by DiffPaSS-IPA for this dataset and these choices of random seeds are actually slightly lower than the ground truth hard loss, which suggests that the optimization has converged to a good solution.",
    "crumbs": [
      "tutorials",
      "Pairing two protein MSAs by maximising mutual information"
    ]
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "model",
    "section": "",
    "text": "IndexPair = tuple[int, int]  # Pair of indices\nIndexPairsInGroup = list[IndexPair]  # Pairs of indices in a group of sequences\nIndexPairsInGroups = list[IndexPairsInGroup]  # Pairs of indices in groups of sequences",
    "crumbs": [
      "model"
    ]
  },
  {
    "objectID": "model.html#type-aliases",
    "href": "model.html#type-aliases",
    "title": "model",
    "section": "",
    "text": "IndexPair = tuple[int, int]  # Pair of indices\nIndexPairsInGroup = list[IndexPair]  # Pairs of indices in a group of sequences\nIndexPairsInGroups = list[IndexPairsInGroup]  # Pairs of indices in groups of sequences",
    "crumbs": [
      "model"
    ]
  },
  {
    "objectID": "model.html#sinkhornmatching-layer-for-softhard-permutations",
    "href": "model.html#sinkhornmatching-layer-for-softhard-permutations",
    "title": "model",
    "section": "Sinkhorn/matching layer for soft/hard permutations",
    "text": "Sinkhorn/matching layer for soft/hard permutations\n\nsource\n\napply_hard_permutation_batch_to_similarity\n\n apply_hard_permutation_batch_to_similarity (x:torch.Tensor,\n                                             perms:list[torch.Tensor])\n\n*Conjugate a single similarity matrix by a batch of hard permutations.\nArgs: perms: List of batches of permutation matrices of shape (…, D, D). x: Similarity matrix of shape (D, D).\nReturns: Batch of conjugated matrices of shape (…, D, D).*\n\nsource\n\n\nglobal_argmax_from_group_argmaxes\n\n global_argmax_from_group_argmaxes\n                                    (mats:collections.abc.Iterable[torch.T\n                                    ensor])\n\n\nsource\n\n\nPermutationConjugate\n\n PermutationConjugate (group_sizes:collections.abc.Sequence[int])\n\nConjugate blocks of a square 2D tensor of shape (n_samples, n_samples) by permutation matrices.\n\nsource\n\n\nMatrixApply\n\n MatrixApply (group_sizes:collections.abc.Sequence[int])\n\nApply matrices to chunks of a tensor of shape (n_samples, length, alphabet_size) and collate the results.\n\nsource\n\n\nGeneralizedPermutation\n\n GeneralizedPermutation (group_sizes:collections.abc.Sequence[int],\n                         fixed_pairings:Optional[list[list[tuple[int,int]]\n                         ]]=None, tau:float=1.0, n_iter:int=1,\n                         noise:bool=False, noise_factor:float=1.0,\n                         noise_std:bool=False,\n                         mode:Literal['soft','hard']='soft')\n\nGeneralized permutation layer implementing both soft and hard permutations.\n\n# Test for GeneralizedPermutation\n\ndef test_generalizedpermutation(*, length, alphabet_size, init_kwargs):\n    group_sizes = init_kwargs[\"group_sizes\"]\n    n_samples = sum(group_sizes)\n\n    x = torch.randn(n_samples, length, alphabet_size)\n    perm = GeneralizedPermutation(**init_kwargs)\n    mats = perm()\n    mat_apply = MatrixApply(group_sizes)\n    y = mat_apply(x, mats=mats)\n\n    assert y.shape == x.shape\n    assert y.requires_grad\n\n    perm.hard_()\n    assert perm.mode == \"hard\"\n\n\ntest_generalizedpermutation(\n    length=5,\n    alphabet_size=10,\n    init_kwargs={\n        \"group_sizes\": [3, 2, 4],\n        \"fixed_pairings\": [[(0, 1)], [(0, 0)], [(1, 0), (2, 3)]],\n        \"tau\": 0.1,\n    }\n)\n\n\ndef test_batch_perm(shape: tuple[int, int, int, int]):\n    perms = torch.randn(*shape)\n    x = torch.randn(shape[-2], shape[-1])\n\n    argmax = perms.argmax(-1)\n    x_permuted_rows = x[argmax]\n    index = argmax.view(*argmax.shape[:-1], 1, -1).expand_as(perms)\n    output = torch.gather(x_permuted_rows, -1, index)\n\n    expected = torch.stack([\n        torch.stack([\n            x[argmax[i, j], :][:, argmax[i, j]] for j in range(shape[1])\n        ], dim=0) for i in range(shape[0])\n    ], dim=0)\n\n    assert torch.equal(output, expected)\n\n\ntest_batch_perm((2, 5, 4, 4))",
    "crumbs": [
      "model"
    ]
  },
  {
    "objectID": "model.html#information-theory-losses",
    "href": "model.html#information-theory-losses",
    "title": "model",
    "section": "Information-theory losses",
    "text": "Information-theory losses\n\nsource\n\nTwoBodyEntropyLoss\n\n TwoBodyEntropyLoss ()\n\nDifferentiable extension of the mean of estimated two-body entropies between all pairs of columns from two one-hot encoded tensors.\n\nsource\n\n\nMILoss\n\n MILoss ()\n\nDifferentiable extension of minus the mean of estimated mutual informations between all pairs of columns from two one-hot encoded tensors.\n\n# Test for TwoBodyEntropyScore\n\ndef test_twobodyentropyloss(\n        *,\n        n_samples, length_x, length_y, alphabet_size\n):\n    x = torch.randn(\n        n_samples, length_x, alphabet_size,\n        requires_grad=True\n    )\n    y = torch.randn(n_samples, length_y, alphabet_size)\n    x_soft = softmax(x, dim=-1)\n    y_soft = softmax(y, dim=-1)\n    two_body_entropy_loss = TwoBodyEntropyLoss()\n    loss = two_body_entropy_loss(x_soft, y_soft)\n\n    assert loss.requires_grad\n\n    # In the following scenario, the score should be close to log2(alphabet_size)\n    x_almost_hard = softmax(x / 1e-5, dim=-1)\n    first_x_almost_hard_length_1 = x_almost_hard[:, :1, :]\n    loss = two_body_entropy_loss(\n        first_x_almost_hard_length_1, first_x_almost_hard_length_1\n    )\n\n    torch.testing.assert_close(\n        loss, torch.log2(torch.tensor(alphabet_size)), atol=1e-3, rtol=1e-7\n    )\n\n\ntest_twobodyentropyloss(\n    n_samples=10_000,\n    length_x=3,\n    length_y=4,\n    alphabet_size=3,\n)",
    "crumbs": [
      "model"
    ]
  },
  {
    "objectID": "model.html#sequence-similarities-hamming-and-blosum62",
    "href": "model.html#sequence-similarities-hamming-and-blosum62",
    "title": "model",
    "section": "Sequence similarities (Hamming and Blosum62)",
    "text": "Sequence similarities (Hamming and Blosum62)\n\nsource\n\nHammingSimilarities\n\n HammingSimilarities\n                      (group_sizes:Optional[collections.abc.Sequence[int]]\n                      =None, use_dot:bool=True, p:Optional[float]=None)\n\n*Compute Hamming similarities between sequences using differentiable operations.\nOptionally, if the sequences are arranged in groups, the computation of similarities can be restricted to within groups. Differentiable operations are used to compute the similarities, which can be either dot products or an L^p distance function.*\n\nsource\n\n\nBlosum62Similarities\n\n Blosum62Similarities\n                       (group_sizes:Optional[collections.abc.Sequence[int]\n                       ]=None, use_dot:bool=True, p:Optional[float]=None,\n                       use_scoredist:bool=False,\n                       aa_to_int:Optional[dict[str,int]]=None,\n                       gaps_as_stars:bool=True)\n\n*Compute Blosum62-based similarities between sequences using differentiable operations.\nOptionally, if the sequences are arranged in groups, the computation of similarities can be restricted to within groups. Differentiable operations are used to compute the similarities, which can be either dot products or an L^p distance function.*\n\n# Tests for HammingSimilarities and Blosum62Similarities\n\ndef test_similarities(\n        *,\n        cls,\n        length, alphabet_size,\n        init_kwargs\n):\n    group_sizes = init_kwargs[\"group_sizes\"]\n    n_samples = sum(group_sizes)\n\n    x = torch.randn(\n        n_samples, length, alphabet_size,\n        requires_grad=True\n    )\n    x_soft = softmax(x, dim=-1)\n\n    _init_kwargs = deepcopy(init_kwargs)\n    _init_kwargs[\"group_sizes\"] = None\n    similarities = cls(**_init_kwargs)\n    out_all = similarities(x_soft)\n\n    assert out_all.shape == (n_samples, n_samples)\n\n    similarities = cls(**init_kwargs)\n    out = similarities(x_soft)\n\n    for sl in similarities._group_slices:\n        assert torch.allclose(\n            out[..., sl, sl], out_all[..., sl, sl]\n        )\n\n\ntest_similarities(\n    cls=HammingSimilarities,\n    length=3,\n    alphabet_size=10,\n    init_kwargs={\"group_sizes\": [3, 2, 4], \"use_dot\": False, \"p\": 1.}\n)\n\ntest_similarities(\n    cls=Blosum62Similarities,\n    length=3,\n    alphabet_size=21,\n    init_kwargs={\"group_sizes\": [3, 2, 4]}\n)",
    "crumbs": [
      "model"
    ]
  },
  {
    "objectID": "model.html#best-hits-from-similarities",
    "href": "model.html#best-hits-from-similarities",
    "title": "model",
    "section": "Best hits from similarities",
    "text": "Best hits from similarities\n\nsource\n\nBestHits\n\n BestHits (reciprocal:bool=True,\n           group_sizes:Optional[collections.abc.Sequence[int]],\n           tau:float=0.1, mode:Literal['soft','hard']='soft')\n\n*Compute (reciprocal) best hits within and between groups of sequences, starting from a similarity matrix.\nBest hits can be either ‘hard’, in which cases they are computed using the argmax, or ‘soft’, in which case they are computed using the softmax with a temperature parameter tau. In both cases, the main diagonal in the similarity matrix is excluded by setting its entries to minus infinity.*",
    "crumbs": [
      "model"
    ]
  },
  {
    "objectID": "model.html#losses-based-on-comparing-similarity-matrices",
    "href": "model.html#losses-based-on-comparing-similarity-matrices",
    "title": "model",
    "section": "Losses based on comparing similarity matrices",
    "text": "Losses based on comparing similarity matrices\n\nsource\n\nInterGroupSimilarityLoss\n\n InterGroupSimilarityLoss (group_sizes:collections.abc.Sequence[int],\n                           score_fn:Optional[&lt;built-\n                           infunctioncallable&gt;]=None)\n\n*Compute a loss that compares similarity matrices restricted to inter-group relationships.\nSimilarity matrices are expected to be square and symmetric. The loss is computed by comparing the (flattened and concatenated) blocks containing inter-group similarities.*\n\nsource\n\n\nIntraGroupSimilarityLoss\n\n IntraGroupSimilarityLoss\n                           (group_sizes:Optional[collections.abc.Sequence[\n                           int]]=None, score_fn:Optional[&lt;built-\n                           infunctioncallable&gt;]=None,\n                           exclude_diagonal:bool=True)\n\n*Compute a loss that compares similarity matrices restricted to intra-group relationships.\nSimilarity matrices are expected to be square and symmetric. Their diagonal elements are ignored if exclude_diagonal is set to True. If group_sizes is provided, the loss is computed by comparing the flattened and concatenated upper triangular blocks containing intra-group similarities. Otherwise, the loss is computed by comparing the upper triangular part of the full similarity matrices.*\n\n# Test for BestHits, InterGroupSimilarityLoss and IntraGroupSimilarityLoss\n\ndef test_sequence_similarity_losses(\n        *,\n        group_sizes,\n        length_x, length_y, alphabet_size,\n        extra_init_kwargs_bh, extra_init_kwargs_loss\n):\n    similarities = HammingSimilarities(group_sizes=None)\n    best_hits = BestHits(group_sizes=group_sizes, **extra_init_kwargs_bh)\n    n_samples = sum(group_sizes)\n\n    y = torch.randn(n_samples, length_y, alphabet_size)\n    y.scatter_(-1, y.argmax(dim=-1, keepdim=True), 1.)\n    similarities_y = similarities(y)\n    best_hits.hard_()\n    best_hits_y = best_hits(similarities_y)\n    best_hits.soft_()\n\n    x = torch.randn(\n        n_samples, length_x, alphabet_size,\n        requires_grad=True\n    )\n    x_soft = softmax(x, dim=-1)\n    similarities_x = similarities(x_soft)\n    best_hits_x = best_hits(similarities_x)\n\n    #### Best hits loss ####\n    inter_group_similarity_loss = InterGroupSimilarityLoss(group_sizes=group_sizes, **extra_init_kwargs_loss)\n    loss = inter_group_similarity_loss(best_hits_x, best_hits_y)\n\n    assert loss.requires_grad\n\n    # In the following scenario, the loss should be close to -1\n    extra_init_kwargs_bh = deepcopy(extra_init_kwargs_bh)\n    extra_init_kwargs_bh[\"tau\"] = 1e-4\n    best_hits = BestHits(group_sizes=group_sizes, **extra_init_kwargs_bh)\n    best_hits_x = best_hits(similarities_y)\n    loss = inter_group_similarity_loss(best_hits_x, best_hits_y)\n\n    torch.testing.assert_close(loss, torch.tensor(-1.))\n\n    #### Mirrortree-like loss ####\n    intra_group_similarity_loss = IntraGroupSimilarityLoss(**extra_init_kwargs_loss)\n    loss = intra_group_similarity_loss(similarities_x, similarities_y)\n\n    assert loss.requires_grad\n\n    # In the following scenario, the loss should be close to -1\n    loss = intra_group_similarity_loss(similarities_y, similarities_y)\n\n    torch.testing.assert_close(loss, torch.tensor(-1.))\n\n\ntest_sequence_similarity_losses( \n    length_x=3,\n    length_y=4,\n    alphabet_size=3,\n    group_sizes=[3, 2, 4],\n    extra_init_kwargs_bh={\n        \"tau\": 0.1,\n    },\n    extra_init_kwargs_loss={\n        \"score_fn\": torch.nn.CosineSimilarity(dim=-1)\n    }\n)",
    "crumbs": [
      "model"
    ]
  },
  {
    "objectID": "tutorials/graph_alignment.html",
    "href": "tutorials/graph_alignment.html",
    "title": "Graph alignment",
    "section": "",
    "text": "Note: This notebook requires networkx.\n# Stdlib imports\nfrom typing import Optional\n\n# NumPy\nimport numpy as np\n\n# PyTorch\nimport torch\n\n# Plotting\nfrom matplotlib import pyplot as plt\n\n# NetworkX\nimport networkx as nx\n\n# Set the number of threads for PyTorch\ntorch.set_num_threads(8)\n\n# Device\nDEVICE = torch.device(\n    f\"cuda{(':' + input('Enter the CUDA device number:')) if torch.cuda.device_count() &gt; 1 else ''}\"\n    if torch.cuda.is_available() else \"cpu\"\n)\n# DEVICE = torch.device(\"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\n# Set the seeds for NumPy and PyTorch\nNUMPY_SEED = 42\nnp.random.seed(NUMPY_SEED)\n\nTORCH_SEED = 42\ntorch.manual_seed(TORCH_SEED);\n\nNX_SEED = 42\n\nUsing device: cuda",
    "crumbs": [
      "tutorials",
      "Graph alignment"
    ]
  },
  {
    "objectID": "tutorials/graph_alignment.html#create-the-adjacency-matrices-of-two-sbm-graphs",
    "href": "tutorials/graph_alignment.html#create-the-adjacency-matrices-of-two-sbm-graphs",
    "title": "Graph alignment",
    "section": "1. Create the adjacency matrices of two SBM graphs",
    "text": "1. Create the adjacency matrices of two SBM graphs\nWe create two SBM graphs with a few communities of equal size. We then randomly permute the nodes in one of the graphs to create the second graph. The Laplacian matrices of these graphs are used as inputs for diffpass.train.GraphAlignment.\n\ndef create_test_sbm_laplacian(\n    block_sizes: list ,\n    block_prob: list,\n):\n    \"\"\"Create a stochastic block model graph with given block sizes and block probabilities. Return its dense Laplacian matrix.\"\"\"\n    g = nx.stochastic_block_model(block_sizes, block_prob, seed=NX_SEED)\n    g.remove_nodes_from(list(nx.isolates(g)))\n    n = len(g)\n    # Laplacian matrix\n    l = nx.laplacian_matrix(g, range(n))\n\n    return torch.tensor(\n        l.todense(), dtype=torch.get_default_dtype(), device=DEVICE\n    )\n\n# Create an SBM graph with `n_blocks` communities of equal size\nn_nodes = 40\nn_blocks = 4\nblock_sizes = [int(n_nodes / n_blocks)] * n_blocks\ninter = 0.3\nintra = 0.7\nprobs = [\n    [intra, inter, inter, inter],\n    [inter, intra, inter, inter],\n    [inter, inter, intra, inter],\n    [inter, inter, inter, intra]\n]\n\n# Create the Laplacian matrix of the first graph\nx = create_test_sbm_laplacian(block_sizes, probs)\nn_nodes = len(x)  # In case the number of nodes has changed due to removal of isolates\n\n# Define the second graph by randomly permuting the nodes in the first\nP = torch.zeros_like(x)\nP[torch.arange(n_nodes), torch.randperm(n_nodes)] = 1\ny = P @ x @ P.T",
    "crumbs": [
      "tutorials",
      "Graph alignment"
    ]
  },
  {
    "objectID": "tutorials/graph_alignment.html#align-the-graphs-by-maximising-the-dot-product-between-the-laplacian-matrices-graphalignment",
    "href": "tutorials/graph_alignment.html#align-the-graphs-by-maximising-the-dot-product-between-the-laplacian-matrices-graphalignment",
    "title": "Graph alignment",
    "section": "2. Align the graphs by maximising the dot product between the Laplacian matrices: GraphAlignment",
    "text": "2. Align the graphs by maximising the dot product between the Laplacian matrices: GraphAlignment\nThe default behaviour of GraphAlignment is to maximise the dot product between the upper triangles (including or excluding the main diagonal) of two input square matrices. The GraphAlignment class also allows for custom comparison losses to be used, but we do not do so in this example\n\nfrom diffpass.model import IntraGroupSimilarityLoss\nfrom diffpass.train import GraphAlignment\n\ngroup_sizes = [n_nodes]\ncomparison_loss = IntraGroupSimilarityLoss(\n    group_sizes=group_sizes, exclude_diagonal=False\n)  # Default behaviour except for inclusion of the main diagonal\n\n# Initialize the GraphAlignment model\ngraph_alignment = GraphAlignment(\n    group_sizes=group_sizes,\n    comparison_loss=comparison_loss\n).to(DEVICE)\n\n\n# Optimization parameters for DiffPaSS bootstrap\nbootstrap_cfg = {\n    \"n_start\": 1,\n    \"n_end\": None,\n    \"step_size\": 1,  # Increase to speed up if needed\n    \"show_pbar\": True,\n    \"single_fit_cfg\": None  # Default\n}\n\n# Run the DiffPaSS bootstrap\nbootstrap_results = graph_alignment.fit_bootstrap(x, y, **bootstrap_cfg)\n\n100%|██████████| 38/38 [00:00&lt;00:00, 252.12it/s]",
    "crumbs": [
      "tutorials",
      "Graph alignment"
    ]
  },
  {
    "objectID": "tutorials/graph_alignment.html#plot-the-resulting-hard-losses",
    "href": "tutorials/graph_alignment.html#plot-the-resulting-hard-losses",
    "title": "Graph alignment",
    "section": "3. Plot the resulting hard losses",
    "text": "3. Plot the resulting hard losses\n\n# Ground truth hard loss\ntarget_hard_loss = graph_alignment.compute_losses_identity_perm(x, x)[\"hard\"]\n\n\ndef plot_hard_losses(\n    results,\n    target_hard_loss: Optional[float] = None\n):\n    hard_losses = [\n        min(hard_losses_this_step)\n        for hard_losses_this_step in results.hard_losses\n    ]\n\n    plt.plot(hard_losses, \".-\", label=\"DiffPaSS hard permutation\")\n    plt.axhline(target_hard_loss, color=\"red\", label=f\"Ground truth loss (identity) = {target_hard_loss:.4f}\")\n    plt.ylabel(\"Hard loss\")\n    plt.xlabel(\"Bootstrap iteration\")\n    plt.title(f\"Minimum loss during optimization: {np.min(hard_losses):.4f}\")\n    plt.legend()\n\n    plt.show()\n\n\nplot_hard_losses(bootstrap_results, target_hard_loss=target_hard_loss)\n\n\n\n\n\n\n\n\nThe hard loss has been decreased, but is a far cry from the ground truth loss.\nCan we do better?",
    "crumbs": [
      "tutorials",
      "Graph alignment"
    ]
  },
  {
    "objectID": "tutorials/graph_alignment.html#increase-the-number-of-repeats-for-each-bootstrap-iteration",
    "href": "tutorials/graph_alignment.html#increase-the-number-of-repeats-for-each-bootstrap-iteration",
    "title": "Graph alignment",
    "section": "4. Increase the number of repeats for each bootstrap iteration",
    "text": "4. Increase the number of repeats for each bootstrap iteration\nWe can use a “greedier” approach and further exploit the randomness in the bootstrap procedure. At each bootstrap iteration, instead of only performing a single gradient optimization, we can perform multiple optimizations – each using a different, randomly sampled set of fixed pairs. We can then choose the one yielding the lowest hard loss.\nThis is done by setting n_repeats to a value greater than 1 in the fit_bootstrap method.\n\nbootstrap_cfg = {\n    \"n_start\": 1,\n    \"n_end\": None,\n    \"step_size\": 1,  # Increase to speed up if needed\n    \"show_pbar\": True,\n    \"single_fit_cfg\": None,  # Default\n    \"n_repeats\": 100  # Number of repeats for each bootstrap iteration -- increasing this should lead to better results\n}\n\ngraph_alignment = GraphAlignment(\n    group_sizes=group_sizes,\n    comparison_loss=comparison_loss\n).to(DEVICE)\n\nbootstrap_results = graph_alignment.fit_bootstrap(x, y, **bootstrap_cfg)\n\nplot_hard_losses(bootstrap_results, target_hard_loss=target_hard_loss)\n\n100%|██████████| 38/38 [00:13&lt;00:00,  2.89it/s]\n\n\n\n\n\n\n\n\n\nAwesome! We have perfectly matched the hard loss given by the ground truth alignment!",
    "crumbs": [
      "tutorials",
      "Graph alignment"
    ]
  },
  {
    "objectID": "base.html",
    "href": "base.html",
    "title": "base",
    "section": "",
    "text": "BootstrapList = list  # List indexed by bootstrap iteration\nGradientDescentList = list  # List indexed by gradient descent iteration\nGroupByGroupList = list  # List indexed by group index\n\nIndexPair = tuple[int, int]  # Pair of indices\nIndexPairsInGroup = list[IndexPair]  # Pairs of indices in a group of sequences\nIndexPairsInGroups = list[IndexPairsInGroup]  # Pairs of indices in groups of sequences\n\nsource\n\n\n\n make_pbar (epochs:int, show_pbar:bool)\n\n\nsource\n\n\n\n\n dccn (x:torch.Tensor)\n\n\nsource\n\n\n\n\n DiffPaSSResults (log_alphas:Union[list[list[numpy.ndarray]],list[list[lis\n                  t[numpy.ndarray]]],NoneType], soft_perms:Union[list[list\n                  [numpy.ndarray]],list[list[list[numpy.ndarray]]],NoneTyp\n                  e], hard_perms:Union[list[list[numpy.ndarray]],list[list\n                  [list[numpy.ndarray]]]], hard_losses:Union[list[list[flo\n                  at]],list[list[list[float]]]], soft_losses:Union[list[li\n                  st[float]],list[list[list[float]]],NoneType])\n\nContainer for results of DiffPaSS fits.\n\nsource\n\n\n\n\n DiffPaSSModel (*args, **kwargs)\n\nBase class for DiffPaSS models.\n\nsource\n\n\n\n\n DiffPaSSModel.fit (x:torch.Tensor, y:torch.Tensor, epochs:int=1,\n                    optimizer_name:Optional[str]='SGD',\n                    optimizer_kwargs:Optional[dict[str,Any]]=None,\n                    mean_centering:bool=False, show_pbar:bool=False,\n                    compute_final_soft:bool=False,\n                    record_log_alphas:bool=False,\n                    record_soft_perms:bool=False,\n                    record_soft_losses:bool=False)\n\nFit permutations to data using gradient descent.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nTensor\n\nThe object (MSA or adjacency matrix of graphs) to be permuted\n\n\ny\nTensor\n\nThe target object (MSA or adjacency matrix of graphs), that the objects represented by x should be paired with. Not acted upon by soft/hard permutations\n\n\nepochs\nint\n1\n\n\n\noptimizer_name\nOptional\nSGD\n\n\n\noptimizer_kwargs\nOptional\nNone\n\n\n\nmean_centering\nbool\nFalse\n\n\n\nshow_pbar\nbool\nFalse\n\n\n\ncompute_final_soft\nbool\nFalse\n\n\n\nrecord_log_alphas\nbool\nFalse\n\n\n\nrecord_soft_perms\nbool\nFalse\n\n\n\nrecord_soft_losses\nbool\nFalse\n\n\n\nReturns\nDiffPaSSResults\n\n\n\n\n\n\nsource\n\n\n\n\n DiffPaSSModel.fit_bootstrap (x:torch.Tensor, y:torch.Tensor,\n                              n_start:int=1, n_end:Optional[int]=None,\n                              step_size:int=1, n_repeats:int=1,\n                              show_pbar:bool=True,\n                              single_fit_cfg:Optional[dict]=None)\n\n*Fit permutations to data using the DiffPaSS bootstrap.\nThe DiffPaSS bootstrap consists of a sequence of short gradient descent runs (default: one epoch per run). At the end of each run, a subset of the found pairings is chosen uniformly at random and fixed for the next run. The number of pairings fixed at each iteration ranges between n_start (default: 1) and n_end (default: total number of pairs), with a step size of step_size.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nTensor\n\nThe object (MSA or adjacency matrix of graphs) to be permuted\n\n\ny\nTensor\n\nThe target object (MSA or adjacency matrix of graphs), that the objects represented by x should be paired with. Not acted upon by soft/hard permutations\n\n\nn_start\nint\n1\n\n\n\nn_end\nOptional\nNone\n\n\n\nstep_size\nint\n1\n\n\n\nn_repeats\nint\n1\n\n\n\nshow_pbar\nbool\nTrue\n\n\n\nsingle_fit_cfg\nOptional\nNone\n\n\n\nReturns\nDiffPaSSResults",
    "crumbs": [
      "base"
    ]
  },
  {
    "objectID": "base.html#type-aliases",
    "href": "base.html#type-aliases",
    "title": "base",
    "section": "",
    "text": "BootstrapList = list  # List indexed by bootstrap iteration\nGradientDescentList = list  # List indexed by gradient descent iteration\nGroupByGroupList = list  # List indexed by group index\n\nIndexPair = tuple[int, int]  # Pair of indices\nIndexPairsInGroup = list[IndexPair]  # Pairs of indices in a group of sequences\nIndexPairsInGroups = list[IndexPairsInGroup]  # Pairs of indices in groups of sequences\n\nsource\n\n\n\n make_pbar (epochs:int, show_pbar:bool)\n\n\nsource\n\n\n\n\n dccn (x:torch.Tensor)\n\n\nsource\n\n\n\n\n DiffPaSSResults (log_alphas:Union[list[list[numpy.ndarray]],list[list[lis\n                  t[numpy.ndarray]]],NoneType], soft_perms:Union[list[list\n                  [numpy.ndarray]],list[list[list[numpy.ndarray]]],NoneTyp\n                  e], hard_perms:Union[list[list[numpy.ndarray]],list[list\n                  [list[numpy.ndarray]]]], hard_losses:Union[list[list[flo\n                  at]],list[list[list[float]]]], soft_losses:Union[list[li\n                  st[float]],list[list[list[float]]],NoneType])\n\nContainer for results of DiffPaSS fits.\n\nsource\n\n\n\n\n DiffPaSSModel (*args, **kwargs)\n\nBase class for DiffPaSS models.\n\nsource\n\n\n\n\n DiffPaSSModel.fit (x:torch.Tensor, y:torch.Tensor, epochs:int=1,\n                    optimizer_name:Optional[str]='SGD',\n                    optimizer_kwargs:Optional[dict[str,Any]]=None,\n                    mean_centering:bool=False, show_pbar:bool=False,\n                    compute_final_soft:bool=False,\n                    record_log_alphas:bool=False,\n                    record_soft_perms:bool=False,\n                    record_soft_losses:bool=False)\n\nFit permutations to data using gradient descent.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nTensor\n\nThe object (MSA or adjacency matrix of graphs) to be permuted\n\n\ny\nTensor\n\nThe target object (MSA or adjacency matrix of graphs), that the objects represented by x should be paired with. Not acted upon by soft/hard permutations\n\n\nepochs\nint\n1\n\n\n\noptimizer_name\nOptional\nSGD\n\n\n\noptimizer_kwargs\nOptional\nNone\n\n\n\nmean_centering\nbool\nFalse\n\n\n\nshow_pbar\nbool\nFalse\n\n\n\ncompute_final_soft\nbool\nFalse\n\n\n\nrecord_log_alphas\nbool\nFalse\n\n\n\nrecord_soft_perms\nbool\nFalse\n\n\n\nrecord_soft_losses\nbool\nFalse\n\n\n\nReturns\nDiffPaSSResults\n\n\n\n\n\n\nsource\n\n\n\n\n DiffPaSSModel.fit_bootstrap (x:torch.Tensor, y:torch.Tensor,\n                              n_start:int=1, n_end:Optional[int]=None,\n                              step_size:int=1, n_repeats:int=1,\n                              show_pbar:bool=True,\n                              single_fit_cfg:Optional[dict]=None)\n\n*Fit permutations to data using the DiffPaSS bootstrap.\nThe DiffPaSS bootstrap consists of a sequence of short gradient descent runs (default: one epoch per run). At the end of each run, a subset of the found pairings is chosen uniformly at random and fixed for the next run. The number of pairings fixed at each iteration ranges between n_start (default: 1) and n_end (default: total number of pairs), with a step size of step_size.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nTensor\n\nThe object (MSA or adjacency matrix of graphs) to be permuted\n\n\ny\nTensor\n\nThe target object (MSA or adjacency matrix of graphs), that the objects represented by x should be paired with. Not acted upon by soft/hard permutations\n\n\nn_start\nint\n1\n\n\n\nn_end\nOptional\nNone\n\n\n\nstep_size\nint\n1\n\n\n\nn_repeats\nint\n1\n\n\n\nshow_pbar\nbool\nTrue\n\n\n\nsingle_fit_cfg\nOptional\nNone\n\n\n\nReturns\nDiffPaSSResults",
    "crumbs": [
      "base"
    ]
  },
  {
    "objectID": "train.html",
    "href": "train.html",
    "title": "train",
    "section": "",
    "text": "IndexPair = tuple[int, int]  # Pair of indices\nIndexPairsInGroup = list[IndexPair]  # Pairs of indices in a group of sequences\nIndexPairsInGroups = list[IndexPairsInGroup]  # Pairs of indices in groups of sequences\n\nsource\n\n\n\n InformationPairing (group_sizes:collections.abc.Sequence[int],\n                     fixed_pairings:Optional[list[list[tuple[int,int]]]]=N\n                     one, permutation_cfg:Optional[dict[str,Any]]=None, in\n                     formation_measure:Literal['MI','TwoBodyEntropy']='Two\n                     BodyEntropy')\n\nDiffPaSS model for information-theoretic pairing of multiple sequence alignments (MSAs).\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ngroup_sizes\nSequence\n\nNumber of sequences in each group (e.g. species) of the two MSAs\n\n\nfixed_pairings\nOptional\nNone\nIf not None, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), …], …] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc.\n\n\npermutation_cfg\nOptional\nNone\nIf not None, configuration dictionary containing init parameters for the internal GeneralizedPermutation object to compute soft/hard permutations\n\n\ninformation_measure\nLiteral\nTwoBodyEntropy\nInformation-theoretic measure to use. For hard permutations, these two measures are equivalent\n\n\n\n\ndef test_information_bootstrap():\n    # Data: two highly correlated MSAs\n    n_classes = 3\n    length = 5\n    size_each_group = 10\n    n_groups = 10\n    # Define first MSA group by group\n    x_tok_by_group = [torch.randint(0, n_classes, (size_each_group, length)) for _ in range(n_groups)]\n    # Within-group shuffling to control for algorithmic biases towards identity permutation\n    x_tok_by_group_shuffle = [x[torch.randperm(size_each_group)] for x in x_tok_by_group]\n    x_tok = torch.cat(x_tok_by_group, dim=0)\n    x_tok_shuffle = torch.cat(x_tok_by_group_shuffle, dim=0)\n    y_tok = (x_tok + 1) % n_classes\n    x = torch.nn.functional.one_hot(x_tok).to(torch.get_default_dtype())\n    x_shuffle = torch.nn.functional.one_hot(x_tok_shuffle).to(torch.get_default_dtype())\n    y = torch.nn.functional.one_hot(y_tok).to(torch.get_default_dtype())\n\n    group_sizes = [size_each_group] * n_groups\n\n    # Model\n    model = InformationPairing(group_sizes=group_sizes)\n    results = model.fit_bootstrap(x_shuffle, y)\n    hard_loss_identity_perm = model.compute_losses_identity_perm(x, y)[\"hard\"]\n\n    # Check that the hard loss of the optimized permutation is close to the ground truth\n    assert np.abs(results.hard_losses[-2][-1] - hard_loss_identity_perm) &lt; 1e-4\n\ntest_information_bootstrap()\n\n\nsource\n\n\n\n\n BestHitsPairing (group_sizes:collections.abc.Sequence[int],\n                  fixed_pairings:Optional[list[list[tuple[int,int]]]]=None\n                  , permutation_cfg:Optional[dict[str,Any]]=None,\n                  similarity_kind:Literal['Hamming','Blosum62']='Hamming',\n                  similarities_cfg:Optional[dict[str,Any]]=None,\n                  compute_in_group_best_hits:bool=True,\n                  best_hits_cfg:Optional[dict[str,Any]]=None,\n                  similarities_comparison_loss:Optional[&lt;built-\n                  infunctioncallable&gt;]=None,\n                  compare_soft_best_hits_to_hard:bool=True)\n\nDiffPaSS model for pairing of multiple sequence alignments (MSAs) by aligning their orthology networks, constructed using (reciprocal) best hits .\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ngroup_sizes\nSequence\n\nNumber of sequences in each group (e.g. species) of the two MSAs\n\n\nfixed_pairings\nOptional\nNone\nIf not None, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), …], …] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc.\n\n\npermutation_cfg\nOptional\nNone\nIf not None, configuration dictionary containing init parameters for the internal GeneralizedPermutation object to compute soft/hard permutations\n\n\nsimilarity_kind\nLiteral\nHamming\n(Smoothly extended) similarity metric to use on all pairs of aligned sequences\n\n\nsimilarities_cfg\nOptional\nNone\nIf not None, configuration dictionary containing init parameters for the internal HammingSimilarities or Blosum62Similarities object to compute similarity matrices\n\n\ncompute_in_group_best_hits\nbool\nTrue\nWhether to also compute best hits within each group (in addition to between different groups)\n\n\nbest_hits_cfg\nOptional\nNone\nIf not None, configuration dictionary containing init parameters for the internal BestHits object to compute soft/hard (reciprocal) best hits\n\n\nsimilarities_comparison_loss\nOptional\nNone\nIf not None, custom callable to compute the differentiable loss between the soft/hard best hits matrices of the two MSAs\n\n\ncompare_soft_best_hits_to_hard\nbool\nTrue\nWhether to compare the soft best hits from the MSA to permute (x) to the hard or soft best hits from the reference MSA (y)\n\n\n\n\ndef test_besthits_bootstrap():\n    # Data: two highly correlated MSAs\n    n_classes = 3\n    length = 100\n    size_each_group = 10\n    n_groups = 10\n    # Define first MSA group by group\n    x_tok_by_group = [torch.randint(0, n_classes, (size_each_group, length)) for _ in range(n_groups)]\n    # Within-group shuffling to control for algorithmic biases towards identity permutation\n    x_tok_by_group_shuffle = [x[torch.randperm(size_each_group)] for x in x_tok_by_group]\n    x_tok = torch.cat(x_tok_by_group, dim=0)\n    x_tok_shuffle = torch.cat(x_tok_by_group_shuffle, dim=0)\n    y_tok = (x_tok + 1) % n_classes\n    x = torch.nn.functional.one_hot(x_tok).to(torch.get_default_dtype())\n    x_shuffle = torch.nn.functional.one_hot(x_tok_shuffle).to(torch.get_default_dtype())\n    y = torch.nn.functional.one_hot(y_tok).to(torch.get_default_dtype())\n\n    group_sizes = [size_each_group] * n_groups\n\n    # Model\n    model = BestHitsPairing(group_sizes=group_sizes)\n    results = model.fit_bootstrap(x_shuffle, y)\n    target_hard_loss = model.compute_losses_identity_perm(x, y)[\"hard\"]\n\n    # Check that the hard loss of the optimized permutation is close to the ground truth\n    assert results.hard_losses[-2][-1] / target_hard_loss &gt; 0.7\n\ntest_besthits_bootstrap()\n\n\nsource\n\n\n\n\n MirrortreePairing (group_sizes:collections.abc.Sequence[int],\n                    fixed_pairings:Optional[list[list[tuple[int,int]]]]=No\n                    ne, permutation_cfg:Optional[dict[str,Any]]=None, simi\n                    larity_kind:Literal['Hamming','Blosum62']='Hamming',\n                    similarities_cfg:Optional[dict[str,Any]]=None,\n                    similarities_comparison_loss:Optional[&lt;built-\n                    infunctioncallable&gt;]=None)\n\nDiffPaSS model for pairing of multiple sequence alignments (MSAs) by aligning their sequence distance networks as in the Mirrortree method.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ngroup_sizes\nSequence\n\nNumber of sequences in each group (e.g. species) of the two MSAs\n\n\nfixed_pairings\nOptional\nNone\nIf not None, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), …], …] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc.\n\n\npermutation_cfg\nOptional\nNone\nIf not None, configuration dictionary containing init parameters for the internal GeneralizedPermutation object to compute soft/hard permutations\n\n\nsimilarity_kind\nLiteral\nHamming\n(Smoothly extended) similarity metric to use on all pairs of aligned sequences\n\n\nsimilarities_cfg\nOptional\nNone\nIf not None, configuration dictionary containing init parameters for the internal HammingSimilarities or Blosum62Similarities object to compute similarity matrices\n\n\nsimilarities_comparison_loss\nOptional\nNone\nIf not None, custom callable to compute the differentiable loss between the similarity matrix of the two MSAs. Default: IntraGroupSimilarityLoss\n\n\n\n\ndef test_mirrortree_bootstrap():\n    # Data: two highly correlated MSAs\n    n_classes = 3\n    length = 100\n    size_each_group = 10\n    n_groups = 10\n    # Define first MSA group by group\n    x_tok_by_group = [torch.randint(0, n_classes, (size_each_group, length)) for _ in range(n_groups)]\n    # Within-group shuffling to control for algorithmic biases towards identity permutation\n    x_tok_by_group_shuffle = [x[torch.randperm(size_each_group)] for x in x_tok_by_group]\n    x_tok = torch.cat(x_tok_by_group, dim=0)\n    x_tok_shuffle = torch.cat(x_tok_by_group_shuffle, dim=0)\n    y_tok = (x_tok + 1) % n_classes\n    x = torch.nn.functional.one_hot(x_tok).to(torch.get_default_dtype())\n    x_shuffle = torch.nn.functional.one_hot(x_tok_shuffle).to(torch.get_default_dtype())\n    y = torch.nn.functional.one_hot(y_tok).to(torch.get_default_dtype())\n\n    group_sizes = [size_each_group] * n_groups\n\n    # Model\n    model = MirrortreePairing(group_sizes=group_sizes)\n    results = model.fit_bootstrap(x_shuffle, y)\n    target_hard_loss = model.compute_losses_identity_perm(x, y)[\"hard\"]\n\n    # Check that the hard loss of the optimized permutation is close to the ground truth\n    assert results.hard_losses[-2][-1] / target_hard_loss &gt; 0.95\n\ntest_mirrortree_bootstrap()\n\n\nsource\n\n\n\n\n GraphAlignment (group_sizes:collections.abc.Sequence[int],\n                 fixed_pairings:Optional[list[list[tuple[int,int]]]]=None,\n                 permutation_cfg:Optional[dict[str,Any]]=None,\n                 comparison_loss:Optional[&lt;built-\n                 infunctioncallable&gt;]=None)\n\nDiffPaSS model for general graph alignment starting from the weighted adjacency matrices of two graphs.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ngroup_sizes\nSequence\n\nNumber of graph nodes in each group (e.g. species), assumed the same between the two graphs to align\n\n\nfixed_pairings\nOptional\nNone\nIf not None, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), …], …] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc.\n\n\npermutation_cfg\nOptional\nNone\nIf not None, configuration dictionary containing init parameters for the internal GeneralizedPermutation object to compute soft/hard permutations. Soft/hard permutations P act on adjacency matrices X via P @ X @ P.T\n\n\ncomparison_loss\nOptional\nNone\nIf not None, custom callable to compute the differentiable loss between the soft/hard-permuted adjacency matrix of graph x and the adjacency matrix of graph y. Defaults to dot product between all upper triangular elements\n\n\n\n\ndef test_graph_alignment_bootstrap():\n    # Data: two identical weighted adjacency matrices\n    size_each_group = 10\n    n_groups = 10\n    n_samples = size_each_group * n_groups\n    x = torch.exp(torch.randn((n_samples, n_samples))).to(torch.get_default_dtype())\n    y = x.clone()\n    # Within-group shuffling to control for algorithmic biases towards identity permutation\n    rand_perm_mats = []\n    for _ in range(n_groups):\n        rp_mat = torch.zeros(\n            (size_each_group, size_each_group), dtype=x.dtype, device=x.device\n        )\n        rp_mat[torch.arange(size_each_group), torch.randperm(size_each_group)] = 1\n        rand_perm_mats.append(rp_mat)\n    x_shuffle = apply_hard_permutation_batch_to_similarity(x=x, perms=rand_perm_mats)\n\n    group_sizes = [size_each_group] * n_groups\n\n    # Model\n    model = GraphAlignment(group_sizes=group_sizes)\n    results = model.fit_bootstrap(x_shuffle, y)\n    target_hard_loss = model.compute_losses_identity_perm(x, y)[\"hard\"]\n\n    # Check that the hard loss of the optimized permutation is close to the ground truth\n    assert results.hard_losses[-2][-1] / target_hard_loss &gt; 0.95\n\ntest_graph_alignment_bootstrap()",
    "crumbs": [
      "train"
    ]
  },
  {
    "objectID": "train.html#type-aliases",
    "href": "train.html#type-aliases",
    "title": "train",
    "section": "",
    "text": "IndexPair = tuple[int, int]  # Pair of indices\nIndexPairsInGroup = list[IndexPair]  # Pairs of indices in a group of sequences\nIndexPairsInGroups = list[IndexPairsInGroup]  # Pairs of indices in groups of sequences\n\nsource\n\n\n\n InformationPairing (group_sizes:collections.abc.Sequence[int],\n                     fixed_pairings:Optional[list[list[tuple[int,int]]]]=N\n                     one, permutation_cfg:Optional[dict[str,Any]]=None, in\n                     formation_measure:Literal['MI','TwoBodyEntropy']='Two\n                     BodyEntropy')\n\nDiffPaSS model for information-theoretic pairing of multiple sequence alignments (MSAs).\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ngroup_sizes\nSequence\n\nNumber of sequences in each group (e.g. species) of the two MSAs\n\n\nfixed_pairings\nOptional\nNone\nIf not None, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), …], …] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc.\n\n\npermutation_cfg\nOptional\nNone\nIf not None, configuration dictionary containing init parameters for the internal GeneralizedPermutation object to compute soft/hard permutations\n\n\ninformation_measure\nLiteral\nTwoBodyEntropy\nInformation-theoretic measure to use. For hard permutations, these two measures are equivalent\n\n\n\n\ndef test_information_bootstrap():\n    # Data: two highly correlated MSAs\n    n_classes = 3\n    length = 5\n    size_each_group = 10\n    n_groups = 10\n    # Define first MSA group by group\n    x_tok_by_group = [torch.randint(0, n_classes, (size_each_group, length)) for _ in range(n_groups)]\n    # Within-group shuffling to control for algorithmic biases towards identity permutation\n    x_tok_by_group_shuffle = [x[torch.randperm(size_each_group)] for x in x_tok_by_group]\n    x_tok = torch.cat(x_tok_by_group, dim=0)\n    x_tok_shuffle = torch.cat(x_tok_by_group_shuffle, dim=0)\n    y_tok = (x_tok + 1) % n_classes\n    x = torch.nn.functional.one_hot(x_tok).to(torch.get_default_dtype())\n    x_shuffle = torch.nn.functional.one_hot(x_tok_shuffle).to(torch.get_default_dtype())\n    y = torch.nn.functional.one_hot(y_tok).to(torch.get_default_dtype())\n\n    group_sizes = [size_each_group] * n_groups\n\n    # Model\n    model = InformationPairing(group_sizes=group_sizes)\n    results = model.fit_bootstrap(x_shuffle, y)\n    hard_loss_identity_perm = model.compute_losses_identity_perm(x, y)[\"hard\"]\n\n    # Check that the hard loss of the optimized permutation is close to the ground truth\n    assert np.abs(results.hard_losses[-2][-1] - hard_loss_identity_perm) &lt; 1e-4\n\ntest_information_bootstrap()\n\n\nsource\n\n\n\n\n BestHitsPairing (group_sizes:collections.abc.Sequence[int],\n                  fixed_pairings:Optional[list[list[tuple[int,int]]]]=None\n                  , permutation_cfg:Optional[dict[str,Any]]=None,\n                  similarity_kind:Literal['Hamming','Blosum62']='Hamming',\n                  similarities_cfg:Optional[dict[str,Any]]=None,\n                  compute_in_group_best_hits:bool=True,\n                  best_hits_cfg:Optional[dict[str,Any]]=None,\n                  similarities_comparison_loss:Optional[&lt;built-\n                  infunctioncallable&gt;]=None,\n                  compare_soft_best_hits_to_hard:bool=True)\n\nDiffPaSS model for pairing of multiple sequence alignments (MSAs) by aligning their orthology networks, constructed using (reciprocal) best hits .\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ngroup_sizes\nSequence\n\nNumber of sequences in each group (e.g. species) of the two MSAs\n\n\nfixed_pairings\nOptional\nNone\nIf not None, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), …], …] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc.\n\n\npermutation_cfg\nOptional\nNone\nIf not None, configuration dictionary containing init parameters for the internal GeneralizedPermutation object to compute soft/hard permutations\n\n\nsimilarity_kind\nLiteral\nHamming\n(Smoothly extended) similarity metric to use on all pairs of aligned sequences\n\n\nsimilarities_cfg\nOptional\nNone\nIf not None, configuration dictionary containing init parameters for the internal HammingSimilarities or Blosum62Similarities object to compute similarity matrices\n\n\ncompute_in_group_best_hits\nbool\nTrue\nWhether to also compute best hits within each group (in addition to between different groups)\n\n\nbest_hits_cfg\nOptional\nNone\nIf not None, configuration dictionary containing init parameters for the internal BestHits object to compute soft/hard (reciprocal) best hits\n\n\nsimilarities_comparison_loss\nOptional\nNone\nIf not None, custom callable to compute the differentiable loss between the soft/hard best hits matrices of the two MSAs\n\n\ncompare_soft_best_hits_to_hard\nbool\nTrue\nWhether to compare the soft best hits from the MSA to permute (x) to the hard or soft best hits from the reference MSA (y)\n\n\n\n\ndef test_besthits_bootstrap():\n    # Data: two highly correlated MSAs\n    n_classes = 3\n    length = 100\n    size_each_group = 10\n    n_groups = 10\n    # Define first MSA group by group\n    x_tok_by_group = [torch.randint(0, n_classes, (size_each_group, length)) for _ in range(n_groups)]\n    # Within-group shuffling to control for algorithmic biases towards identity permutation\n    x_tok_by_group_shuffle = [x[torch.randperm(size_each_group)] for x in x_tok_by_group]\n    x_tok = torch.cat(x_tok_by_group, dim=0)\n    x_tok_shuffle = torch.cat(x_tok_by_group_shuffle, dim=0)\n    y_tok = (x_tok + 1) % n_classes\n    x = torch.nn.functional.one_hot(x_tok).to(torch.get_default_dtype())\n    x_shuffle = torch.nn.functional.one_hot(x_tok_shuffle).to(torch.get_default_dtype())\n    y = torch.nn.functional.one_hot(y_tok).to(torch.get_default_dtype())\n\n    group_sizes = [size_each_group] * n_groups\n\n    # Model\n    model = BestHitsPairing(group_sizes=group_sizes)\n    results = model.fit_bootstrap(x_shuffle, y)\n    target_hard_loss = model.compute_losses_identity_perm(x, y)[\"hard\"]\n\n    # Check that the hard loss of the optimized permutation is close to the ground truth\n    assert results.hard_losses[-2][-1] / target_hard_loss &gt; 0.7\n\ntest_besthits_bootstrap()\n\n\nsource\n\n\n\n\n MirrortreePairing (group_sizes:collections.abc.Sequence[int],\n                    fixed_pairings:Optional[list[list[tuple[int,int]]]]=No\n                    ne, permutation_cfg:Optional[dict[str,Any]]=None, simi\n                    larity_kind:Literal['Hamming','Blosum62']='Hamming',\n                    similarities_cfg:Optional[dict[str,Any]]=None,\n                    similarities_comparison_loss:Optional[&lt;built-\n                    infunctioncallable&gt;]=None)\n\nDiffPaSS model for pairing of multiple sequence alignments (MSAs) by aligning their sequence distance networks as in the Mirrortree method.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ngroup_sizes\nSequence\n\nNumber of sequences in each group (e.g. species) of the two MSAs\n\n\nfixed_pairings\nOptional\nNone\nIf not None, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), …], …] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc.\n\n\npermutation_cfg\nOptional\nNone\nIf not None, configuration dictionary containing init parameters for the internal GeneralizedPermutation object to compute soft/hard permutations\n\n\nsimilarity_kind\nLiteral\nHamming\n(Smoothly extended) similarity metric to use on all pairs of aligned sequences\n\n\nsimilarities_cfg\nOptional\nNone\nIf not None, configuration dictionary containing init parameters for the internal HammingSimilarities or Blosum62Similarities object to compute similarity matrices\n\n\nsimilarities_comparison_loss\nOptional\nNone\nIf not None, custom callable to compute the differentiable loss between the similarity matrix of the two MSAs. Default: IntraGroupSimilarityLoss\n\n\n\n\ndef test_mirrortree_bootstrap():\n    # Data: two highly correlated MSAs\n    n_classes = 3\n    length = 100\n    size_each_group = 10\n    n_groups = 10\n    # Define first MSA group by group\n    x_tok_by_group = [torch.randint(0, n_classes, (size_each_group, length)) for _ in range(n_groups)]\n    # Within-group shuffling to control for algorithmic biases towards identity permutation\n    x_tok_by_group_shuffle = [x[torch.randperm(size_each_group)] for x in x_tok_by_group]\n    x_tok = torch.cat(x_tok_by_group, dim=0)\n    x_tok_shuffle = torch.cat(x_tok_by_group_shuffle, dim=0)\n    y_tok = (x_tok + 1) % n_classes\n    x = torch.nn.functional.one_hot(x_tok).to(torch.get_default_dtype())\n    x_shuffle = torch.nn.functional.one_hot(x_tok_shuffle).to(torch.get_default_dtype())\n    y = torch.nn.functional.one_hot(y_tok).to(torch.get_default_dtype())\n\n    group_sizes = [size_each_group] * n_groups\n\n    # Model\n    model = MirrortreePairing(group_sizes=group_sizes)\n    results = model.fit_bootstrap(x_shuffle, y)\n    target_hard_loss = model.compute_losses_identity_perm(x, y)[\"hard\"]\n\n    # Check that the hard loss of the optimized permutation is close to the ground truth\n    assert results.hard_losses[-2][-1] / target_hard_loss &gt; 0.95\n\ntest_mirrortree_bootstrap()\n\n\nsource\n\n\n\n\n GraphAlignment (group_sizes:collections.abc.Sequence[int],\n                 fixed_pairings:Optional[list[list[tuple[int,int]]]]=None,\n                 permutation_cfg:Optional[dict[str,Any]]=None,\n                 comparison_loss:Optional[&lt;built-\n                 infunctioncallable&gt;]=None)\n\nDiffPaSS model for general graph alignment starting from the weighted adjacency matrices of two graphs.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ngroup_sizes\nSequence\n\nNumber of graph nodes in each group (e.g. species), assumed the same between the two graphs to align\n\n\nfixed_pairings\nOptional\nNone\nIf not None, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), …], …] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc.\n\n\npermutation_cfg\nOptional\nNone\nIf not None, configuration dictionary containing init parameters for the internal GeneralizedPermutation object to compute soft/hard permutations. Soft/hard permutations P act on adjacency matrices X via P @ X @ P.T\n\n\ncomparison_loss\nOptional\nNone\nIf not None, custom callable to compute the differentiable loss between the soft/hard-permuted adjacency matrix of graph x and the adjacency matrix of graph y. Defaults to dot product between all upper triangular elements\n\n\n\n\ndef test_graph_alignment_bootstrap():\n    # Data: two identical weighted adjacency matrices\n    size_each_group = 10\n    n_groups = 10\n    n_samples = size_each_group * n_groups\n    x = torch.exp(torch.randn((n_samples, n_samples))).to(torch.get_default_dtype())\n    y = x.clone()\n    # Within-group shuffling to control for algorithmic biases towards identity permutation\n    rand_perm_mats = []\n    for _ in range(n_groups):\n        rp_mat = torch.zeros(\n            (size_each_group, size_each_group), dtype=x.dtype, device=x.device\n        )\n        rp_mat[torch.arange(size_each_group), torch.randperm(size_each_group)] = 1\n        rand_perm_mats.append(rp_mat)\n    x_shuffle = apply_hard_permutation_batch_to_similarity(x=x, perms=rand_perm_mats)\n\n    group_sizes = [size_each_group] * n_groups\n\n    # Model\n    model = GraphAlignment(group_sizes=group_sizes)\n    results = model.fit_bootstrap(x_shuffle, y)\n    target_hard_loss = model.compute_losses_identity_perm(x, y)[\"hard\"]\n\n    # Check that the hard loss of the optimized permutation is close to the ground truth\n    assert results.hard_losses[-2][-1] / target_hard_loss &gt; 0.95\n\ntest_graph_alignment_bootstrap()",
    "crumbs": [
      "train"
    ]
  },
  {
    "objectID": "sequence_similarity_ops.html",
    "href": "sequence_similarity_ops.html",
    "title": "sequence_similarity_ops",
    "section": "",
    "text": "source\n\nsmooth_substitution_matrix_similarities_dot\n\n smooth_substitution_matrix_similarities_dot (x:torch.Tensor,\n                                              subs_mat:torch.Tensor,\n                                              use_scoredist:bool=False, ex\n                                              pected_value:Optional[float]\n                                              =None)\n\nTODO.\n\nsource\n\n\nsmooth_substitution_matrix_similarities_cdist\n\n smooth_substitution_matrix_similarities_cdist (x:torch.Tensor,\n                                                subs_mat:torch.Tensor,\n                                                p:float=1.0)\n\nTODO.\n\nsource\n\n\nsmooth_hamming_similarities_dot\n\n smooth_hamming_similarities_dot (x:torch.Tensor)\n\nSmooth extension of the normalized Hamming similarity between all pairs of sequences in x. x must have shape (…, N, L, R), and the result has shape (…, N, N).\n\nsource\n\n\nsmooth_hamming_similarities_cdist\n\n smooth_hamming_similarities_cdist (x:torch.Tensor, p:float=1.0)\n\nSmooth extension of the normalized Hamming similarity between all pairs of sequences in x. x must have shape (…, N, L, R), and the result has shape (…, N, N).\n\nsource\n\n\nsoft_best_hits\n\n soft_best_hits (similarities:torch.Tensor, reciprocal:bool=False,\n                 group_slices:collections.abc.Sequence[slice],\n                 tau:Union[float,torch.Tensor]=0.1)\n\nSoft reciprocal best hits graphs from pairwise similarities. similarities must have shape (…, N, N). The main diagonal is excluded by setting its entries to minus infinity before softmax.\n\nsource\n\n\nhard_best_hits\n\n hard_best_hits (similarities:torch.Tensor, reciprocal:bool=False,\n                 group_slices:collections.abc.Sequence[slice])\n\nHard reciprocal best hits graphs from pairwise similarities. similarities must have shape (…, N, N). The main diagonal is excluded by setting its entries to minus infinity before argmax.\n\ndef test_soft_reciprocal_best_hits_bounds():\n    group_slices = [\n        slice(0, 4), slice(4, 7), slice(7, 10), slice(10, 18), slice(18, 20)\n    ]\n    x = torch.randn(20, 6)\n    similarities = torch.cdist(x, x, p=1)\n    similarities /= similarities.max()\n    srbh = soft_best_hits(similarities, group_slices=group_slices, reciprocal=True)\n\n    assert srbh.shape == similarities.shape\n    assert torch.all(torch.logical_and(srbh &lt;= 1, srbh &gt;= 0))\n\n\ntest_soft_reciprocal_best_hits_bounds()",
    "crumbs": [
      "sequence_similarity_ops"
    ]
  },
  {
    "objectID": "data_utils.html",
    "href": "data_utils.html",
    "title": "data_utils",
    "section": "",
    "text": "SeqRecord: tuple[str, str]\nSeqRecords: list[SeqRecord]\nGroupwiseSeqRecords: dict[str, SeqRecords]\n\nsource\n\n\n\n get_single_and_paired_seqs\n                             (data_group_by_group_x:dict[str,list[tuple[st\n                             r,str]]], data_group_by_group_y:dict[str,list\n                             [tuple[str,str]]], group_names:Optional[colle\n                             ctions.abc.Sequence[str]]=None)\n\nSingle and paired sequences from two sequence records. The paired sequences are returned as a list of dictionaries, where the keys are the concatenated sequences and the values are the number of times that pair appears in the concatenated MSA.\n\nsource\n\n\n\n\n create_groupwise_seq_records (seq_records:list[tuple[str,str]],\n                               group_name_func:&lt;built-infunctioncallable&gt;,\n                               remove_groups_with_one_seq:bool=True)\n\nGroup records of the form (header, sequence) in a collection by group name (e.g. species name), extracted from header information using group_name_func.\n\nsource\n\n\n\n\n remove_groups_not_in_both\n                            (data_group_by_group_x:dict[str,list[tuple[str\n                            ,str]]], data_group_by_group_y:dict[str,list[t\n                            uple[str,str]]])\n\nRemove groups that are not present in both input collections.\n\nsource\n\n\n\n\n pad_msas_with_dummy_sequences\n                                (data_group_by_group_x:dict[str,list[tuple\n                                [str,str]]], data_group_by_group_y:dict[st\n                                r,list[tuple[str,str]]],\n                                dummy_symbol:str='-')\n\nPad MSAs with dummy sequences so that all groups/species contain the same number of sequences.\n\nsource\n\n\n\n\n one_hot_encode_msa (seq_records:list[tuple[str,str]],\n                     aa_to_int:Optional[dict[str,int]]=None,\n                     device:Optional[torch.device]=None)\n\nGiven a list of records of the form (header, sequence), assumed to be a parsed MSA, tokenize each sequence and one-hot encode each token. Return a 3D tensor representing the one-hot encoded MSA.\n\nsource\n\n\n\n\n compute_num_correct_pairings (hard_perms_by_group:list[numpy.ndarray],\n                               compare_to_identity_permutation:bool, singl\n                               e_and_paired_seqs:Optional[dict[str,list]]=\n                               None)\n\n*Compute the total number of correct pairings. ‘Correct’ means that they are present in the original paired MSAs, assumed to be the ground truth.\nIf compare_to_identity_permutation is True, then the correct pairings are assumed to be given by the identity permutation, and the x_seqs, y_seqs, and xy_seqs arguments are ignored.*\n\nsource\n\n\n\n\n compute_comparable_group_idxs (group_sizes_arr:numpy.ndarray,\n                                max_size_ratio:int, max_group_size:int)",
    "crumbs": [
      "data_utils"
    ]
  },
  {
    "objectID": "data_utils.html#type-aliases",
    "href": "data_utils.html#type-aliases",
    "title": "data_utils",
    "section": "",
    "text": "SeqRecord: tuple[str, str]\nSeqRecords: list[SeqRecord]\nGroupwiseSeqRecords: dict[str, SeqRecords]\n\nsource\n\n\n\n get_single_and_paired_seqs\n                             (data_group_by_group_x:dict[str,list[tuple[st\n                             r,str]]], data_group_by_group_y:dict[str,list\n                             [tuple[str,str]]], group_names:Optional[colle\n                             ctions.abc.Sequence[str]]=None)\n\nSingle and paired sequences from two sequence records. The paired sequences are returned as a list of dictionaries, where the keys are the concatenated sequences and the values are the number of times that pair appears in the concatenated MSA.\n\nsource\n\n\n\n\n create_groupwise_seq_records (seq_records:list[tuple[str,str]],\n                               group_name_func:&lt;built-infunctioncallable&gt;,\n                               remove_groups_with_one_seq:bool=True)\n\nGroup records of the form (header, sequence) in a collection by group name (e.g. species name), extracted from header information using group_name_func.\n\nsource\n\n\n\n\n remove_groups_not_in_both\n                            (data_group_by_group_x:dict[str,list[tuple[str\n                            ,str]]], data_group_by_group_y:dict[str,list[t\n                            uple[str,str]]])\n\nRemove groups that are not present in both input collections.\n\nsource\n\n\n\n\n pad_msas_with_dummy_sequences\n                                (data_group_by_group_x:dict[str,list[tuple\n                                [str,str]]], data_group_by_group_y:dict[st\n                                r,list[tuple[str,str]]],\n                                dummy_symbol:str='-')\n\nPad MSAs with dummy sequences so that all groups/species contain the same number of sequences.\n\nsource\n\n\n\n\n one_hot_encode_msa (seq_records:list[tuple[str,str]],\n                     aa_to_int:Optional[dict[str,int]]=None,\n                     device:Optional[torch.device]=None)\n\nGiven a list of records of the form (header, sequence), assumed to be a parsed MSA, tokenize each sequence and one-hot encode each token. Return a 3D tensor representing the one-hot encoded MSA.\n\nsource\n\n\n\n\n compute_num_correct_pairings (hard_perms_by_group:list[numpy.ndarray],\n                               compare_to_identity_permutation:bool, singl\n                               e_and_paired_seqs:Optional[dict[str,list]]=\n                               None)\n\n*Compute the total number of correct pairings. ‘Correct’ means that they are present in the original paired MSAs, assumed to be the ground truth.\nIf compare_to_identity_permutation is True, then the correct pairings are assumed to be given by the identity permutation, and the x_seqs, y_seqs, and xy_seqs arguments are ignored.*\n\nsource\n\n\n\n\n compute_comparable_group_idxs (group_sizes_arr:numpy.ndarray,\n                                max_size_ratio:int, max_group_size:int)",
    "crumbs": [
      "data_utils"
    ]
  },
  {
    "objectID": "ipa_utils.html",
    "href": "ipa_utils.html",
    "title": "ipa_utils",
    "section": "",
    "text": "BootstrapList = list  # List indexed by bootstrap iteration\nGradientDescentList = list  # List indexed by gradient descent iteration\nGroupByGroupList = list  # List indexed by group index\n\nIndexPair = tuple[int, int]  # Pair of indices\nIndexPairsInGroup = list[IndexPair]  # Pairs of indices in a group of sequences\nIndexPairsInGroups = list[IndexPairsInGroup]  # Pairs of indices in groups of sequences\n\nsource\n\n\n\n get_robust_pairs (bootstrap_results:diffpass.base.DiffPaSSResults,\n                   cutoff:float=1.0)\n\nGet robust pairs of indices from a DiffPaSSResults object.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nbootstrap_results\nDiffPaSSResults\n\nE.g. results of a run of DiffPaSSModel.fit_bootstrap\n\n\ncutoff\nfloat\n1.0\nFraction of iterations a pair must be present in to be considered robust\n\n\nReturns\nlist\n\nRobust pairs of indices in each group of sequences",
    "crumbs": [
      "ipa_utils"
    ]
  },
  {
    "objectID": "ipa_utils.html#type-aliases",
    "href": "ipa_utils.html#type-aliases",
    "title": "ipa_utils",
    "section": "",
    "text": "BootstrapList = list  # List indexed by bootstrap iteration\nGradientDescentList = list  # List indexed by gradient descent iteration\nGroupByGroupList = list  # List indexed by group index\n\nIndexPair = tuple[int, int]  # Pair of indices\nIndexPairsInGroup = list[IndexPair]  # Pairs of indices in a group of sequences\nIndexPairsInGroups = list[IndexPairsInGroup]  # Pairs of indices in groups of sequences\n\nsource\n\n\n\n get_robust_pairs (bootstrap_results:diffpass.base.DiffPaSSResults,\n                   cutoff:float=1.0)\n\nGet robust pairs of indices from a DiffPaSSResults object.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nbootstrap_results\nDiffPaSSResults\n\nE.g. results of a run of DiffPaSSModel.fit_bootstrap\n\n\ncutoff\nfloat\n1.0\nFraction of iterations a pair must be present in to be considered robust\n\n\nReturns\nlist\n\nRobust pairs of indices in each group of sequences",
    "crumbs": [
      "ipa_utils"
    ]
  }
]