{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base\n",
    "\n",
    "> DiffPaSS base classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Stdlib imports\n",
    "from copy import deepcopy\n",
    "from typing import Optional, Any, Sequence, Union\n",
    "from dataclasses import fields, dataclass, replace\n",
    "\n",
    "# Progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "\n",
    "# DiffPaSS imports\n",
    "from diffpass.model import (\n",
    "    GeneralizedPermutation,\n",
    "    Blosum62Similarities,\n",
    "    HammingSimilarities,\n",
    "    BestHits,\n",
    ")\n",
    "\n",
    "# Constants\n",
    "INGROUP_IDX_DTYPE = np.int16\n",
    "\n",
    "# Type aliases\n",
    "BootstrapList = list  # List indexed by bootstrap iteration\n",
    "GradientDescentList = list  # List indexed by gradient descent iteration\n",
    "GroupByGroupList = list  # List indexed by group index\n",
    "\n",
    "IndexPair = tuple[int, int]  # Pair of indices\n",
    "IndexPairsInGroup = list[IndexPair]  # Pairs of indices in a group of sequences\n",
    "IndexPairsInGroups = list[IndexPairsInGroup]  # Pairs of indices in groups of sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type aliases\n",
    "```python\n",
    "BootstrapList = list  # List indexed by bootstrap iteration\n",
    "GradientDescentList = list  # List indexed by gradient descent iteration\n",
    "GroupByGroupList = list  # List indexed by group index\n",
    "\n",
    "IndexPair = tuple[int, int]  # Pair of indices\n",
    "IndexPairsInGroup = list[IndexPair]  # Pairs of indices in a group of sequences\n",
    "IndexPairsInGroups = list[IndexPairsInGroup]  # Pairs of indices in groups of sequences\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def dccn(x: torch.Tensor) -> np.ndarray:\n",
    "    return x.detach().clone().cpu().numpy()\n",
    "\n",
    "\n",
    "def make_pbar(epochs: int, show_pbar: bool) -> Any:\n",
    "    if show_pbar:\n",
    "        return tqdm(range(epochs + 1))\n",
    "    return range(epochs + 1)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DiffPaSSResults:\n",
    "    \"\"\"Container for results of DiffPaSS fits.\"\"\"\n",
    "\n",
    "    # Optionally, log-alphas for fine-grained information\n",
    "    log_alphas: Optional[\n",
    "        Union[\n",
    "            GradientDescentList[GroupByGroupList[np.ndarray]],\n",
    "            BootstrapList[GradientDescentList[GroupByGroupList[np.ndarray]]],\n",
    "        ]\n",
    "    ]\n",
    "    # Soft permutations\n",
    "    soft_perms: Optional[\n",
    "        Union[\n",
    "            GradientDescentList[GroupByGroupList[np.ndarray]],\n",
    "            BootstrapList[GradientDescentList[GroupByGroupList[np.ndarray]]],\n",
    "        ]\n",
    "    ]\n",
    "    # Hard permutations\n",
    "    hard_perms: Union[\n",
    "        GradientDescentList[GroupByGroupList[np.ndarray]],\n",
    "        BootstrapList[GradientDescentList[GroupByGroupList[np.ndarray]]],\n",
    "    ]\n",
    "    # Hard losses\n",
    "    hard_losses: Union[\n",
    "        GradientDescentList[GroupByGroupList[np.ndarray]],\n",
    "        BootstrapList[GradientDescentList[GroupByGroupList[np.ndarray]]],\n",
    "    ]\n",
    "    # Soft losses\n",
    "    soft_losses: Optional[\n",
    "        Union[\n",
    "            GradientDescentList[GroupByGroupList[np.ndarray]],\n",
    "            BootstrapList[GradientDescentList[GroupByGroupList[np.ndarray]]],\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "\n",
    "class DiffPaSSModel(Module):\n",
    "    \"\"\"Base class for DiffPaSS models.\"\"\"\n",
    "\n",
    "    allowed_permutation_cfg_keys = {\n",
    "        \"tau\",\n",
    "        \"n_iter\",\n",
    "        \"noise\",\n",
    "        \"noise_factor\",\n",
    "        \"noise_std\",\n",
    "    }\n",
    "    allowed_information_measures = {\"MI\", \"TwoBodyEntropy\"}\n",
    "    allowed_similarity_kinds = {\"Hamming\", \"Blosum62\"}\n",
    "    allowed_similarities_cfg_keys = {\n",
    "        \"Hamming\": {\"use_dot\", \"p\"},\n",
    "        \"Blosum62\": {\"use_dot\", \"p\", \"use_scoredist\", \"aa_to_int\", \"gaps_as_stars\"},\n",
    "    }\n",
    "    allowed_best_hits_cfg_keys = {\"tau\", \"reciprocal\"}\n",
    "\n",
    "    group_sizes: Sequence[int]\n",
    "    fixed_pairings: Optional[IndexPairsInGroups]\n",
    "    permutation_cfg: Optional[dict[str, Any]]\n",
    "    effective_permutation_cfg_: dict[str, Any]\n",
    "    information_measure: str\n",
    "    similarity_kind: str\n",
    "    similarities_cfg: Optional[dict[str, Any]]\n",
    "    effective_similarities_cfg_: dict[str, Any]\n",
    "    permutation: GeneralizedPermutation\n",
    "    similarities: Union[Blosum62Similarities, HammingSimilarities]\n",
    "    compute_in_group_best_hits: bool\n",
    "    best_hits_cfg: Optional[dict[str, Any]]\n",
    "    effective_best_hits_cfg_: dict[str, Any]\n",
    "    best_hits: BestHits\n",
    "\n",
    "    single_fit_default_cfg = {\n",
    "        \"epochs\": 1,\n",
    "        \"optimizer_name\": \"SGD\",\n",
    "        \"optimizer_kwargs\": None,\n",
    "        \"mean_centering\": False,\n",
    "        \"show_pbar\": False,\n",
    "        \"compute_final_soft\": False,\n",
    "        \"record_log_alphas\": False,\n",
    "        \"record_soft_perms\": False,\n",
    "        \"record_soft_losses\": False,\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_num_tokens(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Reduce the number of tokens in a one-hot encoded tensor.\"\"\"\n",
    "        used_tokens = x.clone()\n",
    "        for _ in range(x.ndim - 1):\n",
    "            used_tokens = used_tokens.any(-2)\n",
    "\n",
    "        return x[..., used_tokens]\n",
    "\n",
    "    def validate_permutation_cfg(self, permutation_cfg: Optional[dict]) -> None:\n",
    "        if permutation_cfg is None:\n",
    "            return\n",
    "        if not set(permutation_cfg).issubset(self.allowed_permutation_cfg_keys):\n",
    "            raise ValueError(\n",
    "                f\"Invalid keys in `permutation_cfg`: \"\n",
    "                f\"{set(permutation_cfg) - self.allowed_permutation_cfg_keys}\"\n",
    "            )\n",
    "\n",
    "    def validate_information_measure(self, information_measure: str) -> None:\n",
    "        if information_measure not in self.allowed_information_measures:\n",
    "            raise ValueError(\n",
    "                f\"Invalid information measure: {self.information_measure}. \"\n",
    "                f\"Allowed values are: {self.allowed_information_measures}\"\n",
    "            )\n",
    "\n",
    "    def validate_similarity_kind(self, similarity_kind: str) -> None:\n",
    "        if similarity_kind not in self.allowed_similarity_kinds:\n",
    "            raise ValueError(\n",
    "                f\"Invalid similarity kind: {self.similarity_kind}. \"\n",
    "                f\"Allowed values are: {self.allowed_similarity_kinds}\"\n",
    "            )\n",
    "\n",
    "    def validate_similarities_cfg(self, similarities_cfg: Optional[dict]) -> None:\n",
    "        if similarities_cfg is None:\n",
    "            return\n",
    "        if not set(similarities_cfg).issubset(\n",
    "            self.allowed_similarities_cfg_keys[self.similarity_kind]\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"Invalid keys in `similarities_cfg`: \"\n",
    "                f\"{set(similarities_cfg) - self.allowed_similarities_cfg_keys[self.similarity_kind]}\"\n",
    "            )\n",
    "\n",
    "    def validate_best_hits_cfg(self, best_hits_cfg: Optional[dict]) -> None:\n",
    "        if best_hits_cfg is None:\n",
    "            return\n",
    "        if not set(best_hits_cfg).issubset(self.allowed_best_hits_cfg_keys):\n",
    "            raise ValueError(\n",
    "                f\"Invalid keys in `best_hits_cfg`: \"\n",
    "                f\"{set(best_hits_cfg) - self.allowed_best_hits_cfg_keys}\"\n",
    "            )\n",
    "\n",
    "    def init_permutation(\n",
    "        self,\n",
    "        group_sizes: Sequence[int],\n",
    "        fixed_pairings: Optional[IndexPairsInGroups] = None,\n",
    "        permutation_cfg: Optional[dict[str, Any]] = None,\n",
    "    ) -> None:\n",
    "        self.group_sizes = tuple(s for s in group_sizes)\n",
    "        self.fixed_pairings = fixed_pairings\n",
    "        self.permutation_cfg = permutation_cfg\n",
    "\n",
    "        # Validate GeneralizedPermutation config\n",
    "        self.validate_permutation_cfg(permutation_cfg)\n",
    "        if self.permutation_cfg is None:\n",
    "            self.effective_permutation_cfg_ = {}\n",
    "        else:\n",
    "            self.effective_permutation_cfg_ = deepcopy(self.permutation_cfg)\n",
    "\n",
    "        self.permutation = GeneralizedPermutation(\n",
    "            group_sizes=self.group_sizes,\n",
    "            fixed_pairings=self.fixed_pairings,\n",
    "            mode=\"soft\",\n",
    "            **self.effective_permutation_cfg_,\n",
    "        )\n",
    "\n",
    "    def init_similarities(\n",
    "        self, similarity_kind: str, similarities_cfg: Optional[dict[str, Any]] = None\n",
    "    ) -> None:\n",
    "        self.validate_similarity_kind(similarity_kind)\n",
    "        self.similarity_kind = similarity_kind\n",
    "        self.validate_similarities_cfg(similarities_cfg)\n",
    "        self.similarities_cfg = similarities_cfg\n",
    "        if self.similarities_cfg is None:\n",
    "            self.effective_similarities_cfg_ = {}\n",
    "        else:\n",
    "            self.effective_similarities_cfg_ = deepcopy(self.similarities_cfg)\n",
    "        if similarity_kind == \"Blosum62\":\n",
    "            self.similarities = Blosum62Similarities(**self.effective_similarities_cfg_)\n",
    "        elif similarity_kind == \"Hamming\":\n",
    "            self.similarities = HammingSimilarities(**self.effective_similarities_cfg_)\n",
    "\n",
    "    def init_best_hits(self, best_hits_cfg: Optional[dict[str, Any]] = None) -> None:\n",
    "        self.validate_best_hits_cfg(best_hits_cfg)\n",
    "        self.best_hits_cfg = best_hits_cfg\n",
    "        if self.best_hits_cfg is None:\n",
    "            self.effective_best_hits_cfg_ = {}\n",
    "        else:\n",
    "            self.effective_best_hits_cfg_ = deepcopy(self.best_hits_cfg)\n",
    "        self.best_hits = BestHits(\n",
    "            group_sizes=self.group_sizes if self.compute_in_group_best_hits else None,\n",
    "            mode=\"soft\",\n",
    "            **self.effective_best_hits_cfg_,\n",
    "        )\n",
    "\n",
    "    def validate_inputs(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        *,\n",
    "        check_same_alphabet_size: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Validate input tensors representing aligned objects or (dis)similarity matrices.\"\"\"\n",
    "        size_x, size_y = x.shape[0], y.shape[0]\n",
    "        if size_x != size_y:\n",
    "            raise ValueError(f\"Size mismatch between x ({size_x}) and y ({size_y}).\")\n",
    "\n",
    "        if self.are_inputs_msas:\n",
    "            if x.ndim != 3 or y.ndim != 3:\n",
    "                raise ValueError(\n",
    "                    \"Inputs must be 3D tensors of shape (n_samples, length, alphabet_size).\"\n",
    "                )\n",
    "            _, alphabet_size_x = x.shape[1:]\n",
    "            _, alphabet_size_y = y.shape[1:]\n",
    "            if check_same_alphabet_size and (alphabet_size_x != alphabet_size_y):\n",
    "                raise ValueError(\"Inputs must have the same alphabet size.\")\n",
    "        elif x.ndim != 2 or y.ndim != 2:\n",
    "            raise ValueError(\n",
    "                \"Inputs must be 2D square tensors of shape (n_samples, n_samples).\"\n",
    "            )\n",
    "        elif x.shape[1] != size_x or y.shape[1] != size_y:\n",
    "            raise ValueError(\n",
    "                \"Inputs must be square tensors of shape (n_samples, n_samples).\"\n",
    "            )\n",
    "\n",
    "        # Validate group_sizes attribute\n",
    "        n_samples = sum(self.group_sizes)\n",
    "        if size_x != n_samples:\n",
    "            raise ValueError(\n",
    "                f\"Inputs have {n_samples} samples but `group_sizes` implies a total \"\n",
    "                f\"of {n_samples} samples.\"\n",
    "            )\n",
    "\n",
    "    def create_optimizer(\n",
    "        self,\n",
    "        optimizer_name: Optional[str] = single_fit_default_cfg[\"optimizer_name\"],\n",
    "        optimizer_kwargs: Optional[dict[str, Any]] = single_fit_default_cfg[\n",
    "            \"optimizer_kwargs\"\n",
    "        ],\n",
    "    ) -> torch.optim.Optimizer:\n",
    "        optimizer_cls = getattr(torch.optim, optimizer_name)\n",
    "        optimizer_kwargs = (\n",
    "            {\"lr\": 1e-1} if optimizer_kwargs is None else deepcopy(optimizer_kwargs)\n",
    "        )\n",
    "        optimizer = optimizer_cls(self.parameters(), **optimizer_kwargs)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def soft_(self) -> None:\n",
    "        # Iterate through all child modules and call their soft_ method\n",
    "        for module in self.children():\n",
    "            if hasattr(module, \"soft_\"):\n",
    "                module.soft_()\n",
    "\n",
    "    def hard_(self) -> None:\n",
    "        # Iterate through all child modules and call their hard_ method\n",
    "        for module in self.children():\n",
    "            if hasattr(module, \"hard_\"):\n",
    "                module.hard_()\n",
    "\n",
    "    def _hard_pass(\n",
    "        self, x: torch.Tensor, y: Optional[torch.Tensor], *, results: DiffPaSSResults\n",
    "    ) -> None:\n",
    "        self.hard_()\n",
    "        with torch.no_grad():\n",
    "            out = self(x, y)\n",
    "            perms = out[\"perms\"]\n",
    "            loss = out[\"loss\"]\n",
    "            results.hard_perms.append(\n",
    "                [\n",
    "                    dccn(perms_this_group).argmax(axis=-1).astype(INGROUP_IDX_DTYPE)\n",
    "                    for perms_this_group in perms\n",
    "                ]\n",
    "            )\n",
    "            results.hard_losses.append(dccn(loss))\n",
    "\n",
    "    def _soft_pass(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        *,\n",
    "        results: DiffPaSSResults,\n",
    "        record_soft_perms: bool = False,\n",
    "        record_soft_losses: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        self.soft_()\n",
    "        out = self(x, y)\n",
    "        perms = out[\"perms\"]\n",
    "        loss = out[\"loss\"]\n",
    "        if record_soft_perms:\n",
    "            results.soft_perms.append(\n",
    "                [dccn(perms_this_group) for perms_this_group in perms]\n",
    "            )\n",
    "        if record_soft_losses:\n",
    "            results.soft_losses.append(dccn(loss))\n",
    "\n",
    "        # Compute total loss\n",
    "        loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _record_current_log_alphas(self, results: DiffPaSSResults) -> None:\n",
    "        results.log_alphas.append(\n",
    "            [dccn(log_alpha) for log_alpha in self.permutation.log_alphas]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_results(\n",
    "        *,\n",
    "        record_log_alphas: bool = single_fit_default_cfg[\"record_log_alphas\"],\n",
    "        record_soft_perms: bool = single_fit_default_cfg[\"record_soft_perms\"],\n",
    "        record_soft_losses: bool = single_fit_default_cfg[\"record_soft_losses\"],\n",
    "    ) -> DiffPaSSResults:\n",
    "        \"\"\"Initialize DiffPaSSResults object.\"\"\"\n",
    "        results = DiffPaSSResults(\n",
    "            log_alphas=[] if record_log_alphas else None,\n",
    "            soft_perms=[] if record_soft_perms else None,\n",
    "            hard_perms=[],\n",
    "            soft_losses=[] if record_soft_losses else None,\n",
    "            hard_losses=[],\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def check_can_optimize(self) -> bool:\n",
    "        n_samples = sum(self.group_sizes)\n",
    "        n_effectively_fixed = self.permutation._total_number_fixed_pairings\n",
    "\n",
    "        return n_effectively_fixed < n_samples\n",
    "\n",
    "    def mean_center_log_alphas(self) -> None:\n",
    "        with torch.no_grad():\n",
    "            for log_alpha in self.permutation.log_alphas:\n",
    "                log_alpha[...] -= log_alpha.mean(dim=(-1, -2), keepdim=True)\n",
    "\n",
    "    def _fit(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        *,\n",
    "        results: DiffPaSSResults,\n",
    "        epochs: int = single_fit_default_cfg[\"epochs\"],\n",
    "        optimizer_name: Optional[str] = single_fit_default_cfg[\"optimizer_name\"],\n",
    "        optimizer_kwargs: Optional[dict[str, Any]] = single_fit_default_cfg[\n",
    "            \"optimizer_kwargs\"\n",
    "        ],\n",
    "        mean_centering: bool = single_fit_default_cfg[\"mean_centering\"],\n",
    "        show_pbar: bool = single_fit_default_cfg[\"show_pbar\"],\n",
    "        compute_final_soft: bool = single_fit_default_cfg[\"compute_final_soft\"],\n",
    "        record_log_alphas: bool = single_fit_default_cfg[\"record_log_alphas\"],\n",
    "        record_soft_perms: bool = single_fit_default_cfg[\"record_soft_perms\"],\n",
    "        record_soft_losses: bool = single_fit_default_cfg[\"record_soft_losses\"],\n",
    "    ) -> bool:\n",
    "        can_optimize = self.check_can_optimize()\n",
    "        if can_optimize:\n",
    "            # Initialize optimizer\n",
    "            optimizer = self.create_optimizer(optimizer_name, optimizer_kwargs)\n",
    "\n",
    "            # ------------------------------------------------------------------------------------------\n",
    "            ## Gradient descent\n",
    "            # ------------------------------------------------------------------------------------------\n",
    "            pbar = make_pbar(epochs, show_pbar)\n",
    "            for i in pbar:\n",
    "                # Record current log_alphas\n",
    "                if record_log_alphas:\n",
    "                    self._record_current_log_alphas(results)\n",
    "\n",
    "                # Hard pass\n",
    "                self._hard_pass(x, y, results=results)\n",
    "\n",
    "                # Soft pass and backward step\n",
    "                if i < epochs:\n",
    "                    loss = self._soft_pass(\n",
    "                        x,\n",
    "                        y,\n",
    "                        results=results,\n",
    "                        record_soft_perms=record_soft_perms,\n",
    "                        record_soft_losses=record_soft_losses,\n",
    "                    )\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    if mean_centering:\n",
    "                        self.mean_center_log_alphas()\n",
    "                elif compute_final_soft:\n",
    "                    with torch.no_grad():\n",
    "                        self._soft_pass(\n",
    "                            x,\n",
    "                            y,\n",
    "                            results=results,\n",
    "                            record_soft_perms=record_soft_perms,\n",
    "                            record_soft_losses=record_soft_losses,\n",
    "                        )\n",
    "        else:\n",
    "            # Just optionally record current log_alphas and do a single hard pass\n",
    "            if record_log_alphas:\n",
    "                self._record_current_log_alphas(results)\n",
    "            self._hard_pass(x, y, results=results)\n",
    "\n",
    "        return can_optimize\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        x: torch.Tensor,  # The object (MSA or adjacency matrix of graphs) to be permuted\n",
    "        y: torch.Tensor,  # The target object (MSA or adjacency matrix of graphs), that the objects represented by `x` should be paired with. Not acted upon by soft/hard permutations\n",
    "        *,\n",
    "        epochs: int = single_fit_default_cfg[\n",
    "            \"epochs\"\n",
    "        ],  # Number of gradient descent steps\n",
    "        optimizer_name: Optional[str] = single_fit_default_cfg[\n",
    "            \"optimizer_name\"\n",
    "        ],  # If not ``None``, name of the optimizer. Default: ``\"SGD\"``\n",
    "        optimizer_kwargs: Optional[dict[str, Any]] = single_fit_default_cfg[\n",
    "            \"optimizer_kwargs\"\n",
    "        ],  # If not ``None``, keyword arguments for the optimizer. Default: ``None``\n",
    "        mean_centering: bool = single_fit_default_cfg[\n",
    "            \"mean_centering\"\n",
    "        ],  # If ``True``, mean-center log-alphas (stopping gradients) after each gradient descent step. Default: ``False``\n",
    "        show_pbar: bool = single_fit_default_cfg[\n",
    "            \"show_pbar\"\n",
    "        ],  # If ``True``, show progress bar. Default: ``False``\n",
    "        compute_final_soft: bool = single_fit_default_cfg[\n",
    "            \"compute_final_soft\"\n",
    "        ],  # If ``True``, compute soft permutations and losses after the last gradient descent step. Default: ``False``\n",
    "        record_log_alphas: bool = single_fit_default_cfg[\n",
    "            \"record_log_alphas\"\n",
    "        ],  # If ``True``, record log-alphas at each gradient descent step. Default: ``False``\n",
    "        record_soft_perms: bool = single_fit_default_cfg[\n",
    "            \"record_soft_perms\"\n",
    "        ],  # If ``True``, record soft permutations at each gradient descent step. Default: ``False``\n",
    "        record_soft_losses: bool = single_fit_default_cfg[\n",
    "            \"record_soft_losses\"\n",
    "        ],  # If ``True``, record soft losses at each gradient descent step. Default: ``False``\n",
    "    ) -> (\n",
    "        DiffPaSSResults\n",
    "    ):  # `DiffPaSSResults` container for fit results. All attributes are lists indexed by gradient descent iteration\n",
    "        \"\"\"Fit permutations to data using gradient descent.\"\"\"\n",
    "        self.prepare_fit(x, y)\n",
    "\n",
    "        # Initialize DiffPaSSResults object\n",
    "        results = self._init_results(\n",
    "            record_log_alphas=record_log_alphas,\n",
    "            record_soft_perms=record_soft_perms,\n",
    "            record_soft_losses=record_soft_losses,\n",
    "        )\n",
    "\n",
    "        self._fit(\n",
    "            x,\n",
    "            y,\n",
    "            results=results,\n",
    "            epochs=epochs,\n",
    "            optimizer_name=optimizer_name,\n",
    "            optimizer_kwargs=optimizer_kwargs,\n",
    "            mean_centering=mean_centering,\n",
    "            show_pbar=show_pbar,\n",
    "            compute_final_soft=compute_final_soft,\n",
    "            record_log_alphas=record_log_alphas,\n",
    "            record_soft_perms=record_soft_perms,\n",
    "            record_soft_losses=record_soft_losses,\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def fit_bootstrap(\n",
    "        self,\n",
    "        x: torch.Tensor,  # The object (MSA or adjacency matrix of graphs) to be permuted\n",
    "        y: torch.Tensor,  # The target object (MSA or adjacency matrix of graphs), that the objects represented by `x` should be paired with. Not acted upon by soft/hard permutations\n",
    "        *,\n",
    "        n_start: int = 1,  # Number of fixed pairings to choose among the pairs not already fixed by `self.fixed_pairings`, using the results of the first call to `fit`\n",
    "        n_end: Optional[\n",
    "            int\n",
    "        ] = None,  # If ``None``, the bootstrap will end when all pairs are fixed. Otherwise, the bootstrap will end when `n_end` pairs are fixed\n",
    "        step_size: int = 1,  # Difference between the number of fixed pairings chosen at consecutive bootstrap iterations\n",
    "        show_pbar: bool = True,  # If ``True``, show progress bar. Default: ``True``\n",
    "        single_fit_cfg: Optional[\n",
    "            dict\n",
    "        ] = None,  # If not ``None``, configuration dictionary for gradient optimization in each bootstrap iteration (call to `fit`). See `fit` for details\n",
    "    ) -> (\n",
    "        DiffPaSSResults\n",
    "    ):  # `DiffPaSSResults` container for fit results. All attributes are lists indexed by bootstrap iteration, containing lists indexed by gradient descent iteration as per `fit`\n",
    "        \"\"\"Fit permutations to data using the DiffPaSS bootstrap.\n",
    "\n",
    "        The DiffPaSS bootstrap consists of a sequence of short gradient descent runs (default: one epoch per run).\n",
    "        At the end of each run, a subset of the found pairings is chosen uniformly at random\n",
    "        and fixed for the next run.\n",
    "        The number of pairings fixed at each iteration ranges between `n_start` (default: 1) and `n_end` (default: total number of pairs), with a step size of `step_size`.\n",
    "        \"\"\"\n",
    "        self.prepare_fit(x, y)\n",
    "        if self.fixed_pairings is None:\n",
    "            initial_fixed_pairings = [[] for _ in self.group_sizes]\n",
    "        else:\n",
    "            initial_fixed_pairings = [list(fm) for fm in self.fixed_pairings]\n",
    "\n",
    "        _single_fit_cfg = deepcopy(self.single_fit_default_cfg)\n",
    "        if single_fit_cfg is not None:\n",
    "            _single_fit_cfg.update(single_fit_cfg)\n",
    "        single_fit_cfg = _single_fit_cfg\n",
    "\n",
    "        # Initialize DiffPaSSResults object\n",
    "        results = self._init_results(\n",
    "            record_log_alphas=single_fit_cfg[\"record_log_alphas\"],\n",
    "            record_soft_perms=single_fit_cfg[\"record_soft_perms\"],\n",
    "            record_soft_losses=single_fit_cfg[\"record_soft_losses\"],\n",
    "        )\n",
    "        available_fields = [\n",
    "            field.name\n",
    "            for field in fields(results)\n",
    "            if getattr(results, field.name) is not None\n",
    "        ]\n",
    "        field_to_length_so_far = {field_name: 0 for field_name in available_fields}\n",
    "\n",
    "        n_samples = len(x)\n",
    "        n_groups = len(self.group_sizes)\n",
    "        cumsum_group_sizes = np.cumsum([0] + list(self.group_sizes))\n",
    "        offsets = np.repeat(cumsum_group_sizes[:-1], repeats=self.group_sizes)\n",
    "        group_idxs = np.repeat(np.arange(n_groups), repeats=self.group_sizes)\n",
    "\n",
    "        # First fit with initial fixed matchings\n",
    "        can_optimize = self._fit(x, y, results=results, **single_fit_cfg)\n",
    "\n",
    "        # Find effective initial fixed matchings\n",
    "        effective_initial_fixed_idxs = []\n",
    "        for s, efmz in zip(\n",
    "            cumsum_group_sizes, self.permutation._effective_fixed_pairings_zip\n",
    "        ):\n",
    "            if efmz:\n",
    "                effective_initial_fixed_idxs += [\n",
    "                    (s + efmz_fixed) for efmz_fixed in efmz[1]\n",
    "                ]\n",
    "        effective_initial_fixed_idxs = np.asarray(effective_initial_fixed_idxs)\n",
    "        nonfixed_idxs = np.setdiff1d(np.arange(n_samples), effective_initial_fixed_idxs)\n",
    "        n_effective_initial_fixed_pairings = len(effective_initial_fixed_idxs)\n",
    "\n",
    "        if n_end is None:\n",
    "            n_end = n_samples - n_effective_initial_fixed_pairings - 1\n",
    "\n",
    "        # Subsequent fits: at a given iteration we use fixed matchings chosen uniformly at\n",
    "        # random from the results of the previous iteration (excluding the effective initial\n",
    "        # fixed matchings)\n",
    "        pbar = list(range(n_start, n_end, step_size))\n",
    "        pbar = tqdm(pbar) if show_pbar else pbar\n",
    "        n_iters_with_optimization = int(can_optimize)\n",
    "        for N in pbar:\n",
    "            latest_hard_perms = results.hard_perms[-1]\n",
    "            mapped_idxs = offsets + np.concatenate(latest_hard_perms)\n",
    "            rand_fixed_idxs = np.random.permutation(nonfixed_idxs)[:N]\n",
    "            rand_fixed_idxs = np.sort(rand_fixed_idxs)\n",
    "            rand_mapped_idxs = mapped_idxs[rand_fixed_idxs]\n",
    "            rand_group_idxs = group_idxs[rand_fixed_idxs]\n",
    "            rand_fixed_rel_idxs = rand_fixed_idxs - offsets[rand_fixed_idxs]\n",
    "            rand_mapped_rel_idxs = rand_mapped_idxs - offsets[rand_mapped_idxs]\n",
    "\n",
    "            # Update fixed matchings\n",
    "            fixed_pairings = [[] for _ in range(n_groups)]\n",
    "            for rand_group_idx, mapped_rel_idx, fixed_rel_idx in zip(\n",
    "                rand_group_idxs, rand_mapped_rel_idxs, rand_fixed_rel_idxs\n",
    "            ):\n",
    "                pair = (mapped_rel_idx, fixed_rel_idx)\n",
    "                fixed_pairings[rand_group_idx].append(pair)\n",
    "            fixed_pairings = [\n",
    "                initial_fixed_pairings[k] + fixed_pairings[k] for k in range(n_groups)\n",
    "            ]\n",
    "            self.permutation.init_fixed_pairings_and_log_alphas(\n",
    "                fixed_pairings, device=x.device\n",
    "            )\n",
    "\n",
    "            field_to_length_so_far = {\n",
    "                field_name: len(getattr(results, field_name))\n",
    "                for field_name in available_fields\n",
    "            }\n",
    "            can_optimize = self._fit(x, y, results=results, **single_fit_cfg)\n",
    "            if can_optimize:\n",
    "                n_iters_with_optimization += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Reshape results according to number of iterations performed\n",
    "        reshaped_fields = {}\n",
    "        for field_name in available_fields:\n",
    "            results_this_field = getattr(results, field_name)\n",
    "            n_optimized_results_this_field = (\n",
    "                len(results_this_field)\n",
    "                if can_optimize\n",
    "                else field_to_length_so_far[field_name]\n",
    "            )\n",
    "            n_unoptimized_results_this_field = (\n",
    "                len(results_this_field) - n_optimized_results_this_field\n",
    "            )\n",
    "\n",
    "            assert not n_optimized_results_this_field % n_iters_with_optimization\n",
    "            n_in_each_optimized_iter = (\n",
    "                n_optimized_results_this_field // n_iters_with_optimization\n",
    "            )\n",
    "            reshaped_fields[field_name] = [\n",
    "                results_this_field[\n",
    "                    j * n_in_each_optimized_iter : (j + 1) * n_in_each_optimized_iter\n",
    "                ]\n",
    "                for j in range(n_iters_with_optimization)\n",
    "            ] + [results_this_field[n_optimized_results_this_field:]] * bool(\n",
    "                n_unoptimized_results_this_field\n",
    "            )\n",
    "        results = replace(results, **reshaped_fields)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/base.py#L57){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### DiffPaSSResults\n\n>      DiffPaSSResults (log_alphas:Union[list[list[numpy.ndarray]],list[list[lis\n>                       t[numpy.ndarray]]],NoneType], soft_perms:Union[list[list\n>                       [numpy.ndarray]],list[list[list[numpy.ndarray]]],NoneTyp\n>                       e], hard_perms:Union[list[list[numpy.ndarray]],list[list\n>                       [list[numpy.ndarray]]]], hard_losses:Union[list[list[num\n>                       py.ndarray]],list[list[list[numpy.ndarray]]]], soft_loss\n>                       es:Union[list[list[numpy.ndarray]],list[list[list[numpy.\n>                       ndarray]]],NoneType])\n\n*Container for results of DiffPaSS fits.*",
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/base.py#L57){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DiffPaSSResults\n",
       "\n",
       ">      DiffPaSSResults (log_alphas:Union[list[list[numpy.ndarray]],list[list[lis\n",
       ">                       t[numpy.ndarray]]],NoneType], soft_perms:Union[list[list\n",
       ">                       [numpy.ndarray]],list[list[list[numpy.ndarray]]],NoneTyp\n",
       ">                       e], hard_perms:Union[list[list[numpy.ndarray]],list[list\n",
       ">                       [list[numpy.ndarray]]]], hard_losses:Union[list[list[num\n",
       ">                       py.ndarray]],list[list[list[numpy.ndarray]]]], soft_loss\n",
       ">                       es:Union[list[list[numpy.ndarray]],list[list[list[numpy.\n",
       ">                       ndarray]]],NoneType])\n",
       "\n",
       "*Container for results of DiffPaSS fits.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DiffPaSSResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/base.py#L91){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### DiffPaSSModel\n\n>      DiffPaSSModel (*args, **kwargs)\n\n*Base class for DiffPaSS models.*",
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/base.py#L91){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DiffPaSSModel\n",
       "\n",
       ">      DiffPaSSModel (*args, **kwargs)\n",
       "\n",
       "*Base class for DiffPaSS models.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DiffPaSSModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/base.py#L448){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### DiffPaSSModel.fit\n\n>      DiffPaSSModel.fit (x:torch.Tensor, y:torch.Tensor, epochs:int=1,\n>                         optimizer_name:Optional[str]='SGD',\n>                         optimizer_kwargs:Optional[dict[str,Any]]=None,\n>                         mean_centering:bool=False, show_pbar:bool=False,\n>                         compute_final_soft:bool=False,\n>                         record_log_alphas:bool=False,\n>                         record_soft_perms:bool=False,\n>                         record_soft_losses:bool=False)\n\n*Fit permutations to data using gradient descent.*\n\n|    | **Type** | **Default** | **Details** |\n| -- | -------- | ----------- | ----------- |\n| x | Tensor |  | The object (MSA or adjacency matrix of graphs) to be permuted |\n| y | Tensor |  | The target object (MSA or adjacency matrix of graphs), that the objects represented by `x` should be paired with. Not acted upon by soft/hard permutations |\n| epochs | int | 1 |  |\n| optimizer_name | Optional | SGD |  |\n| optimizer_kwargs | Optional | None |  |\n| mean_centering | bool | False |  |\n| show_pbar | bool | False |  |\n| compute_final_soft | bool | False |  |\n| record_log_alphas | bool | False |  |\n| record_soft_perms | bool | False |  |\n| record_soft_losses | bool | False |  |\n| **Returns** | **DiffPaSSResults** |  | **`DiffPaSSResults` container for fit results. All attributes are lists indexed by gradient descent iteration** |",
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/base.py#L448){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DiffPaSSModel.fit\n",
       "\n",
       ">      DiffPaSSModel.fit (x:torch.Tensor, y:torch.Tensor, epochs:int=1,\n",
       ">                         optimizer_name:Optional[str]='SGD',\n",
       ">                         optimizer_kwargs:Optional[dict[str,Any]]=None,\n",
       ">                         mean_centering:bool=False, show_pbar:bool=False,\n",
       ">                         compute_final_soft:bool=False,\n",
       ">                         record_log_alphas:bool=False,\n",
       ">                         record_soft_perms:bool=False,\n",
       ">                         record_soft_losses:bool=False)\n",
       "\n",
       "*Fit permutations to data using gradient descent.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| x | Tensor |  | The object (MSA or adjacency matrix of graphs) to be permuted |\n",
       "| y | Tensor |  | The target object (MSA or adjacency matrix of graphs), that the objects represented by `x` should be paired with. Not acted upon by soft/hard permutations |\n",
       "| epochs | int | 1 |  |\n",
       "| optimizer_name | Optional | SGD |  |\n",
       "| optimizer_kwargs | Optional | None |  |\n",
       "| mean_centering | bool | False |  |\n",
       "| show_pbar | bool | False |  |\n",
       "| compute_final_soft | bool | False |  |\n",
       "| record_log_alphas | bool | False |  |\n",
       "| record_soft_perms | bool | False |  |\n",
       "| record_soft_losses | bool | False |  |\n",
       "| **Returns** | **DiffPaSSResults** |  | **`DiffPaSSResults` container for fit results. All attributes are lists indexed by gradient descent iteration** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DiffPaSSModel.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/base.py#L492){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### DiffPaSSModel.fit_bootstrap\n\n>      DiffPaSSModel.fit_bootstrap (x:torch.Tensor, y:torch.Tensor,\n>                                   n_start:int=1, n_end:Optional[int]=None,\n>                                   step_size:int=1, show_pbar:bool=True,\n>                                   single_fit_cfg:Optional[dict]=None)\n\n*Fit permutations to data using the DiffPaSS bootstrap.\n\nThe DiffPaSS bootstrap consists of a sequence of short gradient descent runs (default: one epoch per run).\nAt the end of each run, a subset of the found pairings is chosen uniformly at random\nand fixed for the next run.\nThe number of pairings fixed at each iteration ranges between `n_start` (default: 1) and `n_end` (default: total number of pairs), with a step size of `step_size`.*\n\n|    | **Type** | **Default** | **Details** |\n| -- | -------- | ----------- | ----------- |\n| x | Tensor |  | The object (MSA or adjacency matrix of graphs) to be permuted |\n| y | Tensor |  | The target object (MSA or adjacency matrix of graphs), that the objects represented by `x` should be paired with. Not acted upon by soft/hard permutations |\n| n_start | int | 1 |  |\n| n_end | Optional | None |  |\n| step_size | int | 1 |  |\n| show_pbar | bool | True |  |\n| single_fit_cfg | Optional | None |  |\n| **Returns** | **DiffPaSSResults** |  | **`DiffPaSSResults` container for fit results. All attributes are lists indexed by bootstrap iteration, containing lists indexed by gradient descent iteration as per `fit`** |",
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/base.py#L492){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DiffPaSSModel.fit_bootstrap\n",
       "\n",
       ">      DiffPaSSModel.fit_bootstrap (x:torch.Tensor, y:torch.Tensor,\n",
       ">                                   n_start:int=1, n_end:Optional[int]=None,\n",
       ">                                   step_size:int=1, show_pbar:bool=True,\n",
       ">                                   single_fit_cfg:Optional[dict]=None)\n",
       "\n",
       "*Fit permutations to data using the DiffPaSS bootstrap.\n",
       "\n",
       "The DiffPaSS bootstrap consists of a sequence of short gradient descent runs (default: one epoch per run).\n",
       "At the end of each run, a subset of the found pairings is chosen uniformly at random\n",
       "and fixed for the next run.\n",
       "The number of pairings fixed at each iteration ranges between `n_start` (default: 1) and `n_end` (default: total number of pairs), with a step size of `step_size`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| x | Tensor |  | The object (MSA or adjacency matrix of graphs) to be permuted |\n",
       "| y | Tensor |  | The target object (MSA or adjacency matrix of graphs), that the objects represented by `x` should be paired with. Not acted upon by soft/hard permutations |\n",
       "| n_start | int | 1 |  |\n",
       "| n_end | Optional | None |  |\n",
       "| step_size | int | 1 |  |\n",
       "| show_pbar | bool | True |  |\n",
       "| single_fit_cfg | Optional | None |  |\n",
       "| **Returns** | **DiffPaSSResults** |  | **`DiffPaSSResults` container for fit results. All attributes are lists indexed by bootstrap iteration, containing lists indexed by gradient descent iteration as per `fit`** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DiffPaSSModel.fit_bootstrap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
