{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base\n",
    "\n",
    "> DiffPASS base classes and mixins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Stdlib imports\n",
    "from collections.abc import Iterable, Sequence\n",
    "from typing import Optional, Union, Any\n",
    "\n",
    "# PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DiffPASSMixin:\n",
    "    allowed_permutation_cfg_keys = {\n",
    "        \"tau\",\n",
    "        \"n_iter\",\n",
    "        \"noise\",\n",
    "        \"noise_factor\",\n",
    "        \"noise_std\",\n",
    "    }\n",
    "    allowed_information_measures = {\"MI\", \"TwoBodyEntropy\"}\n",
    "    allowed_similarity_kinds = {\"Hamming\", \"Blosum62\"}\n",
    "    allowed_similarities_cfg_keys = {\n",
    "        \"Hamming\": {\"use_dot\", \"p\"},\n",
    "        \"Blosum62\": {\"use_scoredist\", \"aa_to_int\", \"gaps_as_stars\"},\n",
    "    }\n",
    "    allowed_best_hits_cfg_keys = {\"tau\", \"reciprocal\"}\n",
    "\n",
    "    group_sizes: Iterable[int]\n",
    "    information_measure: str\n",
    "    similarity_kind: str\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_num_tokens(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Reduce the number of tokens in a one-hot encoded tensor.\"\"\"\n",
    "        used_tokens = x.clone()\n",
    "        for _ in range(x.ndim - 1):\n",
    "            used_tokens = used_tokens.any(-2)\n",
    "\n",
    "        return x[..., used_tokens]\n",
    "\n",
    "    def validate_permutation_cfg(self, permutation_cfg: dict) -> None:\n",
    "        if not set(permutation_cfg).issubset(self.allowed_permutation_cfg_keys):\n",
    "            raise ValueError(\n",
    "                f\"Invalid keys in `permutation_cfg`: \"\n",
    "                f\"{set(permutation_cfg) - self.allowed_permutation_cfg_keys}\"\n",
    "            )\n",
    "\n",
    "    def validate_information_measure(self, information_measure: str) -> None:\n",
    "        if information_measure not in self.allowed_information_measures:\n",
    "            raise ValueError(\n",
    "                f\"Invalid information measure: {self.information_measure}. \"\n",
    "                f\"Allowed values are: {self.allowed_information_measures}\"\n",
    "            )\n",
    "\n",
    "    def validate_similarity_kind(self, similarity_kind: str) -> None:\n",
    "        if similarity_kind not in self.allowed_similarity_kinds:\n",
    "            raise ValueError(\n",
    "                f\"Invalid similarity kind: {self.similarity_kind}. \"\n",
    "                f\"Allowed values are: {self.allowed_similarity_kinds}\"\n",
    "            )\n",
    "\n",
    "    def validate_similarities_cfg(self, similarities_cfg: dict) -> None:\n",
    "        if not set(similarities_cfg).issubset(\n",
    "            self.allowed_similarities_cfg_keys[self.similarity_kind]\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"Invalid keys in `similarities_cfg`: \"\n",
    "                f\"{set(similarities_cfg) - self.allowed_similarities_cfg_keys[self.similarity_kind]}\"\n",
    "            )\n",
    "\n",
    "    def validate_best_hits_cfg(self, best_hits_cfg: dict) -> None:\n",
    "        if not set(best_hits_cfg).issubset(self.allowed_best_hits_cfg_keys):\n",
    "            raise ValueError(\n",
    "                f\"Invalid keys in `best_hits_cfg`: \"\n",
    "                f\"{set(best_hits_cfg) - self.allowed_best_hits_cfg_keys}\"\n",
    "            )\n",
    "\n",
    "    def validate_inputs(\n",
    "        self, x: torch.Tensor, y: torch.Tensor, check_same_alphabet_size: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"Validate input tensors representing aligned objects.\"\"\"\n",
    "        size_x, length_x, alphabet_size_x = x.shape\n",
    "        size_y, length_y, alphabet_size_y = y.shape\n",
    "        if size_x != size_x:\n",
    "            raise ValueError(f\"Size mismatch between x ({size_x}) and y ({size_y}).\")\n",
    "        if check_same_alphabet_size and (alphabet_size_x != alphabet_size_y):\n",
    "            raise ValueError(\"Inputs must have the same alphabet size.\")\n",
    "\n",
    "        # Validate size attribute\n",
    "        total_size = sum(self.group_sizes)\n",
    "        if size_x != total_size:\n",
    "            raise ValueError(\n",
    "                f\"Inputs have size {total_size} but `group_sizes` implies a total \"\n",
    "                f\"size of {total_size}.\"\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def check_can_optimize(n_effectively_fixed: int, n_available: int) -> None:\n",
    "        if n_effectively_fixed == n_available:\n",
    "            raise ValueError(\n",
    "                \"The number of effectively fixed matchings is equal to the number \"\n",
    "                \"of sequences. No optimization can be performed.\"\n",
    "            )\n",
    "        elif n_effectively_fixed > n_available:\n",
    "            raise ValueError(\n",
    "                \"The number of effectively fixed matchings is greater than the number \"\n",
    "                \"of available sequences. Check your inputs.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def scalar_or_1d_tensor(*, param: Any, param_name: str) -> torch.Tensor:\n",
    "    if not isinstance(param, (int, float, torch.Tensor)):\n",
    "        raise TypeError(f\"`{param_name}` must be a scalar or a torch.Tensor.\")\n",
    "    if not isinstance(param, torch.Tensor):\n",
    "        param = torch.tensor(param, dtype=torch.get_default_dtype())\n",
    "    elif param.ndim > 1:\n",
    "        raise ValueError(\n",
    "            f\"`{param_name}` must be a scalar or a tensor of dimension <= 1.\"\n",
    "        )\n",
    "\n",
    "    return param\n",
    "\n",
    "\n",
    "class EnsembleMixin:\n",
    "    def _validate_ensemble_param(\n",
    "        self,\n",
    "        *,\n",
    "        param: Union[float, torch.Tensor],\n",
    "        param_name: str,\n",
    "        ensemble_shape: Sequence[int],\n",
    "        dim_in_ensemble: Optional[int] = None,\n",
    "        n_dims_per_instance: Optional[int] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        param = scalar_or_1d_tensor(param=param, param_name=param_name)\n",
    "\n",
    "        param = self._reshape_ensemble_param(\n",
    "            param=param,\n",
    "            ensemble_shape=ensemble_shape,\n",
    "            dim_in_ensemble=dim_in_ensemble,\n",
    "            n_dims_per_instance=n_dims_per_instance,\n",
    "            param_name=param_name,\n",
    "        )\n",
    "\n",
    "        return param\n",
    "\n",
    "    @staticmethod\n",
    "    def _reshape_ensemble_param(\n",
    "        *,\n",
    "        param: torch.Tensor,\n",
    "        ensemble_shape: Sequence[int],\n",
    "        dim_in_ensemble: Optional[int],\n",
    "        n_dims_per_instance: int,\n",
    "        param_name: str,\n",
    "    ) -> torch.Tensor:\n",
    "        n_ensemble_dims = len(ensemble_shape)\n",
    "        if param.ndim == 1:\n",
    "            if dim_in_ensemble is None:\n",
    "                raise ValueError(\n",
    "                    f\"`dim_in_ensemble` cannot be None if {param_name} is 1D.\"\n",
    "                )\n",
    "            param = param.to(torch.get_default_dtype())\n",
    "            # If param is not a scalar, broadcast it along the `ensemble_dim`-th ensemble dimension\n",
    "            if dim_in_ensemble >= n_ensemble_dims or dim_in_ensemble < -n_ensemble_dims:\n",
    "                raise ValueError(\n",
    "                    f\"Ensemble dimension for {param_name} must be an available index \"\n",
    "                    f\"in `ensemble_shape`.\"\n",
    "                )\n",
    "            elif len(param) != ensemble_shape[dim_in_ensemble]:\n",
    "                raise ValueError(\n",
    "                    f\"Parameter `{param_name}` must have the same length as \"\n",
    "                    f\"``ensemble_shape[dim_in_ensemble]`` = \"\n",
    "                    f\"{ensemble_shape[dim_in_ensemble]}.\"\n",
    "                )\n",
    "            new_shape = (\n",
    "                (1,) * dim_in_ensemble\n",
    "                + param.shape\n",
    "                + (1,) * (n_ensemble_dims - dim_in_ensemble - 1)\n",
    "                + (1,) * n_dims_per_instance\n",
    "            )\n",
    "            param = param.view(*new_shape)\n",
    "\n",
    "        return param"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
