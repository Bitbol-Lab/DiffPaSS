{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "\n",
    "> DiffPASS models for optimizing MSA pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Stdlib imports\n",
    "from collections.abc import Iterable, Sequence\n",
    "from typing import Optional, Union, Iterator, Literal\n",
    "from copy import deepcopy\n",
    "from warnings import warn\n",
    "\n",
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.nn import Module, ParameterList, Parameter, CosineSimilarity\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# DiffPASS imports\n",
    "from diffpass.base import EnsembleMixin\n",
    "from diffpass.gumbel_sinkhorn_ops import gumbel_sinkhorn, gumbel_matching\n",
    "from diffpass.entropy_ops import (\n",
    "    smooth_mean_one_body_entropy,\n",
    "    smooth_mean_two_body_entropy,\n",
    ")\n",
    "from diffpass.constants import get_blosum62_data\n",
    "from diffpass.sequence_similarity_ops import (\n",
    "    smooth_hamming_similarities_dot,\n",
    "    smooth_hamming_similarities_cdist,\n",
    "    smooth_substitution_matrix_similarities,\n",
    "    soft_reciprocal_best_hits,\n",
    "    hard_reciprocal_best_hits,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _consecutive_slices_from_sizes(group_sizes: Optional[Sequence[int]]) -> list[slice]:\n",
    "    if group_sizes is None:\n",
    "        return [slice(None)]\n",
    "    cumsum = np.cumsum(group_sizes).tolist()\n",
    "\n",
    "    return [slice(start, end) for start, end in zip([0] + cumsum, cumsum)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft permutations (Gumbel-Sinkhorn operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class GeneralizedPermutation(Module, EnsembleMixin):\n",
    "    \"\"\"Generalized permutation layer implementing both soft and hard permutations.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        group_sizes: Iterable[int],\n",
    "        ensemble_shape: Optional[Iterable[int]] = None,\n",
    "        fixed_matchings: Optional[Sequence[Sequence[Sequence[int]]]] = None,\n",
    "        tau: Union[float, torch.Tensor] = 1.0,\n",
    "        tau_dim_in_ensemble: Optional[int] = None,\n",
    "        n_iter: int = 10,\n",
    "        noise: bool = False,\n",
    "        noise_factor: float = 1.0,\n",
    "        noise_std: bool = False,\n",
    "        mode: Literal[\"soft\", \"hard\"] = \"soft\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.group_sizes = tuple(s for s in group_sizes)\n",
    "        self.ensemble_shape = torch.Size(\n",
    "            ensemble_shape if ensemble_shape is not None else []\n",
    "        )\n",
    "        self._validate_fixed_matchings(fixed_matchings)\n",
    "        self.fixed_matchings = fixed_matchings\n",
    "        self.n_iter = n_iter\n",
    "        self.noise = noise\n",
    "        self.noise_factor = noise_factor\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        # By default, initialize all parametrization matrices to zero\n",
    "        # If `ensemble_shape` is not None, use the first len(ensemble_shape) dimensions\n",
    "        # as \"batch\" dimensions.\n",
    "        self.nonfixed_group_sizes_ = (\n",
    "            tuple(\n",
    "                s - num_efm\n",
    "                for s, num_efm in zip(\n",
    "                    self.group_sizes, self._effective_number_fixed_matchings\n",
    "                )\n",
    "            )\n",
    "            if self.fixed_matchings\n",
    "            else self.group_sizes\n",
    "        )\n",
    "        self.log_alphas = ParameterList(\n",
    "            [\n",
    "                Parameter(torch.zeros(*self.ensemble_shape, s, s, dtype=torch.float32))\n",
    "                for s in self.nonfixed_group_sizes_\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        tau = self._validate_ensemble_param(\n",
    "            param=tau,\n",
    "            dim_in_ensemble=tau_dim_in_ensemble,\n",
    "            param_name=\"tau\",\n",
    "            ensemble_shape=self.ensemble_shape,\n",
    "            n_dims_per_instance=2,\n",
    "        )\n",
    "        self.register_buffer(\"tau\", tau)\n",
    "        self.tau_dim_in_ensemble = tau_dim_in_ensemble\n",
    "        self.mode = mode\n",
    "\n",
    "    def _validate_fixed_matchings(\n",
    "        self, fixed_matchings: Optional[Sequence[Sequence[Sequence[int]]]] = None\n",
    "    ) -> None:\n",
    "        if fixed_matchings:\n",
    "            if len(fixed_matchings) != len(self.group_sizes):\n",
    "                raise ValueError(\n",
    "                    \"If `fixed_matchings` is provided, it must have the same length as \"\n",
    "                    \"`group_sizes`.\"\n",
    "                )\n",
    "            for s, fm in zip(self.group_sizes, fixed_matchings):\n",
    "                if not fm:\n",
    "                    continue\n",
    "                if any([len(p) != 2 for p in fm]):\n",
    "                    raise ValueError(\n",
    "                        \"All fixed matchings must be pairs of indices (i, j).\"\n",
    "                    )\n",
    "                if any(min(i, j) < 0 or max(i, j) >= s for i, j in fm):\n",
    "                    raise ValueError(\n",
    "                        \"All fixed matchings must be within the range of the corresponding \"\n",
    "                        \"group size.\"\n",
    "                    )\n",
    "            self._effective_number_fixed_matchings = []\n",
    "            self._effective_fixed_matchings_zip = []\n",
    "            for idx, (s, fm) in enumerate(zip(self.group_sizes, fixed_matchings)):\n",
    "                if fm:\n",
    "                    num_fm = len(fm)\n",
    "                    fm_zip = list(zip(*fm))\n",
    "                else:\n",
    "                    num_fm = 0\n",
    "                    fm_zip = ((), ())\n",
    "                complement = s - num_fm  # Effectively fully fixed when complement <= 1\n",
    "                is_fully_fixed = complement <= 1\n",
    "                num_efm = s - (s - num_fm) * (not is_fully_fixed)\n",
    "                self._effective_number_fixed_matchings.append(num_efm)\n",
    "                if is_fully_fixed:\n",
    "                    mask = torch.zeros(*self.ensemble_shape, s, s, dtype=torch.bool)\n",
    "                    if complement:\n",
    "                        possible_idxs = set(range(s))\n",
    "                        fm_zip[0] += tuple((possible_idxs - set(fm_zip[0])))\n",
    "                        fm_zip[1] += tuple((possible_idxs - set(fm_zip[1])))\n",
    "                else:\n",
    "                    mask = torch.ones(*self.ensemble_shape, s, s, dtype=torch.bool)\n",
    "                    for i, j in fm:\n",
    "                        mask[..., j, :] = False\n",
    "                        mask[..., :, i] = False\n",
    "                self.register_buffer(f\"_not_fixed_masks_{idx}\", mask)\n",
    "                self._effective_fixed_matchings_zip.append(fm_zip)\n",
    "\n",
    "    @property\n",
    "    def _not_fixed_masks(self) -> list[torch.Tensor]:\n",
    "        return [\n",
    "            getattr(self, f\"_not_fixed_masks_{idx}\")\n",
    "            for idx in range(len(self.group_sizes))\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def mode(self) -> str:\n",
    "        return self._mode\n",
    "\n",
    "    @mode.setter\n",
    "    def mode(self, value) -> None:\n",
    "        value = value.lower()\n",
    "        if value not in [\"soft\", \"hard\"]:\n",
    "            raise ValueError(\"mode must be either 'soft' or 'hard'.\")\n",
    "        self._mode = value.lower()\n",
    "        _mats_fn_no_fixed = getattr(self, f\"_{self._mode}_mats\")\n",
    "        self._mats_fn = (\n",
    "            _mats_fn_no_fixed\n",
    "            if not self.fixed_matchings\n",
    "            else self._impl_fixed_matchings(_mats_fn_no_fixed)\n",
    "        )\n",
    "\n",
    "    def soft_(self) -> None:\n",
    "        self.mode = \"soft\"\n",
    "\n",
    "    def hard_(self) -> None:\n",
    "        self.mode = \"hard\"\n",
    "\n",
    "    def _impl_fixed_matchings(self, func: callable) -> callable:\n",
    "        \"\"\"Include fixed matchings in the Gumbel-Sinkhorn or Gumbel-matching operators.\"\"\"\n",
    "\n",
    "        def wrapper(gen: Iterator[torch.Tensor]) -> Iterator[torch.Tensor]:\n",
    "            for s, mat, (row_group, col_group), mask in zip(\n",
    "                self.group_sizes,\n",
    "                gen,\n",
    "                self._effective_fixed_matchings_zip,\n",
    "                self._not_fixed_masks,\n",
    "            ):\n",
    "                mat_all = torch.zeros(\n",
    "                    *self.ensemble_shape,\n",
    "                    s,\n",
    "                    s,\n",
    "                    dtype=mat.dtype,\n",
    "                    layout=mat.layout,\n",
    "                    device=mat.device,\n",
    "                )\n",
    "                # mat_all[j, i] = 1 means that row i becomes row j under a permutation,\n",
    "                # using our conventions\n",
    "                mat_all[..., col_group, row_group] = 1\n",
    "                mat_all.masked_scatter_(mask.to(torch.bool), mat)\n",
    "                yield mat_all\n",
    "\n",
    "        return lambda: wrapper(func())\n",
    "\n",
    "    def _soft_mats(self) -> Iterator[torch.Tensor]:\n",
    "        \"\"\"Evaluate the Gumbel-Sinkhorn operator on the current `log_alpha` parameters.\"\"\"\n",
    "        return (\n",
    "            gumbel_sinkhorn(\n",
    "                log_alpha,\n",
    "                tau=self.tau,\n",
    "                n_iter=self.n_iter,\n",
    "                noise=self.noise,\n",
    "                noise_factor=self.noise_factor,\n",
    "                noise_std=self.noise_std,\n",
    "            )\n",
    "            for log_alpha in self.log_alphas\n",
    "        )\n",
    "\n",
    "    def _hard_mats(self) -> Iterator[torch.Tensor]:\n",
    "        \"\"\"Evaluate the Gumbel-matching operator on the current `log_alpha` parameters.\"\"\"\n",
    "        return (\n",
    "            gumbel_matching(\n",
    "                log_alpha,\n",
    "                noise=self.noise,\n",
    "                noise_factor=self.noise_factor,\n",
    "                noise_std=self.noise_std,\n",
    "                unbias_lsa=True,\n",
    "            )\n",
    "            for log_alpha in self.log_alphas\n",
    "        )\n",
    "\n",
    "    def forward(self) -> list[torch.Tensor]:\n",
    "        \"\"\"Compute the soft/hard permutations according to ``self._mats_fn.``\"\"\"\n",
    "        mats = self._mats_fn()\n",
    "\n",
    "        return list(mats)\n",
    "\n",
    "\n",
    "class EnsembleMatrixApply(Module):\n",
    "    \"\"\"Apply batches of matrices to chunks of a tensor of shape (n_samples, length, alphabet_size)\n",
    "    and collate the results.\"\"\"\n",
    "\n",
    "    def __init__(self, group_sizes: Iterable[int]) -> None:\n",
    "        super().__init__()\n",
    "        self.group_sizes = tuple(s for s in group_sizes)\n",
    "        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, *, mats: Sequence[torch.Tensor]) -> torch.Tensor:\n",
    "        ensemble_shape = mats[0].shape[:-2]\n",
    "        out = torch.full_like(x, torch.nan).repeat(*ensemble_shape, 1, 1, 1)\n",
    "        for mats_this_group, sl in zip(mats, self._group_slices):\n",
    "            out[..., sl, :, :].copy_(\n",
    "                torch.tensordot(mats_this_group, x[sl, :, :], dims=1)\n",
    "            )\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(GeneralizedPermutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for GeneralizedPermutation\n",
    "\n",
    "def test_generalizedpermutation_shape(*, length, alphabet_size, init_kwargs):\n",
    "    group_sizes = init_kwargs[\"group_sizes\"]\n",
    "    n_samples = sum(group_sizes)\n",
    "\n",
    "    x = torch.randn(n_samples, length, alphabet_size)\n",
    "    perm = GeneralizedPermutation(**init_kwargs)\n",
    "    mats = perm()\n",
    "    ens_apply = EnsembleMatrixApply(group_sizes)\n",
    "    y = ens_apply(x, mats=mats)\n",
    "\n",
    "    assert y.shape == perm.ensemble_shape + x.shape\n",
    "    assert y.requires_grad\n",
    "\n",
    "    perm.hard_()\n",
    "    assert perm.mode == \"hard\"\n",
    "\n",
    "\n",
    "test_generalizedpermutation_shape(\n",
    "    length=5,\n",
    "    alphabet_size=10,\n",
    "    init_kwargs={\n",
    "        \"group_sizes\": [3, 2, 4],\n",
    "        \"ensemble_shape\": (2, 3, 3, 3),\n",
    "        \"fixed_matchings\": [[[0, 1]], [[0, 0]], [[1, 0], [2, 3]]],\n",
    "        \"tau\": torch.tensor([1., 0.1, 3.]),\n",
    "        \"tau_dim_in_ensemble\": 2\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-body entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TwoBodyEntropyLoss(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        return smooth_mean_two_body_entropy(x, y)\n",
    "\n",
    "\n",
    "class MILoss(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        return smooth_mean_two_body_entropy(x, y) - smooth_mean_one_body_entropy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for TwoBodyEntropyScore\n",
    "\n",
    "def test_twobodyentropyloss(\n",
    "        *,\n",
    "        n_samples, length_x, length_y, alphabet_size, ensemble_shape\n",
    "):\n",
    "    x = torch.randn(\n",
    "        *ensemble_shape, n_samples, length_x, alphabet_size,\n",
    "        requires_grad=True\n",
    "    )\n",
    "    y = torch.randn(n_samples, length_y, alphabet_size)\n",
    "    x_soft = softmax(x, dim=-1)\n",
    "    y_soft = softmax(y, dim=-1)\n",
    "    two_body_entropy_loss = TwoBodyEntropyLoss()\n",
    "    loss = two_body_entropy_loss(x_soft, y_soft)\n",
    "\n",
    "    assert loss.shape == ensemble_shape\n",
    "    assert loss.requires_grad\n",
    "\n",
    "    # In the following scenario, the score should be close to log2(alphabet_size)\n",
    "    x_almost_hard = softmax(x / 1e-5, dim=-1)\n",
    "    first_x_almost_hard_length_1 = x_almost_hard[(0,) * len(ensemble_shape)][:, :1, :]\n",
    "    loss = two_body_entropy_loss(\n",
    "        first_x_almost_hard_length_1, first_x_almost_hard_length_1\n",
    "    )\n",
    "\n",
    "    torch.testing.assert_close(\n",
    "        loss, torch.log2(torch.tensor(alphabet_size)), atol=1e-3, rtol=1e-7\n",
    "    )\n",
    "\n",
    "\n",
    "test_twobodyentropyloss(\n",
    "    n_samples=10_000,\n",
    "    length_x=3,\n",
    "    length_y=4,\n",
    "    alphabet_size=3,\n",
    "    ensemble_shape=(2, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence similarities (Hamming and Blosum62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class HammingSimilarities(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        group_sizes: Optional[Iterable[int]] = None,\n",
    "        use_dot: bool = True,\n",
    "        p: Optional[float] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.group_sizes = (\n",
    "            tuple(s for s in group_sizes) if group_sizes is not None else None\n",
    "        )\n",
    "        self.use_dot = use_dot\n",
    "        self.p = p\n",
    "\n",
    "        if self.use_dot:\n",
    "            if self.p is not None:\n",
    "                warn(\"Since a `p` was provided, `use_dot` will be ignored.\")\n",
    "            self._similarities_fn = smooth_hamming_similarities_dot\n",
    "            self._similarities_fn_kwargs = {}\n",
    "        else:\n",
    "            if self.p is None:\n",
    "                raise ValueError(\"If `use_dot` is False, `p` must be provided.\")\n",
    "            self._similarities_fn = smooth_hamming_similarities_cdist\n",
    "            self._similarities_fn_kwargs = {\"p\": p}\n",
    "\n",
    "        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        size = x.shape[:-3] + (x.shape[-3],) * 2\n",
    "        out = torch.full(\n",
    "            size, torch.nan, dtype=x.dtype, layout=x.layout, device=x.device\n",
    "        )\n",
    "        for sl in self._group_slices:\n",
    "            out[..., sl, sl].copy_(\n",
    "                self._similarities_fn(x[..., sl, :, :], **self._similarities_fn_kwargs)\n",
    "            )\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Blosum62Similarities(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        group_sizes: Optional[Iterable[int]] = None,\n",
    "        use_scoredist: bool = False,\n",
    "        aa_to_int: Optional[dict[str, int]] = None,\n",
    "        gaps_as_stars: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.group_sizes = (\n",
    "            tuple(s for s in group_sizes) if group_sizes is not None else None\n",
    "        )\n",
    "        self.use_scoredist = use_scoredist\n",
    "        self.aa_to_int = aa_to_int\n",
    "        self.gaps_as_stars = gaps_as_stars\n",
    "\n",
    "        blosum62_data = get_blosum62_data(\n",
    "            aa_to_int=self.aa_to_int, gaps_as_stars=self.gaps_as_stars\n",
    "        )\n",
    "        self.register_buffer(\"subs_mat\", blosum62_data.mat)\n",
    "        self.expected_value = blosum62_data.expected_value\n",
    "\n",
    "        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        size = x.shape[:-3] + (x.shape[-3],) * 2\n",
    "        out = torch.full(\n",
    "            size, torch.nan, dtype=x.dtype, layout=x.layout, device=x.device\n",
    "        )\n",
    "        for sl in self._group_slices:\n",
    "            out[..., sl, sl].copy_(\n",
    "                smooth_substitution_matrix_similarities(\n",
    "                    x[..., sl, :, :],\n",
    "                    subs_mat=self.subs_mat,\n",
    "                    expected_value=self.expected_value,\n",
    "                    use_scoredist=self.use_scoredist,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for HammingSimilarities and Blosum62Similarities\n",
    "\n",
    "def test_similarities(\n",
    "        *,\n",
    "        cls,\n",
    "        ensemble_shape, length, alphabet_size,\n",
    "        init_kwargs\n",
    "):\n",
    "    group_sizes = init_kwargs[\"group_sizes\"]\n",
    "    n_samples = sum(group_sizes)\n",
    "\n",
    "    x = torch.randn(\n",
    "        *ensemble_shape, n_samples, length, alphabet_size,\n",
    "        requires_grad=True\n",
    "    )\n",
    "    x_soft = softmax(x, dim=-1)\n",
    "\n",
    "    _init_kwargs = deepcopy(init_kwargs)\n",
    "    _init_kwargs[\"group_sizes\"] = None\n",
    "    similarities = cls(**_init_kwargs)\n",
    "    out_all = similarities(x_soft)\n",
    "\n",
    "    assert out_all.shape == ensemble_shape + (n_samples, n_samples)\n",
    "\n",
    "    similarities = cls(**init_kwargs)\n",
    "    out = similarities(x_soft)\n",
    "\n",
    "    for sl in similarities._group_slices:\n",
    "        assert torch.allclose(\n",
    "            out[..., sl, sl], out_all[..., sl, sl]\n",
    "        )\n",
    "\n",
    "\n",
    "test_similarities(\n",
    "    cls=HammingSimilarities,\n",
    "    ensemble_shape=(3, 2),\n",
    "    length=3,\n",
    "    alphabet_size=10,\n",
    "    init_kwargs={\"group_sizes\": [3, 2, 4], \"use_dot\": False, \"p\": 1.}\n",
    ")\n",
    "\n",
    "test_similarities(\n",
    "    cls=Blosum62Similarities,\n",
    "    ensemble_shape=(3, 2),\n",
    "    length=3,\n",
    "    alphabet_size=21,\n",
    "    init_kwargs={\"group_sizes\": [3, 2, 4]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reciprocal best hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ReciprocalBestHits(Module, EnsembleMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        group_sizes: Iterable[int],\n",
    "        ensemble_shape: Optional[Iterable[int]] = None,\n",
    "        tau: Union[float, torch.Tensor] = 0.1,\n",
    "        tau_dim_in_ensemble: Optional[int] = None,\n",
    "        mode: Literal[\"soft\", \"hard\"] = \"soft\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.group_sizes = tuple(s for s in group_sizes)\n",
    "        self.ensemble_shape = torch.Size(\n",
    "            ensemble_shape if ensemble_shape is not None else []\n",
    "        )\n",
    "\n",
    "        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)\n",
    "\n",
    "        tau = self._validate_ensemble_param(\n",
    "            param=tau,\n",
    "            param_name=\"tau\",\n",
    "            dim_in_ensemble=tau_dim_in_ensemble,\n",
    "            ensemble_shape=self.ensemble_shape,\n",
    "            n_dims_per_instance=2,\n",
    "        )\n",
    "        self.register_buffer(\"tau\", tau)\n",
    "        self.tau_dim_in_ensemble = tau_dim_in_ensemble\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "    @property\n",
    "    def mode(self) -> str:\n",
    "        return self._mode\n",
    "\n",
    "    @mode.setter\n",
    "    def mode(self, value) -> None:\n",
    "        value = value.lower()\n",
    "        if value not in [\"soft\", \"hard\"]:\n",
    "            raise ValueError(\"`mode` must be either 'soft' or 'hard'.\")\n",
    "        self._mode = value.lower()\n",
    "        self._rbh_fn = getattr(self, f\"_{self._mode}_rbh_fn\")\n",
    "\n",
    "    def soft_(self) -> None:\n",
    "        self.mode = \"soft\"\n",
    "\n",
    "    def hard_(self) -> None:\n",
    "        self.mode = \"hard\"\n",
    "\n",
    "    def _soft_rbh_fn(self, similarities: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute soft reciprocal best hits.\"\"\"\n",
    "        return soft_reciprocal_best_hits(\n",
    "            similarities,\n",
    "            group_slices=self._group_slices,\n",
    "            tau=self.tau,\n",
    "        )\n",
    "\n",
    "    def _hard_rbh_fn(self, similarities: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute hard reciprocal best hits.\"\"\"\n",
    "        return hard_reciprocal_best_hits(\n",
    "            similarities,\n",
    "            group_slices=self._group_slices,\n",
    "        )\n",
    "\n",
    "    def prepare_fixed(self, similarities: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Repeat a similarity matrix along the tau ensemble dimension and unsqueeze.\"\"\"\n",
    "        assert similarities.ndim == 2\n",
    "        if self.mode == \"soft\" and self.tau.ndim:\n",
    "            if self.tau_dim_in_ensemble is None:\n",
    "                raise ValueError(\n",
    "                    \"If using soft reciprocal best hits and a 1D `tau`, \"\n",
    "                    \"`tau_dim_in_ensemble` must be provided.\"\n",
    "                )\n",
    "            n_ensemble_dims = len(self.ensemble_shape)\n",
    "            # FIXME (self.tau.shape[self.tau_dim_in_ensemble],) is a hack, it's there\n",
    "            #  because we lost the original 1D tau\n",
    "            new_shape = (\n",
    "                (1,) * self.tau_dim_in_ensemble\n",
    "                + (self.tau.shape[self.tau_dim_in_ensemble],)\n",
    "                + (1,) * (n_ensemble_dims - self.tau_dim_in_ensemble - 1)\n",
    "                + (1,) * 2\n",
    "            )\n",
    "            similarities = similarities.repeat(*new_shape)\n",
    "\n",
    "        return similarities\n",
    "\n",
    "    def forward(self, similarities: torch.Tensor) -> torch.Tensor:\n",
    "        # Input validation\n",
    "        if self.mode == \"soft\" and self.tau.ndim:\n",
    "            if similarities.ndim != len(self.ensemble_shape) + 2:\n",
    "                raise ValueError(\n",
    "                    f\"If using soft reciprocal best hits and a 1D `tau`, the input must have the \"\n",
    "                    f\"same number of dimensions as the ensemble shape plus 2 = \"\n",
    "                    f\"{len(self.ensemble_shape) + 2}, not {similarities.ndim}.\"\n",
    "                )\n",
    "        else:\n",
    "            if similarities.ndim != 2:\n",
    "                raise ValueError(\n",
    "                    f\"If using hard reciprocal best hits or a 0D `tau`, \"\n",
    "                    f\"the input must have 2 dimensions, not {similarities.ndim}.\"\n",
    "                )\n",
    "\n",
    "        return self._rbh_fn(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class InterGroupLoss(Module, EnsembleMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        group_sizes: Iterable[int],\n",
    "        score_fn: Union[callable, None] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.group_sizes = tuple(s for s in group_sizes)\n",
    "        self.score_fn = CosineSimilarity(dim=-1) if score_fn is None else score_fn\n",
    "\n",
    "        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)\n",
    "        diag_blocks_mask = torch.block_diag(\n",
    "            *[torch.ones((s, s), dtype=torch.bool) for s in self.group_sizes]\n",
    "        )\n",
    "        self._upper_no_diag_blocks_mask = torch.triu(~diag_blocks_mask)\n",
    "\n",
    "    def forward(\n",
    "        self, similarities_x: torch.Tensor, similarities_y: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # Input validation\n",
    "        assert similarities_x.ndim >= 2 and similarities_y.ndim >= 2\n",
    "\n",
    "        scores = self.score_fn(\n",
    "            similarities_x[..., self._upper_no_diag_blocks_mask],\n",
    "            similarities_y[..., self._upper_no_diag_blocks_mask],\n",
    "        )\n",
    "        loss = -scores\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class IntraGroupLoss(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        group_sizes: Optional[Iterable[int]] = None,\n",
    "        score_fn: Union[callable, None] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.group_sizes = (\n",
    "            tuple(s for s in group_sizes) if group_sizes is not None else None\n",
    "        )\n",
    "        self.score_fn = CosineSimilarity(dim=-1) if score_fn is None else score_fn\n",
    "\n",
    "        if self.group_sizes is not None:\n",
    "            diag_blocks_mask = torch.block_diag(\n",
    "                *[torch.ones((s, s), dtype=torch.bool) for s in self.group_sizes]\n",
    "            )\n",
    "            # Exclude main diagonal\n",
    "            self._upper_diag_blocks_mask = torch.triu(diag_blocks_mask, diagonal=1)\n",
    "        else:\n",
    "            self._upper_diag_blocks_mask = None\n",
    "\n",
    "    def forward(\n",
    "        self, similarities_x: torch.Tensor, similarities_y: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        assert similarities_x.ndim >= 2 and similarities_y.ndim >= 2\n",
    "\n",
    "        if self._upper_diag_blocks_mask is None:\n",
    "            mask = torch.triu(\n",
    "                torch.ones(\n",
    "                    similarities_x.shape[-2:],\n",
    "                    dtype=torch.bool,\n",
    "                    layout=similarities_x.layout,\n",
    "                    device=similarities_x.device,\n",
    "                ),\n",
    "                diagonal=1,\n",
    "            )\n",
    "        else:\n",
    "            mask = self._upper_diag_blocks_mask\n",
    "\n",
    "        scores = self.score_fn(\n",
    "            similarities_x[..., mask],\n",
    "            similarities_y[..., mask],\n",
    "        )\n",
    "        loss = -scores\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for ReciprocalBestHits, InterGroupLoss and IntraGroupLoss\n",
    "\n",
    "def test_sequence_similarity_losses(\n",
    "        *,\n",
    "        group_sizes,\n",
    "        length_x, length_y, alphabet_size,\n",
    "        extra_init_kwargs_rbh, extra_init_kwargs_loss\n",
    "):\n",
    "    similarities = HammingSimilarities(group_sizes=None)\n",
    "    reciprocal_best_hits = ReciprocalBestHits(group_sizes=group_sizes, **extra_init_kwargs_rbh)\n",
    "    n_samples = sum(group_sizes)\n",
    "\n",
    "    y = torch.randn(n_samples, length_y, alphabet_size)\n",
    "    y.scatter_(-1, y.argmax(dim=-1, keepdim=True), 1.)\n",
    "    similarities_y = similarities(y)\n",
    "    reciprocal_best_hits.hard_()\n",
    "    reciprocal_best_hits_y = reciprocal_best_hits(similarities_y)\n",
    "    reciprocal_best_hits.soft_()\n",
    "\n",
    "    ensemble_shape = extra_init_kwargs_rbh[\"ensemble_shape\"]\n",
    "    x = torch.randn(\n",
    "        *ensemble_shape, n_samples, length_x, alphabet_size,\n",
    "        requires_grad=True\n",
    "    )\n",
    "    x_soft = softmax(x, dim=-1)\n",
    "    similarities_x = similarities(x_soft)\n",
    "    reciprocal_best_hits_x = reciprocal_best_hits(similarities_x)\n",
    "\n",
    "    #### Reciprocal best hits loss ####\n",
    "    inter_group_loss = InterGroupLoss(group_sizes=group_sizes, **extra_init_kwargs_loss)\n",
    "    loss = inter_group_loss(reciprocal_best_hits_x, reciprocal_best_hits_y)\n",
    "\n",
    "    assert loss.shape == ensemble_shape\n",
    "    assert loss.requires_grad\n",
    "\n",
    "    # In the following scenario, the loss should be close to -1\n",
    "    extra_init_kwargs_rbh = deepcopy(extra_init_kwargs_rbh)\n",
    "    extra_init_kwargs_rbh[\"ensemble_shape\"] = None\n",
    "    extra_init_kwargs_rbh[\"tau\"] = 1e-4\n",
    "    reciprocal_best_hits = ReciprocalBestHits(group_sizes=group_sizes, **extra_init_kwargs_rbh)\n",
    "    reciprocal_best_hits_x = reciprocal_best_hits(similarities_y)\n",
    "    loss = inter_group_loss(reciprocal_best_hits_x, reciprocal_best_hits_y)\n",
    "\n",
    "    torch.testing.assert_close(loss, torch.tensor(-1.))\n",
    "\n",
    "    #### Mirrortree-like loss ####\n",
    "    intra_group_loss = IntraGroupLoss(**extra_init_kwargs_loss)\n",
    "    loss = intra_group_loss(similarities_x, similarities_y)\n",
    "\n",
    "    assert loss.shape == ensemble_shape\n",
    "    assert loss.requires_grad\n",
    "\n",
    "    # In the following scenario, the loss should be close to -1\n",
    "    loss = intra_group_loss(similarities_y, similarities_y)\n",
    "\n",
    "    torch.testing.assert_close(loss, torch.tensor(-1.))\n",
    "\n",
    "\n",
    "test_sequence_similarity_losses( \n",
    "    length_x=3,\n",
    "    length_y=4,\n",
    "    alphabet_size=3,\n",
    "    group_sizes=[3, 2, 4],\n",
    "    extra_init_kwargs_rbh={\n",
    "        \"ensemble_shape\": (3, 3),\n",
    "        \"tau\": torch.tensor([1., 0.1, 3.]),\n",
    "        \"tau_dim_in_ensemble\": 1,\n",
    "    },\n",
    "    extra_init_kwargs_loss={\n",
    "        \"score_fn\": None\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
