{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "\n",
    "> DiffPaSS modules for optimizing permutations and computing soft scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Stdlib imports\n",
    "from collections.abc import Iterable, Sequence\n",
    "from typing import Optional, Union, Iterator, Literal\n",
    "from copy import deepcopy\n",
    "from warnings import warn\n",
    "from functools import partial\n",
    "\n",
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.nn import Module, ParameterList, Parameter\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# DiffPaSS imports\n",
    "from diffpass.gumbel_sinkhorn_ops import gumbel_sinkhorn, gumbel_matching\n",
    "from diffpass.entropy_ops import (\n",
    "    smooth_mean_one_body_entropy,\n",
    "    smooth_mean_two_body_entropy,\n",
    ")\n",
    "from diffpass.constants import get_blosum62_data\n",
    "from diffpass.sequence_similarity_ops import (\n",
    "    smooth_hamming_similarities_dot,\n",
    "    smooth_hamming_similarities_cdist,\n",
    "    smooth_substitution_matrix_similarities_dot,\n",
    "    smooth_substitution_matrix_similarities_cdist,\n",
    "    soft_best_hits,\n",
    "    hard_best_hits,\n",
    ")\n",
    "\n",
    "# Type aliases\n",
    "IndexPair = tuple[int, int]  # Pair of indices\n",
    "IndexPairsInGroup = Sequence[IndexPair]  # Pairs of indices in a group of sequences\n",
    "IndexPairsInGroups = Sequence[\n",
    "    IndexPairsInGroup\n",
    "]  # Pairs of indices in groups of sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type aliases\n",
    "```python\n",
    "IndexPair = tuple[int, int]  # Pair of indices\n",
    "IndexPairsInGroup = Sequence[IndexPair]  # Pairs of indices in a group of sequences\n",
    "IndexPairsInGroups = Sequence[IndexPairsInGroup]  # Pairs of indices in groups of sequences\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _consecutive_slices_from_sizes(group_sizes: Optional[Sequence[int]]) -> list[slice]:\n",
    "    if group_sizes is None:\n",
    "        return [slice(None)]\n",
    "    cumsum = np.cumsum(group_sizes).tolist()\n",
    "\n",
    "    return [slice(start, end) for start, end in zip([0] + cumsum, cumsum)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinkhorn/matching layer for soft/hard permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class GeneralizedPermutation(Module):\n",
    "    \"\"\"Generalized permutation layer implementing both soft and hard permutations.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        group_sizes: Iterable[int],\n",
    "        fixed_pairings: Optional[IndexPairsInGroups] = None,\n",
    "        tau: float = 1.0,\n",
    "        n_iter: int = 1,\n",
    "        noise: bool = False,\n",
    "        noise_factor: float = 1.0,\n",
    "        noise_std: bool = False,\n",
    "        mode: Literal[\"soft\", \"hard\"] = \"soft\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.group_sizes = tuple(s for s in group_sizes)\n",
    "\n",
    "        self.init_fixed_pairings_and_log_alphas(fixed_pairings)\n",
    "\n",
    "        self.tau = tau\n",
    "        self.n_iter = n_iter\n",
    "        self.noise = noise\n",
    "        self.noise_factor = noise_factor\n",
    "        self.noise_std = noise_std\n",
    "        self.mode = mode\n",
    "\n",
    "    def init_fixed_pairings_and_log_alphas(\n",
    "        self,\n",
    "        fixed_pairings: IndexPairsInGroups,\n",
    "        device: Optional[torch.device] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize fixed pairings and parameterization matrices.\"\"\"\n",
    "        self._validate_fixed_pairings(fixed_pairings)\n",
    "        self.fixed_pairings = fixed_pairings\n",
    "\n",
    "        # Initialize parameterization matrices ('log-alphas')\n",
    "        # By default, initialize all parametrization matrices to zero\n",
    "        self.nonfixed_group_sizes_ = (\n",
    "            tuple(\n",
    "                s - num_efm\n",
    "                for s, num_efm in zip(\n",
    "                    self.group_sizes, self._effective_number_fixed_pairings\n",
    "                )\n",
    "            )\n",
    "            if self.fixed_pairings\n",
    "            else self.group_sizes\n",
    "        )\n",
    "        self.log_alphas = ParameterList(\n",
    "            [\n",
    "                Parameter(torch.zeros(s, s), requires_grad=bool(s))\n",
    "                for s in self.nonfixed_group_sizes_\n",
    "            ]\n",
    "        )\n",
    "        self.to(device=device)\n",
    "\n",
    "    def _validate_fixed_pairings(\n",
    "        self, fixed_pairings: Optional[IndexPairsInGroups] = None\n",
    "    ) -> None:\n",
    "        if fixed_pairings:\n",
    "            if len(fixed_pairings) != len(self.group_sizes):\n",
    "                raise ValueError(\n",
    "                    \"If `fixed_pairings` is provided, it must have the same length as \"\n",
    "                    \"`group_sizes`.\"\n",
    "                )\n",
    "            for s, fm in zip(self.group_sizes, fixed_pairings):\n",
    "                if not fm:\n",
    "                    continue\n",
    "                if any([len(p) != 2 for p in fm]):\n",
    "                    raise ValueError(\n",
    "                        \"All fixed pairings must be pairs of indices (i, j).\"\n",
    "                    )\n",
    "                if any(min(i, j) < 0 or max(i, j) >= s for i, j in fm):\n",
    "                    raise ValueError(\n",
    "                        \"All fixed pairings must be within the range of the corresponding \"\n",
    "                        \"group size.\"\n",
    "                    )\n",
    "            self._effective_number_fixed_pairings = []\n",
    "            self._effective_fixed_pairings_zip = []\n",
    "            for idx, (s, fm) in enumerate(zip(self.group_sizes, fixed_pairings)):\n",
    "                if fm:\n",
    "                    num_fm = len(fm)\n",
    "                    fm_zip = list(zip(*fm))\n",
    "                else:\n",
    "                    num_fm = 0\n",
    "                    fm_zip = [(), ()]\n",
    "                complement = s - num_fm  # Effectively fully fixed when complement <= 1\n",
    "                is_fully_fixed = complement <= 1\n",
    "                num_efm = s - (s - num_fm) * (not is_fully_fixed)\n",
    "                self._effective_number_fixed_pairings.append(num_efm)\n",
    "                if is_fully_fixed:\n",
    "                    mask = torch.zeros(s, s, dtype=torch.bool)\n",
    "                    if complement:\n",
    "                        possible_idxs = set(range(s))\n",
    "                        fm_zip[0] += tuple((possible_idxs - set(fm_zip[0])))\n",
    "                        fm_zip[1] += tuple((possible_idxs - set(fm_zip[1])))\n",
    "                else:\n",
    "                    mask = torch.ones(s, s, dtype=torch.bool)\n",
    "                    for i, j in fm:\n",
    "                        mask[..., j, :] = False\n",
    "                        mask[..., :, i] = False\n",
    "                self.register_buffer(f\"_not_fixed_masks_{idx}\", mask)\n",
    "                self._effective_fixed_pairings_zip.append(fm_zip)\n",
    "            self._total_number_fixed_pairings = sum(\n",
    "                self._effective_number_fixed_pairings\n",
    "            )\n",
    "        else:\n",
    "            self._effective_fixed_pairings_zip = [[(), ()] for _ in self.group_sizes]\n",
    "            self._effective_number_fixed_pairings = [0] * len(self.group_sizes)\n",
    "            self._total_number_fixed_pairings = 0\n",
    "\n",
    "    @property\n",
    "    def _not_fixed_masks(self) -> list[torch.Tensor]:\n",
    "        return [\n",
    "            getattr(self, f\"_not_fixed_masks_{idx}\")\n",
    "            for idx in range(len(self.group_sizes))\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def mode(self) -> str:\n",
    "        return self._mode\n",
    "\n",
    "    @mode.setter\n",
    "    def mode(self, value) -> None:\n",
    "        value = value.lower()\n",
    "        if value not in [\"soft\", \"hard\"]:\n",
    "            raise ValueError(\"mode must be either 'soft' or 'hard'.\")\n",
    "        self._mode = value.lower()\n",
    "        _mats_fn_no_fixed = getattr(self, f\"_{self._mode}_mats\")\n",
    "        self._mats_fn = (\n",
    "            _mats_fn_no_fixed\n",
    "            if not self.fixed_pairings\n",
    "            else self._impl_fixed_pairings(_mats_fn_no_fixed)\n",
    "        )\n",
    "\n",
    "    def soft_(self) -> None:\n",
    "        self.mode = \"soft\"\n",
    "\n",
    "    def hard_(self) -> None:\n",
    "        self.mode = \"hard\"\n",
    "\n",
    "    def _impl_fixed_pairings(self, func: callable) -> callable:\n",
    "        \"\"\"Include fixed matchings in the Gumbel-Sinkhorn or Gumbel-matching operators.\"\"\"\n",
    "\n",
    "        def wrapper(gen: Iterator[torch.Tensor]) -> Iterator[torch.Tensor]:\n",
    "            for s, mat, (row_group, col_group), mask in zip(\n",
    "                self.group_sizes,\n",
    "                gen,\n",
    "                self._effective_fixed_pairings_zip,\n",
    "                self._not_fixed_masks,\n",
    "            ):\n",
    "                mat_all = torch.zeros(\n",
    "                    s,\n",
    "                    s,\n",
    "                    dtype=mat.dtype,\n",
    "                    layout=mat.layout,\n",
    "                    device=mat.device,\n",
    "                )\n",
    "                # mat_all[j, i] = 1 means that row i becomes row j under a permutation,\n",
    "                # using our conventions\n",
    "                mat_all[..., col_group, row_group] = 1\n",
    "                mat_all.masked_scatter_(mask.to(torch.bool), mat)\n",
    "                yield mat_all\n",
    "\n",
    "        return lambda: wrapper(func())\n",
    "\n",
    "    def _soft_mats(self) -> Iterator[torch.Tensor]:\n",
    "        \"\"\"Evaluate the Gumbel-Sinkhorn operator on the current `log_alpha` parameters.\"\"\"\n",
    "        return (\n",
    "            gumbel_sinkhorn(\n",
    "                log_alpha,\n",
    "                tau=self.tau,\n",
    "                n_iter=self.n_iter,\n",
    "                noise=self.noise,\n",
    "                noise_factor=self.noise_factor,\n",
    "                noise_std=self.noise_std,\n",
    "            )\n",
    "            for log_alpha in self.log_alphas\n",
    "        )\n",
    "\n",
    "    def _hard_mats(self) -> Iterator[torch.Tensor]:\n",
    "        \"\"\"Evaluate the Gumbel-matching operator on the current `log_alpha` parameters.\"\"\"\n",
    "        return (\n",
    "            gumbel_matching(\n",
    "                log_alpha,\n",
    "                noise=self.noise,\n",
    "                noise_factor=self.noise_factor,\n",
    "                noise_std=self.noise_std,\n",
    "                unbias_lsa=True,\n",
    "            )\n",
    "            for log_alpha in self.log_alphas\n",
    "        )\n",
    "\n",
    "    def forward(self) -> list[torch.Tensor]:\n",
    "        \"\"\"Compute the soft/hard permutations according to ``self._mats_fn.``\"\"\"\n",
    "        mats = self._mats_fn()\n",
    "\n",
    "        return list(mats)\n",
    "\n",
    "\n",
    "class MatrixApply(Module):\n",
    "    \"\"\"Apply matrices to chunks of a tensor of shape (n_samples, length, alphabet_size)\n",
    "    and collate the results.\"\"\"\n",
    "\n",
    "    def __init__(self, group_sizes: Iterable[int]) -> None:\n",
    "        super().__init__()\n",
    "        self.group_sizes = tuple(s for s in group_sizes)\n",
    "        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, *, mats: Sequence[torch.Tensor]) -> torch.Tensor:\n",
    "        out = torch.full_like(x, torch.nan)\n",
    "        for mats_this_group, sl in zip(mats, self._group_slices):\n",
    "            out[..., sl, :, :].copy_(\n",
    "                torch.tensordot(mats_this_group, x[sl, :, :], dims=1)\n",
    "            )\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class PermutationConjugate(Module):\n",
    "    \"\"\"Conjugate blocks of a square 2D tensor of shape (n_samples, n_samples) by\n",
    "    permutation matrices.\"\"\"\n",
    "\n",
    "    def __init__(self, group_sizes: Iterable[int]) -> None:\n",
    "        super().__init__()\n",
    "        self.group_sizes = tuple(s for s in group_sizes)\n",
    "        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, *, mats: Sequence[torch.Tensor]) -> torch.Tensor:\n",
    "        out1 = torch.full_like(x, torch.nan)\n",
    "        out2 = torch.full_like(x, torch.nan)\n",
    "        # (P * A) * P.T\n",
    "        for mats_this_group, sl in zip(mats, self._group_slices):\n",
    "            out1[..., sl, :].copy_(torch.tensordot(mats_this_group, x[sl, :], dims=1))\n",
    "        for mats_this_group, sl in zip(mats, self._group_slices):\n",
    "            out2[..., :, sl].copy_(\n",
    "                torch.tensordot(\n",
    "                    out1[..., :, sl], mats_this_group.permute((-1, -2)), dims=1\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "def global_argmax_from_group_argmaxes(mats: Iterable[torch.Tensor]) -> torch.Tensor:\n",
    "    global_argmax = []\n",
    "    start_idx = 0\n",
    "    for mats_this_group in mats:\n",
    "        global_argmax.append(mats_this_group.argmax(-1) + start_idx)\n",
    "        start_idx += mats_this_group.shape[-1]\n",
    "\n",
    "    return torch.cat(global_argmax, dim=-1)\n",
    "\n",
    "\n",
    "def apply_hard_permutation_batch_to_similarity(\n",
    "    *, x: torch.Tensor, perms: list[torch.Tensor]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Conjugate a single similarity matrix by a batch of hard permutations.\n",
    "\n",
    "    Args:\n",
    "        perms: List of batches of permutation matrices of shape (..., D, D).\n",
    "        x: Similarity matrix of shape (D, D).\n",
    "\n",
    "    Returns:\n",
    "        Batch of conjugated matrices of shape (..., D, D).\n",
    "    \"\"\"\n",
    "    global_argmax = global_argmax_from_group_argmaxes(perms)\n",
    "    x_permuted_rows = x[global_argmax]\n",
    "\n",
    "    # Permuting columns is more involved\n",
    "    index = global_argmax.view(*global_argmax.shape[:-1], 1, -1).expand(\n",
    "        *global_argmax.shape, global_argmax.shape[-1]\n",
    "    )\n",
    "    # Example of gather with 4D tensor and dim=-1:\n",
    "    # out[i][j][k][l] = input[i][j][k][index[i][j][k][l]]\n",
    "\n",
    "    return torch.gather(x_permuted_rows, -1, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/model.py#L49){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### GeneralizedPermutation\n\n>      GeneralizedPermutation (group_sizes:collections.abc.Iterable[int], fixed_\n>                              pairings:Optional[collections.abc.Sequence[collec\n>                              tions.abc.Sequence[tuple[int,int]]]]=None,\n>                              tau:float=1.0, n_iter:int=1, noise:bool=False,\n>                              noise_factor:float=1.0, noise_std:bool=False,\n>                              mode:Literal['soft','hard']='soft')\n\n*Generalized permutation layer implementing both soft and hard permutations.*",
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/model.py#L49){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GeneralizedPermutation\n",
       "\n",
       ">      GeneralizedPermutation (group_sizes:collections.abc.Iterable[int], fixed_\n",
       ">                              pairings:Optional[collections.abc.Sequence[collec\n",
       ">                              tions.abc.Sequence[tuple[int,int]]]]=None,\n",
       ">                              tau:float=1.0, n_iter:int=1, noise:bool=False,\n",
       ">                              noise_factor:float=1.0, noise_std:bool=False,\n",
       ">                              mode:Literal['soft','hard']='soft')\n",
       "\n",
       "*Generalized permutation layer implementing both soft and hard permutations.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(GeneralizedPermutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for GeneralizedPermutation\n",
    "\n",
    "def test_generalizedpermutation(*, length, alphabet_size, init_kwargs):\n",
    "    group_sizes = init_kwargs[\"group_sizes\"]\n",
    "    n_samples = sum(group_sizes)\n",
    "\n",
    "    x = torch.randn(n_samples, length, alphabet_size)\n",
    "    perm = GeneralizedPermutation(**init_kwargs)\n",
    "    mats = perm()\n",
    "    mat_apply = MatrixApply(group_sizes)\n",
    "    y = mat_apply(x, mats=mats)\n",
    "\n",
    "    assert y.shape == x.shape\n",
    "    assert y.requires_grad\n",
    "\n",
    "    perm.hard_()\n",
    "    assert perm.mode == \"hard\"\n",
    "\n",
    "\n",
    "test_generalizedpermutation(\n",
    "    length=5,\n",
    "    alphabet_size=10,\n",
    "    init_kwargs={\n",
    "        \"group_sizes\": [3, 2, 4],\n",
    "        \"fixed_pairings\": [[(0, 1)], [(0, 0)], [(1, 0), (2, 3)]],\n",
    "        \"tau\": 0.1,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "def test_batch_perm(shape: tuple[int, int, int, int]):\n",
    "    perms = torch.randn(*shape)\n",
    "    x = torch.randn(shape[-2], shape[-1])\n",
    "\n",
    "    argmax = perms.argmax(-1)\n",
    "    x_permuted_rows = x[argmax]\n",
    "    index = argmax.view(*argmax.shape[:-1], 1, -1).expand_as(perms)\n",
    "    output = torch.gather(x_permuted_rows, -1, index)\n",
    "\n",
    "    expected = torch.stack([\n",
    "        torch.stack([\n",
    "            x[argmax[i, j], :][:, argmax[i, j]] for j in range(shape[1])\n",
    "        ], dim=0) for i in range(shape[0])\n",
    "    ], dim=0)\n",
    "\n",
    "    assert torch.equal(output, expected)\n",
    "\n",
    "\n",
    "test_batch_perm((2, 5, 4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information-theory losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TwoBodyEntropyLoss(Module):\n",
    "    \"\"\"Differentiable extension of the mean of estimated two-body entropies between\n",
    "    all pairs of columns from two one-hot encoded tensors.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        return smooth_mean_two_body_entropy(x, y)\n",
    "\n",
    "\n",
    "class MILoss(Module):\n",
    "    \"\"\"Differentiable extension of minus the mean of estimated mutual informations\n",
    "    between all pairs of columns from two one-hot encoded tensors.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        return smooth_mean_two_body_entropy(x, y) - smooth_mean_one_body_entropy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/model.py#L329){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### TwoBodyEntropyLoss\n\n>      TwoBodyEntropyLoss ()\n\n*Differentiable extension of the mean of estimated two-body entropies between\nall pairs of columns from two one-hot encoded tensors.*",
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/model.py#L329){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TwoBodyEntropyLoss\n",
       "\n",
       ">      TwoBodyEntropyLoss ()\n",
       "\n",
       "*Differentiable extension of the mean of estimated two-body entropies between\n",
       "all pairs of columns from two one-hot encoded tensors.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TwoBodyEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/model.py#L340){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MILoss\n\n>      MILoss ()\n\n*Differentiable extension of minus the mean of estimated mutual informations\nbetween all pairs of columns from two one-hot encoded tensors.*",
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/model.py#L340){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MILoss\n",
       "\n",
       ">      MILoss ()\n",
       "\n",
       "*Differentiable extension of minus the mean of estimated mutual informations\n",
       "between all pairs of columns from two one-hot encoded tensors.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MILoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for TwoBodyEntropyScore\n",
    "\n",
    "def test_twobodyentropyloss(\n",
    "        *,\n",
    "        n_samples, length_x, length_y, alphabet_size\n",
    "):\n",
    "    x = torch.randn(\n",
    "        n_samples, length_x, alphabet_size,\n",
    "        requires_grad=True\n",
    "    )\n",
    "    y = torch.randn(n_samples, length_y, alphabet_size)\n",
    "    x_soft = softmax(x, dim=-1)\n",
    "    y_soft = softmax(y, dim=-1)\n",
    "    two_body_entropy_loss = TwoBodyEntropyLoss()\n",
    "    loss = two_body_entropy_loss(x_soft, y_soft)\n",
    "\n",
    "    assert loss.requires_grad\n",
    "\n",
    "    # In the following scenario, the score should be close to log2(alphabet_size)\n",
    "    x_almost_hard = softmax(x / 1e-5, dim=-1)\n",
    "    first_x_almost_hard_length_1 = x_almost_hard[:, :1, :]\n",
    "    loss = two_body_entropy_loss(\n",
    "        first_x_almost_hard_length_1, first_x_almost_hard_length_1\n",
    "    )\n",
    "\n",
    "    torch.testing.assert_close(\n",
    "        loss, torch.log2(torch.tensor(alphabet_size)), atol=1e-3, rtol=1e-7\n",
    "    )\n",
    "\n",
    "\n",
    "test_twobodyentropyloss(\n",
    "    n_samples=10_000,\n",
    "    length_x=3,\n",
    "    length_y=4,\n",
    "    alphabet_size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence similarities (Hamming and Blosum62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class HammingSimilarities(Module):\n",
    "    \"\"\"Compute Hamming similarities between sequences using differentiable\n",
    "    operations.\n",
    "\n",
    "    Optionally, if the sequences are arranged in groups, the computation of\n",
    "    similarities can be restricted to within groups.\n",
    "    Differentiable operations are used to compute the similarities, which can be\n",
    "    either dot products or an L^p distance function.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        group_sizes: Optional[Iterable[int]] = None,\n",
    "        use_dot: bool = True,\n",
    "        p: Optional[float] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.group_sizes = (\n",
    "            tuple(s for s in group_sizes) if group_sizes is not None else None\n",
    "        )\n",
    "        self.use_dot = use_dot\n",
    "        self.p = p\n",
    "\n",
    "        if self.use_dot:\n",
    "            if self.p is not None:\n",
    "                warn(\"Since a `p` was provided, `use_dot` will be ignored.\")\n",
    "            self._similarities_fn = smooth_hamming_similarities_dot\n",
    "            self._similarities_fn_kwargs = {}\n",
    "        else:\n",
    "            if self.p is None:\n",
    "                raise ValueError(\"If `use_dot` is False, `p` must be provided.\")\n",
    "            self._similarities_fn = smooth_hamming_similarities_cdist\n",
    "            self._similarities_fn_kwargs = {\"p\": self.p}\n",
    "\n",
    "        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        size = x.shape[:-3] + (x.shape[-3],) * 2\n",
    "        out = torch.full(\n",
    "            size, torch.nan, dtype=x.dtype, layout=x.layout, device=x.device\n",
    "        )\n",
    "        for sl in self._group_slices:\n",
    "            out[..., sl, sl].copy_(\n",
    "                self._similarities_fn(x[..., sl, :, :], **self._similarities_fn_kwargs)\n",
    "            )\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Blosum62Similarities(Module):\n",
    "    \"\"\"Compute Blosum62-based similarities between sequences using differentiable\n",
    "    operations.\n",
    "\n",
    "    Optionally, if the sequences are arranged in groups, the computation of\n",
    "    similarities can be restricted to within groups.\n",
    "    Differentiable operations are used to compute the similarities, which can be\n",
    "    either dot products or an L^p distance function.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        group_sizes: Optional[Iterable[int]] = None,\n",
    "        use_dot: bool = True,\n",
    "        p: Optional[float] = None,\n",
    "        use_scoredist: bool = False,\n",
    "        aa_to_int: Optional[dict[str, int]] = None,\n",
    "        gaps_as_stars: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.group_sizes = (\n",
    "            tuple(s for s in group_sizes) if group_sizes is not None else None\n",
    "        )\n",
    "        self.use_dot = use_dot\n",
    "        self.p = p\n",
    "        self.use_scoredist = use_scoredist\n",
    "        self.aa_to_int = aa_to_int\n",
    "        self.gaps_as_stars = gaps_as_stars\n",
    "\n",
    "        blosum62_data = get_blosum62_data(\n",
    "            aa_to_int=self.aa_to_int, gaps_as_stars=self.gaps_as_stars\n",
    "        )\n",
    "        self.register_buffer(\"subs_mat\", blosum62_data.mat)\n",
    "        self.expected_value = blosum62_data.expected_value\n",
    "\n",
    "        self._similarities_fn_kwargs = {\"subs_mat\": self.subs_mat}\n",
    "        if self.use_dot:\n",
    "            if self.p is not None:\n",
    "                warn(\"Since a `p` was provided, `use_dot` will be ignored.\")\n",
    "            self._similarities_fn = smooth_substitution_matrix_similarities_dot\n",
    "            self._similarities_fn_kwargs = {\n",
    "                \"use_scoredist\": self.use_scoredist,\n",
    "                \"expected_value\": self.expected_value,\n",
    "            }\n",
    "        else:\n",
    "            if self.p is None:\n",
    "                raise ValueError(\"If `use_dot` is False, `p` must be provided.\")\n",
    "            self._similarities_fn = smooth_substitution_matrix_similarities_cdist\n",
    "            self._similarities_fn_kwargs = {\"p\": self.p}\n",
    "\n",
    "        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        size = x.shape[:-3] + (x.shape[-3],) * 2\n",
    "        out = torch.full(\n",
    "            size, torch.nan, dtype=x.dtype, layout=x.layout, device=x.device\n",
    "        )\n",
    "        for sl in self._group_slices:\n",
    "            out[..., sl, sl].copy_(\n",
    "                self._similarities_fn(\n",
    "                    x[..., sl, :, :],\n",
    "                    subs_mat=self.subs_mat,\n",
    "                    **self._similarities_fn_kwargs,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/model.py#L351){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### HammingSimilarities\n\n>      HammingSimilarities\n>                           (group_sizes:Optional[collections.abc.Iterable[int]]\n>                           =None, use_dot:bool=True, p:Optional[float]=None)\n\nCompute Hamming similarities between sequences using differentiable\noperations.\n\nOptionally, if the sequences are arranged in groups, the computation of\nsimilarities can be restricted to within groups.\nDifferentiable operations are used to compute the similarities, which can be\neither dot products or an L^p distance function.",
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/model.py#L351){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HammingSimilarities\n",
       "\n",
       ">      HammingSimilarities\n",
       ">                           (group_sizes:Optional[collections.abc.Iterable[int]]\n",
       ">                           =None, use_dot:bool=True, p:Optional[float]=None)\n",
       "\n",
       "Compute Hamming similarities between sequences using differentiable\n",
       "operations.\n",
       "\n",
       "Optionally, if the sequences are arranged in groups, the computation of\n",
       "similarities can be restricted to within groups.\n",
       "Differentiable operations are used to compute the similarities, which can be\n",
       "either dot products or an L^p distance function."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(HammingSimilarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/model.py#L400){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### Blosum62Similarities\n\n>      Blosum62Similarities\n>                            (group_sizes:Optional[collections.abc.Iterable[int]\n>                            ]=None, use_dot:bool=True, p:Optional[float]=None,\n>                            use_scoredist:bool=False,\n>                            aa_to_int:Optional[dict[str,int]]=None,\n>                            gaps_as_stars:bool=True)\n\nCompute Blosum62-based similarities between sequences using differentiable\noperations.\n\nOptionally, if the sequences are arranged in groups, the computation of\nsimilarities can be restricted to within groups.\nDifferentiable operations are used to compute the similarities, which can be\neither dot products or an L^p distance function.",
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/model.py#L400){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Blosum62Similarities\n",
       "\n",
       ">      Blosum62Similarities\n",
       ">                            (group_sizes:Optional[collections.abc.Iterable[int]\n",
       ">                            ]=None, use_dot:bool=True, p:Optional[float]=None,\n",
       ">                            use_scoredist:bool=False,\n",
       ">                            aa_to_int:Optional[dict[str,int]]=None,\n",
       ">                            gaps_as_stars:bool=True)\n",
       "\n",
       "Compute Blosum62-based similarities between sequences using differentiable\n",
       "operations.\n",
       "\n",
       "Optionally, if the sequences are arranged in groups, the computation of\n",
       "similarities can be restricted to within groups.\n",
       "Differentiable operations are used to compute the similarities, which can be\n",
       "either dot products or an L^p distance function."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Blosum62Similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for HammingSimilarities and Blosum62Similarities\n",
    "\n",
    "def test_similarities(\n",
    "        *,\n",
    "        cls,\n",
    "        length, alphabet_size,\n",
    "        init_kwargs\n",
    "):\n",
    "    group_sizes = init_kwargs[\"group_sizes\"]\n",
    "    n_samples = sum(group_sizes)\n",
    "\n",
    "    x = torch.randn(\n",
    "        n_samples, length, alphabet_size,\n",
    "        requires_grad=True\n",
    "    )\n",
    "    x_soft = softmax(x, dim=-1)\n",
    "\n",
    "    _init_kwargs = deepcopy(init_kwargs)\n",
    "    _init_kwargs[\"group_sizes\"] = None\n",
    "    similarities = cls(**_init_kwargs)\n",
    "    out_all = similarities(x_soft)\n",
    "\n",
    "    assert out_all.shape == (n_samples, n_samples)\n",
    "\n",
    "    similarities = cls(**init_kwargs)\n",
    "    out = similarities(x_soft)\n",
    "\n",
    "    for sl in similarities._group_slices:\n",
    "        assert torch.allclose(\n",
    "            out[..., sl, sl], out_all[..., sl, sl]\n",
    "        )\n",
    "\n",
    "\n",
    "test_similarities(\n",
    "    cls=HammingSimilarities,\n",
    "    length=3,\n",
    "    alphabet_size=10,\n",
    "    init_kwargs={\"group_sizes\": [3, 2, 4], \"use_dot\": False, \"p\": 1.}\n",
    ")\n",
    "\n",
    "test_similarities(\n",
    "    cls=Blosum62Similarities,\n",
    "    length=3,\n",
    "    alphabet_size=21,\n",
    "    init_kwargs={\"group_sizes\": [3, 2, 4]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best hits from similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BestHits(Module):\n",
    "    \"\"\"Compute (reciprocal) best hits within and between groups of sequences,\n",
    "    starting from a similarity matrix.\n",
    "\n",
    "    Best hits can be either 'hard', in which cases they are computed using the\n",
    "    argmax, or 'soft', in which case they are computed using the softmax with a\n",
    "    temperature parameter `tau`. In both cases, the main diagonal in the similarity\n",
    "    matrix is excluded by setting its entries to minus infinity.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        reciprocal: bool = True,\n",
    "        group_sizes: Optional[Iterable[int]],\n",
    "        tau: float = 0.1,\n",
    "        mode: Literal[\"soft\", \"hard\"] = \"soft\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.reciprocal = reciprocal\n",
    "        self.group_sizes = (\n",
    "            tuple(s for s in group_sizes) if group_sizes is not None else None\n",
    "        )\n",
    "        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)\n",
    "        self.tau = tau\n",
    "        self.mode = mode\n",
    "\n",
    "    @property\n",
    "    def mode(self) -> str:\n",
    "        return self._mode\n",
    "\n",
    "    @mode.setter\n",
    "    def mode(self, value) -> None:\n",
    "        value = value.lower()\n",
    "        if value not in [\"soft\", \"hard\"]:\n",
    "            raise ValueError(\"`mode` must be either 'soft' or 'hard'.\")\n",
    "        self._mode = value.lower()\n",
    "        self._bh_fn = getattr(self, f\"_{self._mode}_bh_fn\")\n",
    "\n",
    "    def soft_(self) -> None:\n",
    "        self.mode = \"soft\"\n",
    "\n",
    "    def hard_(self) -> None:\n",
    "        self.mode = \"hard\"\n",
    "\n",
    "    def _soft_bh_fn(self, similarities: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute soft best hits.\"\"\"\n",
    "        return soft_best_hits(\n",
    "            similarities,\n",
    "            reciprocal=self.reciprocal,\n",
    "            group_slices=self._group_slices,\n",
    "            tau=self.tau,\n",
    "        )\n",
    "\n",
    "    def _hard_bh_fn(self, similarities: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute hard best hits.\"\"\"\n",
    "        return hard_best_hits(\n",
    "            similarities,\n",
    "            reciprocal=self.reciprocal,\n",
    "            group_slices=self._group_slices,\n",
    "        )\n",
    "\n",
    "    def forward(self, similarities: torch.Tensor) -> torch.Tensor:\n",
    "        return self._bh_fn(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/model.py#L469){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### BestHits\n\n>      BestHits (reciprocal:bool=True,\n>                group_sizes:Optional[collections.abc.Iterable[int]],\n>                tau:float=0.1, mode:Literal['soft','hard']='soft')\n\nCompute (reciprocal) best hits within and between groups of sequences,\nstarting from a similarity matrix.\n\nBest hits can be either 'hard', in which cases they are computed using the\nargmax, or 'soft', in which case they are computed using the softmax with a\ntemperature parameter `tau`. In both cases, the main diagonal in the similarity\nmatrix is excluded by setting its entries to minus infinity.",
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/model.py#L469){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BestHits\n",
       "\n",
       ">      BestHits (reciprocal:bool=True,\n",
       ">                group_sizes:Optional[collections.abc.Iterable[int]],\n",
       ">                tau:float=0.1, mode:Literal['soft','hard']='soft')\n",
       "\n",
       "Compute (reciprocal) best hits within and between groups of sequences,\n",
       "starting from a similarity matrix.\n",
       "\n",
       "Best hits can be either 'hard', in which cases they are computed using the\n",
       "argmax, or 'soft', in which case they are computed using the softmax with a\n",
       "temperature parameter `tau`. In both cases, the main diagonal in the similarity\n",
       "matrix is excluded by setting its entries to minus infinity."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(BestHits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses based on comparing similarity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class InterGroupSimilarityLoss(Module):\n",
    "    \"\"\"Compute a loss that compares similarity matrices restricted to inter-group\n",
    "    relationships.\n",
    "\n",
    "    Similarity matrices are expected to be square and symmetric. The loss is computed\n",
    "    by comparing the (unrolled and concatenated) upper triangular blocks containing\n",
    "    inter-group similarities.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        group_sizes: Iterable[int],\n",
    "        score_fn: Union[callable, None] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.group_sizes = tuple(s for s in group_sizes)\n",
    "        self.score_fn = (\n",
    "            partial(torch.tensordot, dims=1) if score_fn is None else score_fn\n",
    "        )\n",
    "\n",
    "        diag_blocks_mask = torch.block_diag(\n",
    "            *[torch.ones((s, s), dtype=torch.bool) for s in self.group_sizes]\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"_upper_no_diag_blocks_mask\", torch.triu(~diag_blocks_mask)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        similarities_x: torch.Tensor,\n",
    "        similarities_y: torch.Tensor,\n",
    "        *,\n",
    "        mats: Optional[Sequence[torch.Tensor]] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # Input validation\n",
    "        assert similarities_x.ndim >= 2 and similarities_y.ndim >= 2\n",
    "\n",
    "        scores = self.score_fn(\n",
    "            similarities_x[..., self._upper_no_diag_blocks_mask],\n",
    "            similarities_y[..., self._upper_no_diag_blocks_mask],\n",
    "        )\n",
    "        loss = -scores\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class IntraGroupSimilarityLoss(Module):\n",
    "    \"\"\"Compute a loss that compares similarity matrices restricted to intra-group\n",
    "    relationships.\n",
    "\n",
    "    Similarity matrices are expected to be square and symmetric. Their diagonal\n",
    "    elements are ignored.\n",
    "    If `group_sizes` is provided, the loss is computed by comparing the (unrolled\n",
    "    and concatenated) upper triangular blocks containing intra-group similarities.\n",
    "    Otherwise, the loss is computed by comparing the upper triangular part of the\n",
    "    full similarity matrices, excluding the main diagonal.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        group_sizes: Optional[Iterable[int]] = None,\n",
    "        score_fn: Union[callable, None] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.group_sizes = (\n",
    "            tuple(s for s in group_sizes) if group_sizes is not None else None\n",
    "        )\n",
    "        self.score_fn = (\n",
    "            partial(torch.tensordot, dims=1) if score_fn is None else score_fn\n",
    "        )\n",
    "\n",
    "        if self.group_sizes is not None:\n",
    "            # Boolean mask for the main diagonal blocks corresponding to groups\n",
    "            diag_blocks_mask = torch.block_diag(\n",
    "                *[torch.ones((s, s), dtype=torch.bool) for s in self.group_sizes]\n",
    "            )\n",
    "            # Extract the upper triangular part, excluding the main diagonal\n",
    "            self.register_buffer(\n",
    "                \"_upper_diag_blocks_mask\", torch.triu(diag_blocks_mask, diagonal=1)\n",
    "            )\n",
    "        else:\n",
    "            self._upper_diag_blocks_mask = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        similarities_x: torch.Tensor,\n",
    "        similarities_y: torch.Tensor,\n",
    "        *,\n",
    "        mats: Optional[Sequence[torch.Tensor]] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        assert similarities_x.ndim >= 2 and similarities_y.ndim >= 2\n",
    "        assert similarities_x.shape[-2:] == similarities_x.shape[-2:]\n",
    "\n",
    "        if self._upper_diag_blocks_mask is None:\n",
    "            mask = torch.triu(\n",
    "                torch.ones(\n",
    "                    similarities_x.shape[-2:],\n",
    "                    dtype=torch.bool,\n",
    "                    layout=similarities_x.layout,\n",
    "                    device=similarities_x.device,\n",
    "                ),\n",
    "                diagonal=1,\n",
    "            )\n",
    "        else:\n",
    "            mask = self._upper_diag_blocks_mask\n",
    "\n",
    "        scores = self.score_fn(similarities_x[..., mask], similarities_y[..., mask])\n",
    "        loss = -scores\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/model.py#L534){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### InterGroupSimilarityLoss\n\n>      InterGroupSimilarityLoss (group_sizes:collections.abc.Iterable[int],\n>                                score_fn:Optional[<built-\n>                                infunctioncallable>]=None)\n\nCompute a loss that compares similarity matrices restricted to inter-group\nrelationships.\n\nSimilarity matrices are expected to be square and symmetric. The loss is computed\nby comparing the (unrolled and concatenated) upper triangular blocks containing\ninter-group similarities.",
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/model.py#L534){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### InterGroupSimilarityLoss\n",
       "\n",
       ">      InterGroupSimilarityLoss (group_sizes:collections.abc.Iterable[int],\n",
       ">                                score_fn:Optional[<built-\n",
       ">                                infunctioncallable>]=None)\n",
       "\n",
       "Compute a loss that compares similarity matrices restricted to inter-group\n",
       "relationships.\n",
       "\n",
       "Similarity matrices are expected to be square and symmetric. The loss is computed\n",
       "by comparing the (unrolled and concatenated) upper triangular blocks containing\n",
       "inter-group similarities."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(InterGroupSimilarityLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/model.py#L580){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### IntraGroupSimilarityLoss\n\n>      IntraGroupSimilarityLoss\n>                                (group_sizes:Optional[collections.abc.Iterable[\n>                                int]]=None, score_fn:Optional[<built-\n>                                infunctioncallable>]=None)\n\nCompute a loss that compares similarity matrices restricted to intra-group\nrelationships.\n\nSimilarity matrices are expected to be square and symmetric. Their diagonal\nelements are ignored.\nIf `group_sizes` is provided, the loss is computed by comparing the (unrolled\nand concatenated) upper triangular blocks containing intra-group similarities.\nOtherwise, the loss is computed by comparing the upper triangular part of the\nfull similarity matrices, excluding the main diagonal.",
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/model.py#L580){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IntraGroupSimilarityLoss\n",
       "\n",
       ">      IntraGroupSimilarityLoss\n",
       ">                                (group_sizes:Optional[collections.abc.Iterable[\n",
       ">                                int]]=None, score_fn:Optional[<built-\n",
       ">                                infunctioncallable>]=None)\n",
       "\n",
       "Compute a loss that compares similarity matrices restricted to intra-group\n",
       "relationships.\n",
       "\n",
       "Similarity matrices are expected to be square and symmetric. Their diagonal\n",
       "elements are ignored.\n",
       "If `group_sizes` is provided, the loss is computed by comparing the (unrolled\n",
       "and concatenated) upper triangular blocks containing intra-group similarities.\n",
       "Otherwise, the loss is computed by comparing the upper triangular part of the\n",
       "full similarity matrices, excluding the main diagonal."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(IntraGroupSimilarityLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for BestHits, InterGroupSimilarityLoss and IntraGroupSimilarityLoss\n",
    "\n",
    "def test_sequence_similarity_losses(\n",
    "        *,\n",
    "        group_sizes,\n",
    "        length_x, length_y, alphabet_size,\n",
    "        extra_init_kwargs_bh, extra_init_kwargs_loss\n",
    "):\n",
    "    similarities = HammingSimilarities(group_sizes=None)\n",
    "    best_hits = BestHits(group_sizes=group_sizes, **extra_init_kwargs_bh)\n",
    "    n_samples = sum(group_sizes)\n",
    "\n",
    "    y = torch.randn(n_samples, length_y, alphabet_size)\n",
    "    y.scatter_(-1, y.argmax(dim=-1, keepdim=True), 1.)\n",
    "    similarities_y = similarities(y)\n",
    "    best_hits.hard_()\n",
    "    best_hits_y = best_hits(similarities_y)\n",
    "    best_hits.soft_()\n",
    "\n",
    "    x = torch.randn(\n",
    "        n_samples, length_x, alphabet_size,\n",
    "        requires_grad=True\n",
    "    )\n",
    "    x_soft = softmax(x, dim=-1)\n",
    "    similarities_x = similarities(x_soft)\n",
    "    best_hits_x = best_hits(similarities_x)\n",
    "\n",
    "    #### Best hits loss ####\n",
    "    inter_group_similarity_loss = InterGroupSimilarityLoss(group_sizes=group_sizes, **extra_init_kwargs_loss)\n",
    "    loss = inter_group_similarity_loss(best_hits_x, best_hits_y)\n",
    "\n",
    "    assert loss.requires_grad\n",
    "\n",
    "    # In the following scenario, the loss should be close to -1\n",
    "    extra_init_kwargs_bh = deepcopy(extra_init_kwargs_bh)\n",
    "    extra_init_kwargs_bh[\"tau\"] = 1e-4\n",
    "    best_hits = BestHits(group_sizes=group_sizes, **extra_init_kwargs_bh)\n",
    "    best_hits_x = best_hits(similarities_y)\n",
    "    loss = inter_group_similarity_loss(best_hits_x, best_hits_y)\n",
    "\n",
    "    torch.testing.assert_close(loss, torch.tensor(-1.))\n",
    "\n",
    "    #### Mirrortree-like loss ####\n",
    "    intra_group_similarity_loss = IntraGroupSimilarityLoss(**extra_init_kwargs_loss)\n",
    "    loss = intra_group_similarity_loss(similarities_x, similarities_y)\n",
    "\n",
    "    assert loss.requires_grad\n",
    "\n",
    "    # In the following scenario, the loss should be close to -1\n",
    "    loss = intra_group_similarity_loss(similarities_y, similarities_y)\n",
    "\n",
    "    torch.testing.assert_close(loss, torch.tensor(-1.))\n",
    "\n",
    "\n",
    "test_sequence_similarity_losses( \n",
    "    length_x=3,\n",
    "    length_y=4,\n",
    "    alphabet_size=3,\n",
    "    group_sizes=[3, 2, 4],\n",
    "    extra_init_kwargs_bh={\n",
    "        \"tau\": 0.1,\n",
    "    },\n",
    "    extra_init_kwargs_loss={\n",
    "        \"score_fn\": torch.nn.CosineSimilarity(dim=-1)\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
