{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train\n",
    "\n",
    "> Perform optimization using DiffPaSS models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Stdlib imports\n",
    "from collections.abc import Sequence\n",
    "from typing import Optional, Any, Literal\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# DiffPaSS imports\n",
    "from diffpass.base import DiffPaSSModel\n",
    "from diffpass.model import (\n",
    "    MatrixApply,\n",
    "    PermutationConjugate,\n",
    "    apply_hard_permutation_batch_to_similarity,\n",
    "    TwoBodyEntropyLoss,\n",
    "    MILoss,\n",
    "    InterGroupSimilarityLoss,\n",
    "    IntraGroupSimilarityLoss,\n",
    ")\n",
    "\n",
    "# Type aliases\n",
    "IndexPair = tuple[int, int]  # Pair of indices\n",
    "IndexPairsInGroup = list[IndexPair]  # Pairs of indices in a group of sequences\n",
    "IndexPairsInGroups = list[IndexPairsInGroup]  # Pairs of indices in groups of sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type aliases\n",
    "\n",
    "```python\n",
    "IndexPair = tuple[int, int]  # Pair of indices\n",
    "IndexPairsInGroup = list[IndexPair]  # Pairs of indices in a group of sequences\n",
    "IndexPairsInGroups = list[IndexPairsInGroup]  # Pairs of indices in groups of sequences\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# Imports for tests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class InformationPairing(DiffPaSSModel):\n",
    "    \"\"\"DiffPaSS model for information-theoretic pairing of multiple sequence alignments (MSAs).\"\"\"\n",
    "\n",
    "    are_inputs_msas = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Number of sequences in each group (e.g. species) of the two MSAs\n",
    "        group_sizes: Sequence[int],\n",
    "        # If not ``None``, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), ...], ...] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc.\n",
    "        fixed_pairings: Optional[IndexPairsInGroups] = None,\n",
    "        # If not ``None``, configuration dictionary containing init parameters for the internal `GeneralizedPermutation` object to compute soft/hard permutations\n",
    "        permutation_cfg: Optional[dict[str, Any]] = None,\n",
    "        # Information-theoretic measure to use. For hard permutations, these two measures are equivalent\n",
    "        information_measure: Literal[\"MI\", \"TwoBodyEntropy\"] = \"TwoBodyEntropy\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize permutation and matrix apply modules\n",
    "        # (self.permutation and self.matrix_apply)\n",
    "        self.init_permutation(\n",
    "            group_sizes=group_sizes,\n",
    "            fixed_pairings=fixed_pairings,\n",
    "            permutation_cfg=permutation_cfg,\n",
    "        )\n",
    "        self.matrix_apply = MatrixApply(group_sizes=self.group_sizes)\n",
    "\n",
    "        # Initialize information-theoretic loss module\n",
    "        self.validate_information_measure(information_measure)\n",
    "        self.information_measure = information_measure\n",
    "        if self.information_measure == \"TwoBodyEntropy\":\n",
    "            self.information_loss = TwoBodyEntropyLoss()\n",
    "        elif self.information_measure == \"MI\":\n",
    "            self.information_loss = MILoss()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        # Soft or hard permutations (list)\n",
    "        perms = self.permutation()\n",
    "        x_perm = self.matrix_apply(x, mats=perms)\n",
    "\n",
    "        # Two-body entropy portion of the loss\n",
    "        loss = self.information_loss(x_perm, y)\n",
    "\n",
    "        return {\"perms\": perms, \"x_perm\": x_perm, \"loss\": loss}\n",
    "\n",
    "    def prepare_fit(self, x: torch.Tensor, y: torch.Tensor) -> None:\n",
    "        # Validate inputs\n",
    "        self.validate_inputs(x, y, check_same_alphabet_size=True)\n",
    "\n",
    "    def compute_losses_identity_perm(\n",
    "        self, x: torch.Tensor, y: torch.Tensor\n",
    "    ) -> dict[str, float]:\n",
    "        # Compute hard/soft losses when using identity permutation\n",
    "        self.hard_()\n",
    "        with torch.no_grad():\n",
    "            hard_loss_identity_perm = self.information_loss(x, y).item()\n",
    "            soft_loss_identity_perm = hard_loss_identity_perm\n",
    "\n",
    "        return {\"hard\": hard_loss_identity_perm, \"soft\": soft_loss_identity_perm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/train.py#L36){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### InformationPairing\n\n>      InformationPairing (group_sizes:collections.abc.Sequence[int],\n>                          fixed_pairings:Optional[list[list[tuple[int,int]]]]=N\n>                          one, permutation_cfg:Optional[dict[str,Any]]=None, in\n>                          formation_measure:Literal['MI','TwoBodyEntropy']='Two\n>                          BodyEntropy')\n\n*DiffPaSS model for information-theoretic pairing of multiple sequence alignments (MSAs).*\n\n|    | **Type** | **Default** | **Details** |\n| -- | -------- | ----------- | ----------- |\n| group_sizes | Sequence |  | Number of sequences in each group (e.g. species) of the two MSAs |\n| fixed_pairings | Optional | None | If not ``None``, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), ...], ...] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc. |\n| permutation_cfg | Optional | None | If not ``None``, configuration dictionary containing init parameters for the internal `GeneralizedPermutation` object to compute soft/hard permutations |\n| information_measure | Literal | TwoBodyEntropy | Information-theoretic measure to use. For hard permutations, these two measures are equivalent |",
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/train.py#L36){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### InformationPairing\n",
       "\n",
       ">      InformationPairing (group_sizes:collections.abc.Sequence[int],\n",
       ">                          fixed_pairings:Optional[list[list[tuple[int,int]]]]=N\n",
       ">                          one, permutation_cfg:Optional[dict[str,Any]]=None, in\n",
       ">                          formation_measure:Literal['MI','TwoBodyEntropy']='Two\n",
       ">                          BodyEntropy')\n",
       "\n",
       "*DiffPaSS model for information-theoretic pairing of multiple sequence alignments (MSAs).*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| group_sizes | Sequence |  | Number of sequences in each group (e.g. species) of the two MSAs |\n",
       "| fixed_pairings | Optional | None | If not ``None``, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), ...], ...] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc. |\n",
       "| permutation_cfg | Optional | None | If not ``None``, configuration dictionary containing init parameters for the internal `GeneralizedPermutation` object to compute soft/hard permutations |\n",
       "| information_measure | Literal | TwoBodyEntropy | Information-theoretic measure to use. For hard permutations, these two measures are equivalent |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(InformationPairing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_information_bootstrap():\n",
    "    # Data: two highly correlated MSAs\n",
    "    n_classes = 3\n",
    "    length = 5\n",
    "    size_each_group = 10\n",
    "    n_groups = 10\n",
    "    # Define first MSA group by group\n",
    "    x_tok_by_group = [torch.randint(0, n_classes, (size_each_group, length)) for _ in range(n_groups)]\n",
    "    # Within-group shuffling to control for algorithmic biases towards identity permutation\n",
    "    x_tok_by_group_shuffle = [x[torch.randperm(size_each_group)] for x in x_tok_by_group]\n",
    "    x_tok = torch.cat(x_tok_by_group, dim=0)\n",
    "    x_tok_shuffle = torch.cat(x_tok_by_group_shuffle, dim=0)\n",
    "    y_tok = (x_tok + 1) % n_classes\n",
    "    x = torch.nn.functional.one_hot(x_tok).to(torch.get_default_dtype())\n",
    "    x_shuffle = torch.nn.functional.one_hot(x_tok_shuffle).to(torch.get_default_dtype())\n",
    "    y = torch.nn.functional.one_hot(y_tok).to(torch.get_default_dtype())\n",
    "\n",
    "    group_sizes = [size_each_group] * n_groups\n",
    "\n",
    "    # Model\n",
    "    model = InformationPairing(group_sizes=group_sizes)\n",
    "    results = model.fit_bootstrap(x_shuffle, y)\n",
    "    hard_loss_identity_perm = model.compute_losses_identity_perm(x, y)[\"hard\"]\n",
    "\n",
    "    # Check that the hard loss of the optimized permutation is close to the ground truth\n",
    "    assert np.abs(results.hard_losses[-2][-1] - hard_loss_identity_perm) < 1e-4\n",
    "\n",
    "test_information_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BestHitsPairing(DiffPaSSModel):\n",
    "    \"\"\"DiffPaSS model for pairing of multiple sequence alignments (MSAs) by aligning their orthology networks, constructed using (reciprocal) best hits .\"\"\"\n",
    "\n",
    "    are_inputs_msas = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Number of sequences in each group (e.g. species) of the two MSAs\n",
    "        group_sizes: Sequence[int],\n",
    "        # If not ``None``, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), ...], ...] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc.\n",
    "        fixed_pairings: Optional[IndexPairsInGroups] = None,\n",
    "        # If not ``None``, configuration dictionary containing init parameters for the internal `GeneralizedPermutation` object to compute soft/hard permutations\n",
    "        permutation_cfg: Optional[dict[str, Any]] = None,\n",
    "        # (Smoothly extended) similarity metric to use on all pairs of aligned sequences\n",
    "        similarity_kind: Literal[\"Hamming\", \"Blosum62\"] = \"Hamming\",\n",
    "        # If not ``None``, configuration dictionary containing init parameters for the internal `HammingSimilarities` or `Blosum62Similarities` object to compute similarity matrices\n",
    "        similarities_cfg: Optional[dict[str, Any]] = None,\n",
    "        # Whether to also compute best hits within each group (in addition to between different groups)\n",
    "        compute_in_group_best_hits: bool = True,\n",
    "        # If not ``None``, configuration dictionary containing init parameters for the internal `BestHits` object to compute soft/hard (reciprocal) best hits\n",
    "        best_hits_cfg: Optional[dict[str, Any]] = None,\n",
    "        # If not ``None``, custom callable to compute the differentiable loss between the soft/hard best hits matrices of the two MSAs\n",
    "        similarities_comparison_loss: Optional[callable] = None,\n",
    "        # Whether to compare the soft best hits from the MSA to permute (``x``) to the hard or soft best hits from the reference MSA (``y``)\n",
    "        compare_soft_best_hits_to_hard: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize permutation and matrix apply modules\n",
    "        # (self.permutation and self.matrix_apply)\n",
    "        self.init_permutation(\n",
    "            group_sizes=group_sizes,\n",
    "            fixed_pairings=fixed_pairings,\n",
    "            permutation_cfg=permutation_cfg,\n",
    "        )\n",
    "        self.matrix_apply = MatrixApply(group_sizes=self.group_sizes)\n",
    "\n",
    "        # Validate similarity kind/config and initialize similarities module\n",
    "        self.init_similarities(\n",
    "            similarity_kind=similarity_kind, similarities_cfg=similarities_cfg\n",
    "        )\n",
    "\n",
    "        # Validate best hits config and initialize best hits module\n",
    "        self.compute_in_group_best_hits = compute_in_group_best_hits\n",
    "        self.init_best_hits(best_hits_cfg)\n",
    "\n",
    "        self.compare_soft_best_hits_to_hard = compare_soft_best_hits_to_hard\n",
    "\n",
    "        # Similarities comparison loss\n",
    "        self.similarities_comparison_loss = similarities_comparison_loss\n",
    "        if self.similarities_comparison_loss is None:\n",
    "            self.effective_similarities_comparison_loss_ = InterGroupSimilarityLoss(\n",
    "                group_sizes=self.group_sizes\n",
    "            )\n",
    "        else:\n",
    "            self.effective_similarities_comparison_loss_ = (\n",
    "                self.similarities_comparison_loss\n",
    "            )\n",
    "\n",
    "    def _precompute_bh(self, x: torch.Tensor, y: torch.Tensor) -> None:\n",
    "        mode = self.best_hits.mode\n",
    "\n",
    "        # Temporarily switch to hard BH\n",
    "        self.best_hits.hard_()\n",
    "        similarities_x = self.similarities(x)\n",
    "        self.register_buffer(\"_bh_hard_x\", self.best_hits(similarities_x))\n",
    "        similarities_y = self.similarities(y)\n",
    "        self.register_buffer(\"_bh_hard_y\", self.best_hits(similarities_y))\n",
    "\n",
    "        # Switch to soft BH\n",
    "        self.best_hits.soft_()\n",
    "        self.register_buffer(\"_bh_soft_x\", self.best_hits(similarities_x))\n",
    "        self.register_buffer(\"_bh_soft_y\", self.best_hits(similarities_y))\n",
    "\n",
    "        # Restore initial mode\n",
    "        self.best_hits.mode = mode\n",
    "\n",
    "    @property\n",
    "    def _bh_y_for_soft_x(self):\n",
    "        if self.compare_soft_best_hits_to_hard:\n",
    "            return self._bh_hard_y\n",
    "        return self._bh_soft_y\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, y: Optional[torch.Tensor] = None\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        mode = self.permutation.mode\n",
    "        assert (\n",
    "            mode == self.best_hits.mode\n",
    "        ), \"Permutation and best hits must be either both in soft mode or both in hard mode.\"\n",
    "\n",
    "        # Soft or hard permutations\n",
    "        perms = self.permutation()\n",
    "        x_perm = self.matrix_apply(x, mats=perms)\n",
    "\n",
    "        # Best hits loss, with shortcut for hard permutations\n",
    "        if mode == \"soft\":\n",
    "            similarities_x = self.similarities(x_perm)\n",
    "            bh_x = self.best_hits(similarities_x)\n",
    "            # Ensure comparisons are soft_x-{soft,hard}_y, depending on\n",
    "            # self.compare_soft_best_hits_to_hard\n",
    "            loss = self.effective_similarities_comparison_loss_(\n",
    "                bh_x, self._bh_y_for_soft_x\n",
    "            )\n",
    "        else:\n",
    "            bh_x = apply_hard_permutation_batch_to_similarity(\n",
    "                x=self._bh_hard_x, perms=perms\n",
    "            )\n",
    "            loss = self.effective_similarities_comparison_loss_(bh_x, self._bh_hard_y)\n",
    "\n",
    "        return {\n",
    "            \"perms\": perms,\n",
    "            \"x_perm\": x_perm,\n",
    "            \"loss\": loss,\n",
    "        }\n",
    "\n",
    "    def prepare_fit(self, x: torch.Tensor, y: torch.Tensor) -> None:\n",
    "        # Validate inputs\n",
    "        self.validate_inputs(x, y, check_same_alphabet_size=True)\n",
    "\n",
    "        # Precompute matrices of best hits\n",
    "        self._precompute_bh(x, y)\n",
    "\n",
    "    def compute_losses_identity_perm(\n",
    "        self, x: torch.Tensor, y: torch.Tensor\n",
    "    ) -> dict[str, float]:\n",
    "        # Precompute matrices of best hits\n",
    "        self._precompute_bh(x, y)\n",
    "\n",
    "        # Compute hard/soft losses when using identity permutation\n",
    "        with torch.no_grad():\n",
    "            hard_loss_identity_perm = self.effective_similarities_comparison_loss_(\n",
    "                self._bh_hard_x, self._bh_hard_y\n",
    "            ).item()\n",
    "            soft_loss_identity_perm = self.effective_similarities_comparison_loss_(\n",
    "                self._bh_soft_x, self._bh_y_for_soft_x\n",
    "            ).item()\n",
    "\n",
    "        return {\"hard\": hard_loss_identity_perm, \"soft\": soft_loss_identity_perm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/train.py#L101){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### BestHitsPairing\n\n>      BestHitsPairing (group_sizes:collections.abc.Sequence[int],\n>                       fixed_pairings:Optional[list[list[tuple[int,int]]]]=None\n>                       , permutation_cfg:Optional[dict[str,Any]]=None,\n>                       similarity_kind:Literal['Hamming','Blosum62']='Hamming',\n>                       similarities_cfg:Optional[dict[str,Any]]=None,\n>                       compute_in_group_best_hits:bool=True,\n>                       best_hits_cfg:Optional[dict[str,Any]]=None,\n>                       similarities_comparison_loss:Optional[<built-\n>                       infunctioncallable>]=None,\n>                       compare_soft_best_hits_to_hard:bool=True)\n\n*DiffPaSS model for pairing of multiple sequence alignments (MSAs) by aligning their orthology networks, constructed using (reciprocal) best hits .*\n\n|    | **Type** | **Default** | **Details** |\n| -- | -------- | ----------- | ----------- |\n| group_sizes | Sequence |  | Number of sequences in each group (e.g. species) of the two MSAs |\n| fixed_pairings | Optional | None | If not ``None``, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), ...], ...] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc. |\n| permutation_cfg | Optional | None | If not ``None``, configuration dictionary containing init parameters for the internal `GeneralizedPermutation` object to compute soft/hard permutations |\n| similarity_kind | Literal | Hamming | (Smoothly extended) similarity metric to use on all pairs of aligned sequences |\n| similarities_cfg | Optional | None | If not ``None``, configuration dictionary containing init parameters for the internal `HammingSimilarities` or `Blosum62Similarities` object to compute similarity matrices |\n| compute_in_group_best_hits | bool | True | Whether to also compute best hits within each group (in addition to between different groups) |\n| best_hits_cfg | Optional | None | If not ``None``, configuration dictionary containing init parameters for the internal `BestHits` object to compute soft/hard (reciprocal) best hits |\n| similarities_comparison_loss | Optional | None | If not ``None``, custom callable to compute the differentiable loss between the soft/hard best hits matrices of the two MSAs |\n| compare_soft_best_hits_to_hard | bool | True | Whether to compare the soft best hits from the MSA to permute (``x``) to the hard or soft best hits from the reference MSA (``y``) |",
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/train.py#L101){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BestHitsPairing\n",
       "\n",
       ">      BestHitsPairing (group_sizes:collections.abc.Sequence[int],\n",
       ">                       fixed_pairings:Optional[list[list[tuple[int,int]]]]=None\n",
       ">                       , permutation_cfg:Optional[dict[str,Any]]=None,\n",
       ">                       similarity_kind:Literal['Hamming','Blosum62']='Hamming',\n",
       ">                       similarities_cfg:Optional[dict[str,Any]]=None,\n",
       ">                       compute_in_group_best_hits:bool=True,\n",
       ">                       best_hits_cfg:Optional[dict[str,Any]]=None,\n",
       ">                       similarities_comparison_loss:Optional[<built-\n",
       ">                       infunctioncallable>]=None,\n",
       ">                       compare_soft_best_hits_to_hard:bool=True)\n",
       "\n",
       "*DiffPaSS model for pairing of multiple sequence alignments (MSAs) by aligning their orthology networks, constructed using (reciprocal) best hits .*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| group_sizes | Sequence |  | Number of sequences in each group (e.g. species) of the two MSAs |\n",
       "| fixed_pairings | Optional | None | If not ``None``, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), ...], ...] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc. |\n",
       "| permutation_cfg | Optional | None | If not ``None``, configuration dictionary containing init parameters for the internal `GeneralizedPermutation` object to compute soft/hard permutations |\n",
       "| similarity_kind | Literal | Hamming | (Smoothly extended) similarity metric to use on all pairs of aligned sequences |\n",
       "| similarities_cfg | Optional | None | If not ``None``, configuration dictionary containing init parameters for the internal `HammingSimilarities` or `Blosum62Similarities` object to compute similarity matrices |\n",
       "| compute_in_group_best_hits | bool | True | Whether to also compute best hits within each group (in addition to between different groups) |\n",
       "| best_hits_cfg | Optional | None | If not ``None``, configuration dictionary containing init parameters for the internal `BestHits` object to compute soft/hard (reciprocal) best hits |\n",
       "| similarities_comparison_loss | Optional | None | If not ``None``, custom callable to compute the differentiable loss between the soft/hard best hits matrices of the two MSAs |\n",
       "| compare_soft_best_hits_to_hard | bool | True | Whether to compare the soft best hits from the MSA to permute (``x``) to the hard or soft best hits from the reference MSA (``y``) |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(BestHitsPairing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_besthits_bootstrap():\n",
    "    # Data: two highly correlated MSAs\n",
    "    n_classes = 3\n",
    "    length = 100\n",
    "    size_each_group = 10\n",
    "    n_groups = 10\n",
    "    # Define first MSA group by group\n",
    "    x_tok_by_group = [torch.randint(0, n_classes, (size_each_group, length)) for _ in range(n_groups)]\n",
    "    # Within-group shuffling to control for algorithmic biases towards identity permutation\n",
    "    x_tok_by_group_shuffle = [x[torch.randperm(size_each_group)] for x in x_tok_by_group]\n",
    "    x_tok = torch.cat(x_tok_by_group, dim=0)\n",
    "    x_tok_shuffle = torch.cat(x_tok_by_group_shuffle, dim=0)\n",
    "    y_tok = (x_tok + 1) % n_classes\n",
    "    x = torch.nn.functional.one_hot(x_tok).to(torch.get_default_dtype())\n",
    "    x_shuffle = torch.nn.functional.one_hot(x_tok_shuffle).to(torch.get_default_dtype())\n",
    "    y = torch.nn.functional.one_hot(y_tok).to(torch.get_default_dtype())\n",
    "\n",
    "    group_sizes = [size_each_group] * n_groups\n",
    "\n",
    "    # Model\n",
    "    model = BestHitsPairing(group_sizes=group_sizes)\n",
    "    results = model.fit_bootstrap(x_shuffle, y)\n",
    "    target_hard_loss = model.compute_losses_identity_perm(x, y)[\"hard\"]\n",
    "\n",
    "    # Check that the hard loss of the optimized permutation is close to the ground truth\n",
    "    assert results.hard_losses[-2][-1] / target_hard_loss > 0.7\n",
    "\n",
    "test_besthits_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MirrortreePairing(DiffPaSSModel):\n",
    "    \"\"\"DiffPaSS model for pairing of multiple sequence alignments (MSAs) by aligning their sequence distance networks as in the Mirrortree method.\"\"\"\n",
    "\n",
    "    are_inputs_msas = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Number of sequences in each group (e.g. species) of the two MSAs\n",
    "        group_sizes: Sequence[int],\n",
    "        # If not ``None``, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), ...], ...] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc.\n",
    "        fixed_pairings: Optional[IndexPairsInGroups] = None,\n",
    "        # If not ``None``, configuration dictionary containing init parameters for the internal `GeneralizedPermutation` object to compute soft/hard permutations\n",
    "        permutation_cfg: Optional[dict[str, Any]] = None,\n",
    "        # (Smoothly extended) similarity metric to use on all pairs of aligned sequences\n",
    "        similarity_kind: Literal[\"Hamming\", \"Blosum62\"] = \"Hamming\",\n",
    "        # If not ``None``, configuration dictionary containing init parameters for the internal `HammingSimilarities` or `Blosum62Similarities` object to compute similarity matrices\n",
    "        similarities_cfg: Optional[dict[str, Any]] = None,\n",
    "        # If not ``None``, custom callable to compute the differentiable loss between the similarity matrix of the two MSAs. Default: `IntraGroupSimilarityLoss`\n",
    "        similarities_comparison_loss: Optional[callable] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize permutation and matrix apply modules\n",
    "        # (self.permutation and self.matrix_apply)\n",
    "        self.init_permutation(\n",
    "            group_sizes=group_sizes,\n",
    "            fixed_pairings=fixed_pairings,\n",
    "            permutation_cfg=permutation_cfg,\n",
    "        )\n",
    "        self.matrix_apply = MatrixApply(group_sizes=self.group_sizes)\n",
    "\n",
    "        # Validate similarity kind/config and initialize similarities module\n",
    "        self.init_similarities(\n",
    "            similarity_kind=similarity_kind, similarities_cfg=similarities_cfg\n",
    "        )\n",
    "\n",
    "        #  Similarities comparison loss\n",
    "        self.similarities_comparison_loss = similarities_comparison_loss\n",
    "        if self.similarities_comparison_loss is None:\n",
    "            self.effective_similarities_comparison_loss_ = IntraGroupSimilarityLoss(\n",
    "                group_sizes=self.group_sizes\n",
    "            )\n",
    "        else:\n",
    "            self.effective_similarities_comparison_loss_ = (\n",
    "                self.similarities_comparison_loss\n",
    "            )\n",
    "\n",
    "    def _precompute_similarities(self, x: torch.Tensor, y: torch.Tensor) -> None:\n",
    "        self.register_buffer(\"_similarities_hard_x\", self.similarities(x))\n",
    "        self.register_buffer(\"_similarities_hard_y\", self.similarities(y))\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, y: Optional[torch.Tensor] = None\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        mode = self.permutation.mode\n",
    "\n",
    "        # Soft or hard permutations (list)\n",
    "        perms = self.permutation()\n",
    "        x_perm = self.matrix_apply(x, mats=perms)\n",
    "\n",
    "        # Compute similarity matrix of soft- or hard-permuted x\n",
    "        if mode == \"soft\":\n",
    "            similarities_x = self.similarities(x_perm)\n",
    "        else:\n",
    "            similarities_x = apply_hard_permutation_batch_to_similarity(\n",
    "                x=self._similarities_hard_x, perms=perms\n",
    "            )\n",
    "\n",
    "        loss = self.effective_similarities_comparison_loss_(\n",
    "            similarities_x, self._similarities_hard_y\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"perms\": perms,\n",
    "            \"x_perm\": x_perm,\n",
    "            \"loss\": loss,\n",
    "        }\n",
    "\n",
    "    def prepare_fit(self, x: torch.Tensor, y: torch.Tensor) -> None:\n",
    "        # Validate inputs\n",
    "        self.validate_inputs(x, y, check_same_alphabet_size=True)\n",
    "\n",
    "        # Precompute similarity matrices\n",
    "        self._precompute_similarities(x, y)\n",
    "\n",
    "    def compute_losses_identity_perm(\n",
    "        self, x: torch.Tensor, y: torch.Tensor\n",
    "    ) -> dict[str, float]:\n",
    "        # Precompute matrices of best hits\n",
    "        self._precompute_similarities(x, y)\n",
    "\n",
    "        # Compute hard/soft losses when using identity permutation\n",
    "        with torch.no_grad():\n",
    "            hard_loss_identity_perm = self.effective_similarities_comparison_loss_(\n",
    "                self._similarities_hard_x, self._similarities_hard_y\n",
    "            ).item()\n",
    "            soft_loss_identity_perm = hard_loss_identity_perm\n",
    "\n",
    "        return {\"hard\": hard_loss_identity_perm, \"soft\": soft_loss_identity_perm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/train.py#L242){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MirrortreePairing\n\n>      MirrortreePairing (group_sizes:collections.abc.Sequence[int],\n>                         fixed_pairings:Optional[list[list[tuple[int,int]]]]=No\n>                         ne, permutation_cfg:Optional[dict[str,Any]]=None, simi\n>                         larity_kind:Literal['Hamming','Blosum62']='Hamming',\n>                         similarities_cfg:Optional[dict[str,Any]]=None,\n>                         similarities_comparison_loss:Optional[<built-\n>                         infunctioncallable>]=None)\n\n*DiffPaSS model for pairing of multiple sequence alignments (MSAs) by aligning their sequence distance networks as in the Mirrortree method.*\n\n|    | **Type** | **Default** | **Details** |\n| -- | -------- | ----------- | ----------- |\n| group_sizes | Sequence |  | Number of sequences in each group (e.g. species) of the two MSAs |\n| fixed_pairings | Optional | None | If not ``None``, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), ...], ...] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc. |\n| permutation_cfg | Optional | None | If not ``None``, configuration dictionary containing init parameters for the internal `GeneralizedPermutation` object to compute soft/hard permutations |\n| similarity_kind | Literal | Hamming | (Smoothly extended) similarity metric to use on all pairs of aligned sequences |\n| similarities_cfg | Optional | None | If not ``None``, configuration dictionary containing init parameters for the internal `HammingSimilarities` or `Blosum62Similarities` object to compute similarity matrices |\n| similarities_comparison_loss | Optional | None | If not ``None``, custom callable to compute the differentiable loss between the similarity matrix of the two MSAs. Default: `IntraGroupSimilarityLoss` |",
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/train.py#L242){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MirrortreePairing\n",
       "\n",
       ">      MirrortreePairing (group_sizes:collections.abc.Sequence[int],\n",
       ">                         fixed_pairings:Optional[list[list[tuple[int,int]]]]=No\n",
       ">                         ne, permutation_cfg:Optional[dict[str,Any]]=None, simi\n",
       ">                         larity_kind:Literal['Hamming','Blosum62']='Hamming',\n",
       ">                         similarities_cfg:Optional[dict[str,Any]]=None,\n",
       ">                         similarities_comparison_loss:Optional[<built-\n",
       ">                         infunctioncallable>]=None)\n",
       "\n",
       "*DiffPaSS model for pairing of multiple sequence alignments (MSAs) by aligning their sequence distance networks as in the Mirrortree method.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| group_sizes | Sequence |  | Number of sequences in each group (e.g. species) of the two MSAs |\n",
       "| fixed_pairings | Optional | None | If not ``None``, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), ...], ...] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc. |\n",
       "| permutation_cfg | Optional | None | If not ``None``, configuration dictionary containing init parameters for the internal `GeneralizedPermutation` object to compute soft/hard permutations |\n",
       "| similarity_kind | Literal | Hamming | (Smoothly extended) similarity metric to use on all pairs of aligned sequences |\n",
       "| similarities_cfg | Optional | None | If not ``None``, configuration dictionary containing init parameters for the internal `HammingSimilarities` or `Blosum62Similarities` object to compute similarity matrices |\n",
       "| similarities_comparison_loss | Optional | None | If not ``None``, custom callable to compute the differentiable loss between the similarity matrix of the two MSAs. Default: `IntraGroupSimilarityLoss` |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MirrortreePairing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mirrortree_bootstrap():\n",
    "    # Data: two highly correlated MSAs\n",
    "    n_classes = 3\n",
    "    length = 100\n",
    "    size_each_group = 10\n",
    "    n_groups = 10\n",
    "    # Define first MSA group by group\n",
    "    x_tok_by_group = [torch.randint(0, n_classes, (size_each_group, length)) for _ in range(n_groups)]\n",
    "    # Within-group shuffling to control for algorithmic biases towards identity permutation\n",
    "    x_tok_by_group_shuffle = [x[torch.randperm(size_each_group)] for x in x_tok_by_group]\n",
    "    x_tok = torch.cat(x_tok_by_group, dim=0)\n",
    "    x_tok_shuffle = torch.cat(x_tok_by_group_shuffle, dim=0)\n",
    "    y_tok = (x_tok + 1) % n_classes\n",
    "    x = torch.nn.functional.one_hot(x_tok).to(torch.get_default_dtype())\n",
    "    x_shuffle = torch.nn.functional.one_hot(x_tok_shuffle).to(torch.get_default_dtype())\n",
    "    y = torch.nn.functional.one_hot(y_tok).to(torch.get_default_dtype())\n",
    "\n",
    "    group_sizes = [size_each_group] * n_groups\n",
    "\n",
    "    # Model\n",
    "    model = MirrortreePairing(group_sizes=group_sizes)\n",
    "    results = model.fit_bootstrap(x_shuffle, y)\n",
    "    target_hard_loss = model.compute_losses_identity_perm(x, y)[\"hard\"]\n",
    "\n",
    "    # Check that the hard loss of the optimized permutation is close to the ground truth\n",
    "    assert results.hard_losses[-2][-1] / target_hard_loss > 0.95\n",
    "\n",
    "test_mirrortree_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class GraphAlignment(DiffPaSSModel):\n",
    "    \"\"\"DiffPaSS model for general graph alignment starting from the weighted adjacency matrices of two graphs.\"\"\"\n",
    "\n",
    "    are_inputs_msas = False\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Number of graph nodes in each group (e.g. species), assumed the same between the two graphs to align\n",
    "        group_sizes: Sequence[int],\n",
    "        # If not ``None``, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), ...], ...] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc.\n",
    "        fixed_pairings: Optional[IndexPairsInGroups] = None,\n",
    "        # If not ``None``, configuration dictionary containing init parameters for the internal `GeneralizedPermutation` object to compute soft/hard permutations. Soft/hard permutations ``P`` act on adjacency matrices ``X`` via ``P @ X @ P.T``\n",
    "        permutation_cfg: Optional[dict[str, Any]] = None,\n",
    "        # If not ``None``, custom callable to compute the differentiable loss between the soft/hard-permuted adjacency matrix of graph ``x`` and the adjacency matrix of graph ``y``. Defaults to dot product between all upper triangular elements\n",
    "        comparison_loss: Optional[callable] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize permutation and matrix apply modules\n",
    "        # (self.permutation and self.matrix_apply)\n",
    "        self.init_permutation(\n",
    "            group_sizes=group_sizes,\n",
    "            fixed_pairings=fixed_pairings,\n",
    "            permutation_cfg=permutation_cfg,\n",
    "        )\n",
    "        self.permutation_conjugate = PermutationConjugate(group_sizes=self.group_sizes)\n",
    "\n",
    "        #  Comparison loss\n",
    "        self.comparison_loss = comparison_loss\n",
    "        if self.comparison_loss is None:\n",
    "            # Default: dot product between all upper triangular elements\n",
    "            self.effective_comparison_loss_ = IntraGroupSimilarityLoss(group_sizes=None)\n",
    "        else:\n",
    "            self.effective_comparison_loss_ = self.comparison_loss\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> dict[str, torch.Tensor]:\n",
    "        mode = self.permutation.mode\n",
    "\n",
    "        # Soft or hard permutations (list)\n",
    "        perms = self.permutation()\n",
    "\n",
    "        # Conjugate adjacency matrix x by soft/hard permutation P: P @ x @ P.T\n",
    "        if mode == \"soft\":\n",
    "            x_perm = self.permutation_conjugate(x, mats=perms)\n",
    "        else:\n",
    "            x_perm = apply_hard_permutation_batch_to_similarity(x=x, perms=perms)\n",
    "        loss = self.effective_comparison_loss_(x_perm, y, mats=perms)\n",
    "\n",
    "        return {\n",
    "            \"perms\": perms,\n",
    "            \"x_perm\": x_perm,\n",
    "            \"loss\": loss,\n",
    "        }\n",
    "\n",
    "    def prepare_fit(self, x: torch.Tensor, y: torch.Tensor) -> None:\n",
    "        # Validate inputs\n",
    "        self.validate_inputs(x, y)\n",
    "\n",
    "    def compute_losses_identity_perm(\n",
    "        self, x: torch.Tensor, y: torch.Tensor\n",
    "    ) -> dict[str, float]:\n",
    "        # Compute hard/soft losses when using identity permutation\n",
    "        with torch.no_grad():\n",
    "            hard_loss_identity_perm = self.effective_comparison_loss_(\n",
    "                x, y, mats=[torch.eye(s).to(x.device) for s in self.group_sizes]\n",
    "            ).item()\n",
    "            soft_loss_identity_perm = hard_loss_identity_perm\n",
    "\n",
    "        return {\"hard\": hard_loss_identity_perm, \"soft\": soft_loss_identity_perm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/train.py#L343){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### GraphAlignment\n\n>      GraphAlignment (group_sizes:collections.abc.Sequence[int],\n>                      fixed_pairings:Optional[list[list[tuple[int,int]]]]=None,\n>                      permutation_cfg:Optional[dict[str,Any]]=None,\n>                      comparison_loss:Optional[<built-\n>                      infunctioncallable>]=None)\n\n*DiffPaSS model for general graph alignment starting from the weighted adjacency matrices of two graphs.*\n\n|    | **Type** | **Default** | **Details** |\n| -- | -------- | ----------- | ----------- |\n| group_sizes | Sequence |  | Number of graph nodes in each group (e.g. species), assumed the same between the two graphs to align |\n| fixed_pairings | Optional | None | If not ``None``, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), ...], ...] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc. |\n| permutation_cfg | Optional | None | If not ``None``, configuration dictionary containing init parameters for the internal `GeneralizedPermutation` object to compute soft/hard permutations. Soft/hard permutations ``P`` act on adjacency matrices ``X`` via ``P @ X @ P.T`` |\n| comparison_loss | Optional | None | If not ``None``, custom callable to compute the differentiable loss between the soft/hard-permuted adjacency matrix of graph ``x`` and the adjacency matrix of graph ``y``. Defaults to dot product between all upper triangular elements |",
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Bitbol-Lab/DiffPaSS/blob/main/diffpass/train.py#L343){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GraphAlignment\n",
       "\n",
       ">      GraphAlignment (group_sizes:collections.abc.Sequence[int],\n",
       ">                      fixed_pairings:Optional[list[list[tuple[int,int]]]]=None,\n",
       ">                      permutation_cfg:Optional[dict[str,Any]]=None,\n",
       ">                      comparison_loss:Optional[<built-\n",
       ">                      infunctioncallable>]=None)\n",
       "\n",
       "*DiffPaSS model for general graph alignment starting from the weighted adjacency matrices of two graphs.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| group_sizes | Sequence |  | Number of graph nodes in each group (e.g. species), assumed the same between the two graphs to align |\n",
       "| fixed_pairings | Optional | None | If not ``None``, fixed pairings between groups, of the form [[(i1, j1), (i2, j2), ...], ...] where (i1, j1) are the indices of the first fixed pair in the first group to be paired, etc. |\n",
       "| permutation_cfg | Optional | None | If not ``None``, configuration dictionary containing init parameters for the internal `GeneralizedPermutation` object to compute soft/hard permutations. Soft/hard permutations ``P`` act on adjacency matrices ``X`` via ``P @ X @ P.T`` |\n",
       "| comparison_loss | Optional | None | If not ``None``, custom callable to compute the differentiable loss between the soft/hard-permuted adjacency matrix of graph ``x`` and the adjacency matrix of graph ``y``. Defaults to dot product between all upper triangular elements |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(GraphAlignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_graph_alignment_bootstrap():\n",
    "    # Data: two identical weighted adjacency matrices\n",
    "    size_each_group = 10\n",
    "    n_groups = 10\n",
    "    n_samples = size_each_group * n_groups\n",
    "    x = torch.exp(torch.randn((n_samples, n_samples))).to(torch.get_default_dtype())\n",
    "    y = x.clone()\n",
    "    # Within-group shuffling to control for algorithmic biases towards identity permutation\n",
    "    rand_perm_mats = []\n",
    "    for _ in range(n_groups):\n",
    "        rp_mat = torch.zeros(\n",
    "            (size_each_group, size_each_group), dtype=x.dtype, device=x.device\n",
    "        )\n",
    "        rp_mat[torch.arange(size_each_group), torch.randperm(size_each_group)] = 1\n",
    "        rand_perm_mats.append(rp_mat)\n",
    "    x_shuffle = apply_hard_permutation_batch_to_similarity(x=x, perms=rand_perm_mats)\n",
    "\n",
    "    group_sizes = [size_each_group] * n_groups\n",
    "\n",
    "    # Model\n",
    "    model = GraphAlignment(group_sizes=group_sizes)\n",
    "    results = model.fit_bootstrap(x_shuffle, y)\n",
    "    target_hard_loss = model.compute_losses_identity_perm(x, y)[\"hard\"]\n",
    "\n",
    "    # Check that the hard loss of the optimized permutation is close to the ground truth\n",
    "    assert results.hard_losses[-2][-1] / target_hard_loss > 0.95\n",
    "\n",
    "test_graph_alignment_bootstrap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
