{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7e8f88-8bca-4db0-b710-a4bc7259437b",
   "metadata": {},
   "source": [
    "# data_utils\n",
    "\n",
    "> Utilities for dataset generation and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e870ea3-a486-4d9b-960f-1fe61cbe59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93722153-82d9-4fa2-9800-d231a594c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections.abc import Sequence\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "\n",
    "from diffpass.constants import DEFAULT_AA_TO_INT\n",
    "\n",
    "\n",
    "def create_specieswise_dict(\n",
    "    msa_data: list[list[tuple[str, str]]],\n",
    "    species_name_func: callable,\n",
    "    *,\n",
    "    remove_species_with_one_seq: bool = True,\n",
    ") -> list[dict[str, list[tuple[str, str]]]]:\n",
    "    msa_data_species_by_species = []\n",
    "    for msa_side in msa_data:\n",
    "        msa_species_by_species_this_side = defaultdict(list)\n",
    "        for rec in msa_side:\n",
    "            species_name = species_name_func(rec[0])\n",
    "            msa_species_by_species_this_side[species_name].append(rec)\n",
    "        msa_data_species_by_species.append(msa_species_by_species_this_side)\n",
    "\n",
    "    if remove_species_with_one_seq:\n",
    "        for msa_data_species_by_species_this_side in msa_data_species_by_species:\n",
    "            for species_name in list(msa_data_species_by_species_this_side.keys()):\n",
    "                if len(msa_data_species_by_species_this_side[species_name]) < 2:\n",
    "                    msa_data_species_by_species_this_side.pop(species_name)\n",
    "\n",
    "    return msa_data_species_by_species\n",
    "\n",
    "\n",
    "def get_dataset_from_species(\n",
    "    msa_data_species_by_species: list[dict[str, list[tuple[str, str]]]],\n",
    "    species: Sequence[str],\n",
    "    *,\n",
    "    pop_species: bool = False,\n",
    ") -> dict:\n",
    "    dataset = {\"msa\": {}, \"positive_examples\": None}\n",
    "    group_sizes = {}\n",
    "    for msa_side, side in zip(msa_data_species_by_species, [\"left\", \"right\"]):\n",
    "        dataset[\"msa\"][side] = []\n",
    "        group_sizes[side] = []\n",
    "        for species_name in species:\n",
    "            recs = msa_side[species_name]\n",
    "            dataset[\"msa\"][side].extend(recs)\n",
    "            group_sizes[side].append(len(recs))\n",
    "            if pop_species:\n",
    "                msa_side.pop(species_name)\n",
    "\n",
    "    return {\n",
    "        \"dataset\": dataset,\n",
    "        \"group_sizes_left\": group_sizes[\"left\"],\n",
    "        \"group_sizes_right\": group_sizes[\"right\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_dataset(\n",
    "    parameters: dict,\n",
    "    msa_data: list[list[tuple[str, str]]],\n",
    "    species_name_func: callable,\n",
    "    return_species: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function that given the two full paired MSAs of interacting sequences (seen as a list of tuples)\n",
    "    creates the dataset (dictionary of MSAs, both \"left\" and \"right\" ones), made of:\n",
    "\n",
    "    - \"msa\":   the MSA used to start the training of the permutation matrix.\n",
    "    - \"positive_examples\":  MSA of correct pairs to use as context during the training. It can be None\n",
    "                            if we don't want any context.\n",
    "\n",
    "    We have to specify if we either want the list of blocks or the positive examples by setting the value of\n",
    "    `generate_blocks` to True or False.\n",
    "\n",
    "    We can also limit the depth of the MSA by changing `limit_depth`.\n",
    "    Keep in mind that the maximum limit of sequences depends on the GPU memory.\n",
    "    \"\"\"\n",
    "    assert len(msa_data[0]) == len(msa_data[1])\n",
    "    dataset = {}\n",
    "\n",
    "    # Set random generators\n",
    "    rng = default_rng(seed=parameters[\"NUMPY_SEED\"])\n",
    "    rng_other = default_rng(seed=parameters[\"NUMPY_SEED_OTHER\"])\n",
    "    # Parameters of msa\n",
    "    N_init = parameters[\"N\"]\n",
    "    max_size_init = parameters[\"max_size\"]\n",
    "    # Count species in full MSA\n",
    "    species_l, inverse_l, sizes_l = np.unique(\n",
    "        [species_name_func(rec[0]) for rec in msa_data[0]],\n",
    "        return_inverse=True,\n",
    "        return_counts=True,\n",
    "    )\n",
    "    species_r, inverse_r, sizes_r = np.unique(\n",
    "        [species_name_func(rec[0]) for rec in msa_data[1]],\n",
    "        return_inverse=True,\n",
    "        return_counts=True,\n",
    "    )\n",
    "    assert set(species_l) == set(\n",
    "        species_r\n",
    "    ), \"Species must be the same in the left and right MSA.\"\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # MAIN MSA\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Set positive_examples to None\n",
    "    dataset[\"positive_examples\"] = None\n",
    "    while True:\n",
    "        # Iterate until we find a collection of sequences with total depth\n",
    "        # 0.9 * N <= D <= 1.1 * N\n",
    "        idxs_shuffled = np.arange(len(species_l))\n",
    "        rng.shuffle(idxs_shuffled)\n",
    "        cumsum_sizes_shuffled = np.cumsum(sizes_l[idxs_shuffled])\n",
    "        idxs_in_range = np.flatnonzero(\n",
    "            np.abs(cumsum_sizes_shuffled - N_init) <= N_init * 0.1\n",
    "        )\n",
    "        if len(idxs_in_range):\n",
    "            num_species = rng.choice(idxs_in_range) + 1\n",
    "            rand_species = np.sort(idxs_shuffled[:num_species])\n",
    "            group_sizes = sizes_l[rand_species]\n",
    "            if np.all(group_sizes > 1) and np.all(group_sizes <= max_size_init):\n",
    "                break\n",
    "    # Create msa by concatenating the selected sequences\n",
    "    rand_idxs_l = []\n",
    "    rand_idxs_r = []\n",
    "    for unique_species_idx in rand_species:\n",
    "        rand_idxs_l += [\n",
    "            i for i, label in enumerate(inverse_l) if label == unique_species_idx\n",
    "        ]\n",
    "        rand_idxs_r += [\n",
    "            i for i, label in enumerate(inverse_r) if label == unique_species_idx\n",
    "        ]\n",
    "    dataset[\"msa\"] = {\n",
    "        \"left\": [msa_data[0][i] for i in rand_idxs_l],\n",
    "        \"right\": [msa_data[1][i] for i in rand_idxs_r],\n",
    "    }\n",
    "    # Print data\n",
    "    print(\"Generated initial MSA\")\n",
    "    print(\"\\tSpecies selected, total number of species selected:\")\n",
    "    print(species_l[rand_species])\n",
    "    print(rand_species, \",\", len(rand_species))\n",
    "    print(\"\\tPairs per species, total number of pairs:\")\n",
    "    print(group_sizes, \",\", sum(group_sizes))\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # POSITIVE EXAMPLES\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    if parameters[\"pos\"]:\n",
    "        while True:\n",
    "            # Indices of species not used in msa\n",
    "            unused_species_idxs = idxs_shuffled[num_species:].copy()\n",
    "            rng_other.shuffle(unused_species_idxs)\n",
    "            cumsum_sizes_shuffled = np.cumsum(sizes_l[unused_species_idxs])\n",
    "            # Iterate until we find a collection of sequences with total depth\n",
    "            # 0.9 * pos <= D <= 1.1 * pos\n",
    "            idxs_in_range_pos = np.flatnonzero(\n",
    "                np.abs(cumsum_sizes_shuffled - parameters[\"pos\"])\n",
    "                <= parameters[\"pos\"] * 0.1\n",
    "            )\n",
    "            if len(idxs_in_range_pos):\n",
    "                num_species_pos = rng.choice(idxs_in_range_pos) + 1\n",
    "                rand_species_pos = np.sort(unused_species_idxs[:num_species_pos])\n",
    "                group_sizes_pos = sizes_l[rand_species_pos]\n",
    "                if np.all(group_sizes_pos > 1):\n",
    "                    break\n",
    "        # Create msa of positive examples by concatenating the selected sequences\n",
    "        rand_idxs_pos_l = []\n",
    "        rand_idxs_pos_r = []\n",
    "        for unique_species_idx in rand_species_pos:\n",
    "            rand_idxs_pos_l += [\n",
    "                i for i, label in enumerate(inverse_l) if label == unique_species_idx\n",
    "            ]\n",
    "            rand_idxs_pos_r += [\n",
    "                i for i, label in enumerate(inverse_r) if label == unique_species_idx\n",
    "            ]\n",
    "        dataset[\"positive_examples\"] = {\n",
    "            \"left\": [msa_data[0][i] for i in rand_idxs_pos_l],\n",
    "            \"right\": [msa_data[1][i] for i in rand_idxs_pos_r],\n",
    "        }\n",
    "        # Print data\n",
    "        print(\"\\n\\nGenerated positive examples\")\n",
    "        print(\"\\tSpecies selected, total number of species selected:\")\n",
    "        print(species_l[rand_species_pos])\n",
    "        print(rand_species_pos, \",\", len(rand_species_pos))\n",
    "        print(\"\\tPairs per species, total number of pairs:\")\n",
    "        print(group_sizes_pos, \",\", sum(group_sizes_pos))\n",
    "    else:\n",
    "        dataset[\"positive_examples\"] = None\n",
    "\n",
    "    if return_species:\n",
    "        return dataset, group_sizes, species_l[rand_species]\n",
    "    return dataset, group_sizes\n",
    "\n",
    "\n",
    "def get_single_and_paired_seqs(\n",
    "    msa_x: list[tuple[str, str]],\n",
    "    msa_y: list[tuple[str, str]],\n",
    "    *,\n",
    "    group_sizes: Sequence[int],\n",
    ") -> dict[str, Union[list[list[tuple]], list[dict[str, int]]]]:\n",
    "    \"\"\"Single and paired sequences from two MSAs. The paired sequences are returned as a list of\n",
    "    dictionaries, where the keys are the concatenated sequences and the values are the number of\n",
    "    times that pair appears in the concatenated MSA.\"\"\"\n",
    "    x_seqs = []\n",
    "    y_seqs = []\n",
    "\n",
    "    idx = 0\n",
    "    xy_seqs_to_counts = []\n",
    "    for s in group_sizes:\n",
    "        x_seqs_this_group = list(zip(*msa_x[idx : s + idx]))[1]\n",
    "        x_seqs.append(x_seqs_this_group)\n",
    "        y_seqs_this_group = list(zip(*msa_y[idx : s + idx]))[1]\n",
    "        y_seqs.append(y_seqs_this_group)\n",
    "        xy_seqs_this_group = [\n",
    "            f\"{x_seq}:{y_seq}\"\n",
    "            for x_seq, y_seq in zip(x_seqs_this_group, y_seqs_this_group)\n",
    "        ]\n",
    "        unique_xy, counts_xy = np.unique(\n",
    "            np.array(xy_seqs_this_group), return_counts=True\n",
    "        )\n",
    "        xy_seqs_to_counts.append(dict(zip(unique_xy, counts_xy)))\n",
    "        idx += s\n",
    "\n",
    "    return {\"x_seqs\": x_seqs, \"y_seqs\": y_seqs, \"xy_seqs_to_counts\": xy_seqs_to_counts}\n",
    "\n",
    "\n",
    "def msa_tokenizer(\n",
    "    msa: list[tuple[str, str]],\n",
    "    aa_to_int: Optional[dict[str, int]] = None,\n",
    "    device: Optional[torch.device] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Function that given an MSA (seen as a list of tuples) tokenizes it using the MSA Transformer\n",
    "    tokenizer and transform the tokens into one-hot encodings.\n",
    "    \"\"\"\n",
    "    if aa_to_int is None:\n",
    "        aa_to_int = DEFAULT_AA_TO_INT\n",
    "\n",
    "    tokenized_msa = []\n",
    "    for header, seq in msa:\n",
    "        tokenized_msa.append([aa_to_int[c] for c in seq])\n",
    "    tokenized_msa = torch.tensor(tokenized_msa, device=device)\n",
    "\n",
    "    tokenized_msa = torch.nn.functional.one_hot(tokenized_msa).to(torch.float32)\n",
    "\n",
    "    return tokenized_msa\n",
    "\n",
    "\n",
    "def dataset_tokenizer(dataset, device: Optional[torch.device] = None):\n",
    "    \"\"\"\n",
    "    Function that given a dictionary `dataset` of MSAs (initial MSA, blocks, positive examples) tokenizes\n",
    "    each MSA and return them in a dictionary with the same keys.\n",
    "    \"\"\"\n",
    "    dataset_tokens = {}\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        # Tokenize initial MSA\n",
    "        dataset_tokens[\"msa\"] = {\n",
    "            key: msa_tokenizer(dataset[\"msa\"][key], device=device)\n",
    "            for key in dataset[\"msa\"].keys()\n",
    "        }\n",
    "        # Tokenize MSAs of positive examples and concatenate together the correct pairs, returns None\n",
    "        # if there are no positive examples\n",
    "        if dataset[\"positive_examples\"] is None:\n",
    "            dataset_tokens[\"positive_examples\"] = None\n",
    "        else:\n",
    "            tmp_pos_examples = {\n",
    "                key: msa_tokenizer(dataset[\"positive_examples\"][key], device=device)\n",
    "                for key in dataset[\"positive_examples\"].keys()\n",
    "            }\n",
    "            dataset_tokens[\"positive_examples\"] = torch.cat(\n",
    "                (tmp_pos_examples[\"left\"], tmp_pos_examples[\"right\"][..., 1:, :]), dim=2\n",
    "            )\n",
    "\n",
    "        return dataset_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
